\documentclass[12pt]{article}

\usepackage{preamble}

%optionally, include packages for syntax highlighting
\usepackage{physics}

\input{config.tex}
\input{quantum-commands.tex}

\title{\COURSECODE\ - \FULLCOURSENAME\ - \UNIVERSITY}
\author{\PROFESSORNAME\ - \STUDENTNAME}
\date{\SEMESTER\ Semester \YEAR}

\pgfplotsset{compat=1.18}

\begin{document}

\maketitle

\hfill

The underpinnings of all scientific advancements is the ability to express natural phenomena with the art of Mathmatics; this is no different for the subject of Quantum Computing. While the boundaries of quantum computing have been pushed beyond limits in theoretical terms on university blackboards, it has become of great interest to realize the theoretical computational power with the advances of hardware and technology. 

However, these notes mainly concerns itself with the mathmatical underpinnings of quantum computing that the course surrounds itself with. \FULLCOURSENAME takes a scaffolding approach designed to efficiently convey the required theoretical understanding of mathmatics in order to able to learn quantum computing. As of writing, we are basing the notes on verison one of the textbook published in March 2025. In this text, we will primarily be using dirac notation for the expression of vectors, operators, and their interactions. 

\tableofcontents

\break


\section{Summation and Product Notations}

This section primarily focuses on the common notations applied across mathmatics to denote and shorten addition and product notation. 

\subsection{Summation over a single Variable}

The sigma notation is defined as follows

$$\sum_{i=1}^{n}f(i)$$

where we use sigma $\sum$ to represent the sum of a series. For example, the sum of all numbers in a series beginning with $m$ and ending at index $n$ is written as:

$$\sum_{i=m}^{n} a_i = a_m + a_{m+1} + a_{m+2} + \cdots + a_{n-1} + a_n$$

Sums can also be infinite, commonly seen when Sigma looks as follows: $\sum^{\infty}_{i=m}$. Infinite sums are either convergent or divergent. A few of the most common converging infinite sums are as follows:

$$\sum_{i=0}^{\infty} \frac{1}{2^i} = 1 + \frac{1}{2} + \frac{1}{4} + \cdots = 2$$
$$\sum_{i=0}^{\infty}  \frac{1}{i^2} = \frac{1}{1^2} + \frac{1}{2^2} + \frac{1}{3^2} + \cdots = \frac{\pi^2}{6}$$

The first example is an infinite geometric series, and the sum of the first $n$ terms is given by:

$$S_n = \sum_{i=0}^{n} \frac{1}{2^i} = \frac{1 - \frac{1}{2^n}}{1 - \frac{1}{2}}$$

As $n \rightarrow \infty, \frac{1}{2^n} \rightarrow 0$. Consequently, $S_n \rightarrow \frac{1}{1 - \frac{1}{2}} = 2$. A rigorous proof of the second example requires extensive calculus and is not immediately obvious. While any mathmatical symbol can be used for the index of a summation, it is more practical to use something other than $i$ as in the context of complex numbers, $i$ commonly denotes the complex number$\sqrt{-1}$. moreover, sume can also be specified using descriptions. For example, 

$$\sum_{p \in P} f(p) \qquad P \in \mathbb{N^\prime}$$

where $\mathbb{N^\prime}$ is the set of all prime numbers. Summations can also contain parameters other than the index, which results in functions of those parameters. For example the discrete Fourier transform (DFT) is given by

$$\tilde{x}_k = \frac{1}{\sqrt{N}} \sum_{n=0}^{N - 1} x_n e^{- \frac{2 \pi i}{N} kn}, \quad k = 0, 1, \cdots N-1$$

where $x_n$ represents the $N$ values index by $n$ and $\tilde{x}_k$ are the Fourier coefficients. Here, $i$ is the imaginary numebr and $N$ is a positive integer representing the dimension fo the DFT, of which we will cover in greater depth in Chapter 3. The following are some useful summation forumae commonly encountered in quantum computing:

\[
\sum_{i=1}^{n} i = \frac{n(n+1)}{2}
\]

\[
\sum_{i=1}^{n} i^2 = \frac{n(n+1)(2n+1)}{6}
\]

\[
\sum_{i=1}^{n} i^3 = \left(\frac{n(n+1)}{2}\right)^2
\]

\[
\sum_{i=0}^{n} \left(a_0 + id\right) = (n+1)\left(a_0 + \frac{nd}{2}\right) \quad \text{(arithmetic series)}
\]

\[
\sum_{i=0}^{n} a^i = \frac{1 - a^{n+1}}{1 - a} \quad \text{(geometric series)}
\]

\[
(a + b)^n = \sum_{i=0}^{n} \binom{n}{i} a^{n-i} b^i \quad \text{(binomial theorem)}
\]

\[
\frac{1}{1 - x} = \sum_{n=0}^{\infty} x^n = 1 + x + x^2 + x^3 + \cdots \quad (\lvert x \rvert < 1)
\]

\[
\frac{1}{(1-x)^2} = \sum_{n=1}^{\infty} n x^{n-1} = 1 + 2x + 3x^2 + 4x^3 + \cdots \quad (\lvert x \rvert < 1)
\]

\[
\ln(1 + x) = \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n} x^n = x - \frac{x^2}{2} + \frac{x^3}{3} - \cdots \quad (\lvert x \rvert < 1)
\]

\[
e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots
\]

\[
\sin x = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n+1)!} x^{2n+1} = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots
\]

\[
\cos x = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!} x^{2n} = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \cdots
\]

Below are also a list of the common summations rules and manupulations:

\[
\sum_{i=m}^{n} a_i = \sum_{j=m}^{n} a_j \quad \text{(change of index variable)}
\]

\[
\sum_{i=s}^{t} f(i) = \sum_{n=s}^{t} f(n) \quad \text{(change of index variable)}
\]

\[
\sum_{n=s}^{t} f(n) = \sum_{n=s}^{j} f(n) + \sum_{n=j+1}^{t} f(n) \quad \text{(splitting a sum)}
\]

\[
\sum_{n=s}^{t} f(n) = \sum_{n=0}^{t-s} f(t-n) \quad \text{(reverse order)}
\]

\[
\sum_{n=s}^{t} f(n) = \sum_{n=s+p}^{t+p} f(n-p) \quad \text{(index shift)}
\]

\[
\sum_{n=s}^{t} a \cdot f(n) = a \cdot \sum_{n=s}^{t} f(n) \quad \text{(distributivity)}
\]

\[
\sum_{n=s}^{t} f(n) \pm \sum_{n=s}^{t} g(n) = \sum_{n=s}^{t} \left(f(n) \pm g(n)\right) \quad \text{(commutativity)}
\]


\subsection{Products and other Notations}

Similar to the $\sum$ notation for addition, the $\prod$ (Pi) symbol is also more commonly used to dentoe the product of a series of terms. In this 

$$\prod_{i=m}^{n} a_i = a_m \cdot a_{m+1} \cdot a_{m+2} \cdot \cdots \cdot a_{n-1} \cdot a_n$$ 

for example, the factorial of $n$ is expressed as 

$$\prod_{i=0}^{n} i = n!$$

and the relationship between $\sum$ and $\prod$, which are 

$$b^{\sum_{n=s}^{t} f(n)} = \prod_{n=s}^{t} b^{f(n)}$$

$$\sum_{n=s}^{t} \log_b f(n) = log_b \prod_{n=s}^{t} f(n)$$

It is worth noting that in quantum computing and linear algebra, there are a few special notations such as the modulo-2 sum (bitwise XOR), or in other contexts the direct sum of linear spaces, represented by $\oplus$, and the tensor product represented by $\otimes$.

\subsection{Summation over Multiple Variables}

The double summation over a rectangular array is given by 

\begin{align*}
    \sum_{i=1,j=1}^{n_1, n_2} a_{i,j} &= \sum_{i=1}^{n_1} \sum_{j=1}^{n_2} a_{i, j} = \sum_{j=1}^{n_2} \sum_{i=1}^{n_1} a_{i, j} \\
    &= a_{1, 1} + a_{1, 2} + a_{1, 3} + a_{1, 4} + \cdots + a_{1, n_2} \\
    &+ a_{2, 1} + a_{2, 2} + a_{2, 3} + a_{2, 4} + \cdots + a_{2, n_2} \\
    & + a_{3, 1} + a_{3, 2} + a_{3, 3} + a_{3, 4} + \cdots + a_{3, n_2} \\
    & + a_{4, 1} + a_{4, 2} + a_{4, 3} + a_{4, 4} + \cdots + a_{4, n_2} \\
    & + \cdots \\
    & + a_{n_1, 1} + a_{n_1, 2} + a_{n_1, 3} + a_{n_1, 4} + \cdots + a_{n_1, n_2}
\end{align*}

Here, $\sum_{i=1}^{n_1} \sum_{j=1}^{n_2}$ represents summing over each row first and then summing the results, while $\sum_{j=1}^{n_2} \sum_{i=1}^{n_1}$ will represent summing over the columns and then summing those results. The term $\sum_{i=1,j=1}^{n_1, n_2} a_{i,j}$ represents the summation over the rectangular array, irrespecive of the order. The product of two sums can be expanded into a double sum as follows:

\begin{align*}
\left( \sum_{i=1}^{m} a_i \right) \left( \sum_{j=1}^{n} b_j \right)
    &= (a_1 + a_2 + \cdots + a_m)(b_1 + b_2 + \cdots + b_n) \\
    &=\; a_1b_1 + a_1b_2 + a_1b_3 + a_1b_4 + \cdots + a_1b_n \\
    &+ a_2b_1 + a_2b_2 + a_2b_3 + a_2b_4 + \cdots + a_2b_n \\
    &+ a_3b_1 + a_3b_2 + a_3b_3 + a_3b_4 + \cdots + a_3b_n \\
    &+ \cdots \\
    &+ a_mb_1 + a_mb_2 + a_mb_3 + a_mb_4 + \cdots + a_mb_n \\
    &= \sum_{i=1}^{m} \sum_{j=1}^{n} a_i b_j = \sum_{i=1}^{m} a_i \sum_{j=1}^{n} b_j
\end{align*}

which is actually rather intuitive given how the expansion of the standard expansion of the term $(a+b)^2$ plays out, a more elementary application of the distributive property which the above equation generalizes over. For a triangular matrix, in this case the lower triangular matrix, the sum is given by

\begin{align*}
    \sum_{1 \leq j \leq n}^{} a_{i,j} &= \sum_{i=1}^{n} \sum_{j=1}^{i} a_{i, j} = 
    \sum_{j=1}^{n} \sum_{i=j}^{n} a_{i, j} = 
    \sum_{j=0}^{n-1} \sum_{j=1}^{n-j} a_{i+j, i} \\
    &= a_{1, 1} \\
    & + a_{2, 1} + a_{2, 2}  \\
    & + a_{3, 1} + a_{3, 2} + a_{3, 3} \\
    & + a_{4, 1} + a_{4, 2} + a_{4, 3} + a_{4, 4} \\
    & + \cdots \\
    & + a_{n, 1} + a_{n, 2} + a_{n, 3} + a_{n, 4} + \cdots + a_{n, n}
\end{align*}

where the term $\sum_{1 \leq j \leq n}^{} a_{i,j}$ denotes the summation over all elements in a lower triangular array including the diagonal. The first notation variation will sum up each row to the $i$th element then aggregate while the second notation sums each column starting from the $j$th element downwards then aggregate the sums. The final expression will sum along the diagonal where $j=0$ represents the main diagonal and $j=n-1$ is the first off-diagonal, which is a single term. 

\begin{example}
    Say we would like to expand the product of $\left(1 + x_i\right)$ from $1$ to $n$. We have 

    $$\prod_{i=1}^{n} \left(1 + x_i\right) = 1 + \sum_{k=1}^{n} \left(\sum_{1 \leq i_1 < \cdots < i_k \leq n} \prod_{j=1}^{k} x_{i_j}\right)$$
\end{example}

This formula represents the \textit{multinomial expansion} of a product. When you expand the equation by hand, you get the product

$$\prod_{i=1}^{n} \left(1 + x_i\right) = (1 + x_1)(1 + x_2)\cdots(1 + x_n)$$

If we break this down, we see that the outer summation $\sum_{k=1}^{n}$ will go through each possible summation size in terms of the variables in question, and that the inner summation $\sum_{1 \leq i_1 < \cdots < i_k \leq n}$ will iterate through each possible unique product of the variables. while ensuring that they are unique. Not sure how this works, but if all $x_i$ are the same, then wesee that the equation actually simplifies to a subset of the binomial theorem 

$$(1 + x)^n = \sum_{k=0}^{n} \binom{n}{k} x^k$$

where $\binom{n}{k}$ is the binomial coefficient representing the numebr of ways to choose $k$ elements from a set of $n$ distinct elements.

\break

\section{Trigonometry}

You can't escape this.

\subsection{Definitions}

I mean, where do I start? The basic trigonometric functions are defined as the ratios between the angles of a right triangle. I will not show how these ratios remain the same given the same angle, nor will I go into great mathmatical detail of how to prove these items. However, we still have to go over this. Don't ask me why. 

\includegraphics[width=\textwidth]{2_trig0}

The functions of trigonometric functions can also be plotted out as follows:

\includegraphics[width=\textwidth]{2_trig1}

\subsection{Basic Properties and Inverse Functions}

\includegraphics[width=\textwidth]{2_trig2}

% --- 1. Trig Functions ---
\begin{center}
\begin{tikzpicture}[scale=1.5]
\begin{axis}[
    axis lines=middle,
    xlabel={}, ylabel={},
    xmin=-pi, xmax=pi,
    ymin=-5, ymax=5,
    restrict y to domain=-5:5,
    samples=300,
    legend style={at={(1.05,1)}, anchor=north west},
    domain=-2*pi:2*pi
]
\addplot[blue, thick] {sin(deg(x))};
\addlegendentry{$\sin(x)$}

\addplot[red, thick] {cos(deg(x))};
\addlegendentry{$\cos(x)$}

\addplot[green!70!black, thick] {tan(deg(x))};
\addlegendentry{$\tan(x)$}

\addplot[orange, thick, domain=-2*pi:-0.1] {1/sin(deg(x))};
\addplot[orange, thick, domain=0.1:2*pi] {1/sin(deg(x))};
\addlegendentry{$\csc(x)$}

\addplot[yellow, thick, domain=-2*pi:-pi/2-0.1] {1/cos(deg(x))};
\addplot[yellow, thick, domain=-pi/2+0.1:pi/2-0.1] {1/cos(deg(x))};
\addplot[yellow, thick, domain=pi/2+0.1:2*pi] {1/cos(deg(x))};
\addlegendentry{$\sec(x)$}

\addplot[brown, thick, domain=-2*pi:-0.1] {cos(deg(x))/sin(deg(x))};
\addplot[brown, thick, domain=0.1:2*pi] {cos(deg(x))/sin(deg(x))};
\addlegendentry{$\cot(x)$}

\end{axis}
\end{tikzpicture}
\end{center}

We can also see that there are certain useful symmetric properties of the trigonometric functions

\begin{align*}
\sin(-\theta) &= - \sin(\theta)           &  \sin(\pi-\theta) &= \sin(\theta)              &  \sin(\pi+\theta) &= - \sin(\theta)\\
\cos(-\theta) &= \cos(\theta)         &  \cos(\pi-\theta) &= - \cos(\theta)   &  \cos(\pi+\theta) &= - \cos(\theta)\\
\tan(-\theta) &= - \tan(\theta)   &  \tan(\pi-\theta) &= - \tan(\theta)         &  \tan(\pi+\theta) &= \tan(\theta)
\end{align*}

There are also some common inverse functions associated with the functions.

\includegraphics[width=\textwidth]{2_trig4}

But also some interesting extensions of the commonly known inverse functions, with the example being $\mathrm{arctan2}$, a function that effectively doubles the domain of the function while preserving its properties for the purpose of, say, converting values from cartesian to spherical coordinates for the azimuthal angle $\phi$.

\[
\mathrm{arctan2}(y, x) = 
\begin{cases}
  \arctan\left(\dfrac{y}{x}\right) & \text{if } x > 0, \\[2ex]
  \arctan\left(\dfrac{y}{x}\right) + \pi & \text{if } x < 0 \text{ and } y \geq 0, \\[2ex]
  \arctan\left(\dfrac{y}{x}\right) - \pi & \text{if } x < 0 \text{ and } y < 0, \\[2ex]
  +\dfrac{\pi}{2} & \text{if } x = 0 \text{ and } y > 0, \\[2ex]
  -\dfrac{\pi}{2} & \text{if } x = 0 \text{ and } y < 0, \\[2ex]
  0 & \text{if } x = 0 \text{ and } y = 0.
\end{cases}
\]

\includegraphics[width=\textwidth]{2_Arctangent2}

\subsection{Special Angles and Function Values}

There are a few special angles that are worth remembering for the trigonometric functions mentioned above, given by the wheel below.

\includegraphics[width=\textwidth]{2_trig6}

\subsection{Trigonometric Identities}

\subsubsection*{Reciprocal and Quotient Identities}

\begin{align*}
\csc \theta &= \frac{1}{\sin \theta} &
\sec \theta &= \frac{1}{\cos \theta} &
\cot \theta &= \frac{1}{\tan \theta} \\
\end{align*}

\begin{align*}
    \tan \theta &= \frac{\sin \theta}{\cos \theta}, &
\cot \theta &= \frac{\cos \theta}{\sin \theta}
\end{align*}

\subsubsection*{Cofunction Identities}
\begin{align*}
\sin\!\left(\frac{\pi}{2} - \theta \right) &= \cos \theta, &
\cos\!\left(\frac{\pi}{2} - \theta \right) &= \sin \theta \\
\tan\!\left(\frac{\pi}{2} - \theta \right) &= \cot \theta, &
\cot\!\left(\frac{\pi}{2} - \theta \right) &= \tan \theta \\
\sec\!\left(\frac{\pi}{2} - \theta \right) &= \csc \theta, &
\csc\!\left(\frac{\pi}{2} - \theta \right) &= \sec \theta
\end{align*}

\subsubsection*{Pythagorean Identities}
\begin{align*}
\sin^2 \theta + \cos^2 \theta &= 1 \\
1 + \tan^2 \theta &= \sec^2 \theta \\
1 + \cot^2 \theta &= \csc^2 \theta
\end{align*}

\subsubsection*{Evenâ€“Odd Symmetry}
\begin{align*}
\sin(-\theta) &= -\sin \theta, &
\cos(-\theta) &= \cos \theta, &
\tan(-\theta) &= -\tan \theta \\
\csc(-\theta) &= -\csc \theta, &
\sec(-\theta) &= \sec \theta, &
\cot(-\theta) &= -\cot \theta
\end{align*}

\subsubsection*{Sum and Difference Formulas}
\begin{align*}
\sin(\alpha \pm \beta) &= \sin \alpha \cos \beta \pm \cos \alpha \sin \beta \\
\cos(\alpha \pm \beta) &= \cos \alpha \cos \beta \mp \sin \alpha \sin \beta \\
\tan(\alpha \pm \beta) &= \frac{\tan \alpha \pm \tan \beta}{1 \mp \tan \alpha \tan \beta}
\end{align*}

\subsubsection*{Double Angle Formulas}
\begin{align*}
\sin(2\theta) &= 2\sin \theta \cos \theta \\
\cos(2\theta) &= \cos^2 \theta - \sin^2 \theta \\
&= 2\cos^2 \theta - 1 \\
&= 1 - 2\sin^2 \theta \\
\tan(2\theta) &= \frac{2\tan \theta}{1 - \tan^2 \theta}
\end{align*}

\subsubsection*{Half Angle Formulas}
\begin{align*}
\sin^2\!\left(\frac{\theta}{2}\right) &= \frac{1 - \cos \theta}{2} \\
\cos^2\!\left(\frac{\theta}{2}\right) &= \frac{1 + \cos \theta}{2} \\
\tan\!\left(\frac{\theta}{2}\right) &= \frac{\sin \theta}{1 + \cos \theta} 
= \frac{1 - \cos \theta}{\sin \theta}
\end{align*}

\subsubsection*{Product-to-Sum Identities}
\begin{align*}
\sin \alpha \sin \beta &= \frac{1}{2}\big[\cos(\alpha - \beta) - \cos(\alpha + \beta)\big] \\
\cos \alpha \cos \beta &= \frac{1}{2}\big[\cos(\alpha - \beta) + \cos(\alpha + \beta)\big] \\
\sin \alpha \cos \beta &= \frac{1}{2}\big[\sin(\alpha + \beta) + \sin(\alpha - \beta)\big]
\end{align*}

\subsubsection*{Sum-to-Product Identities}
\begin{align*}
\sin \alpha + \sin \beta &= 2 \sin\!\left(\frac{\alpha + \beta}{2}\right) 
                           \cos\!\left(\frac{\alpha - \beta}{2}\right) \\
\sin \alpha - \sin \beta &= 2 \cos\!\left(\frac{\alpha + \beta}{2}\right) 
                           \sin\!\left(\frac{\alpha - \beta}{2}\right) \\
\cos \alpha + \cos \beta &= 2 \cos\!\left(\frac{\alpha + \beta}{2}\right) 
                           \cos\!\left(\frac{\alpha - \beta}{2}\right) \\
\cos \alpha - \cos \beta &= -2 \sin\!\left(\frac{\alpha + \beta}{2}\right) 
                           \sin\!\left(\frac{\alpha - \beta}{2}\right)
\end{align*}

A rather nice photo to sum the secion up is by relating the angles to each other using the following image

\includegraphics[width=\textwidth]{2_trig7.png}

\subsection{The Spherical Coordinate System}

While the expansion of the cartesian coordinate system into three dimensions is the logical linear expansion to take, some interesting basis begin to form if we consider the angles as a unit of measurement in three dimensions

\includegraphics[width=\textwidth]{2_trig8.png}

The cylindrical coordinate system elevates the polar coordinate system into three dimensions with the addition of the $z$ axis, while the spherical system is a more organic translation of the polar coordinate system into three dimensions. While the former is best suited for describing not only cylinder-like structures yet also helices, while the latter is best suited for rotations in three-dimensional space, which is particularly potent in the field of quantum computing. 

For example, the spherical coordinate system is commonly used to represent qubit states on the Bloch sphere, employing a radius $(r)$, a polar angle $(\theta)$, and an azimuthal angle $(\phi)$ to represent a point in three-dimensional space. The azimuthal angle $\phi$ is measured in the $xy$-plane from the positive $x$-axis with common values ranging from $(-\pi, \pi]$ or $(0, 2\pi]$. The polar angle is commonly measured from the positive $z$ axis towards the $xy$-plane, with values ranging from $[0, \pi]$. Note that $r \in \mathbb{R}$, meaning that we can cover the other half of the range simply by flipping the sign around. 

Conversion is relatively simple, with conversion to and from spherical to cartesian being as follows

\begin{align*}
    x &= r \sin \theta \cos \phi &     r &= \sqrt{x^2 + y^2 + z^2} \\
    y &= r \sin \theta \sin \phi &     \phi &= \mathrm{arctan2} (y, x) \\
    z &= r \cos \theta &         \theta &= \arccos \frac{z}{r}
\end{align*}

where $\mathrm{arctan2}$ was previously defined as a optimal inverse mapping onto the range of $[- \pi, \pi]$. We also have a few definitions

\begin{definition}
    The Law of Sines is defined as 

    $$\frac{\sin A}{a} = \frac{\sin B}{b} = \frac{\sin C}{c}$$
\end{definition}

\begin{definition}
    The Law of Cosines is defined as 

    $$a^2 = b^2 + c^2 - 2bc \cos A$$

    which can be rewritten as 

    $$\cos A = \frac{b^2 + c^2 - 2bc}{a^2}$$
\end{definition}

\begin{definition}
    The Law of Tangents is defined as 

    $$\frac{a - b}{a + b} = \frac{\tan \frac{1}{2}(A - B)}{\tan \frac{1}{2}(A + B)}$$
\end{definition}


\break


\section{Complex Numbers}

We consider numbers to be complex when they compose of a real and imaginary part, and they are not only fundamental to a complete understanding of algebra and mathmatics as a whole, but also form the backbone of quantum mechanics, and, by extension, quantum computing. Mastering complex numbers is like Rosie mastering the rivet gun, so we have to study it. 

\subsection{Cartesian Form}


\begin{definition}
    A complex number $z$ is defined as 

    $$
        z = x + iy, \qquad x, y \in \mathbb{R}, \quad i^2 = -1
    $$
\end{definition}

This is called the \textbf{cartesian form} of the complex number $z$ and corresponds to a point in the two-dimensional complex plane. We commonly refer to $i$ as the imaginary unit. It may seem ironic that we need imaginary numbers in quantum computing, or that we really need the imaginary number. Take it as you may. 

\includegraphics[width=\textwidth]{3_cn0.png}

% \[
% \begin{tikzpicture}[scale=1.2]
%   % Axes
%   \draw[->] (-0.2,0) -- (6,0) node[below right] {$\mathrm{Re}$};
%   \draw[->] (0,-3.2) -- (0,3.2) node[above left] {$\mathrm{Im}$};

%   % Points and projections
%   \coordinate (O) at (0,0);
%   \coordinate (Z) at (5,2.4);
%   \coordinate (Zc) at (5,-2.4);

%   \draw[dashed] (Z) -- (5,0) node[below] {$x$};
%   \draw[dashed] (Z) -- (0,2.4) node[left] {$y$};

%   \draw[dashed] (Zc) -- (5,0);
%   \draw[dashed] (Zc) -- (0,-2.4) node[left] {$-y$};

%   % Vectors
%   \draw[line width=1.6pt,orange] (O) -- (Z) node[midway,above right] {$r$};
%   \draw[line width=1.6pt,orange,dashed] (O) -- (Zc) node[midway,below right] {$r$};

%   % Angles
%   \draw (0.9,0) arc (0:25:0.9);
%   \node at (1.2,0.35) {$\theta$};

%   \draw (0.9,0) arc (0:-25:0.9);
%   \node at (1.3,-0.35) {$-\theta$};

%   % Dots
%   \fill (Z) circle (2pt);
%   \fill (Zc) circle (2pt);

%   % Labels near points
%   \node[above right] at (Z) {$z=x+iy=e^{i\theta}$};
%   \node[below right] at (Zc) {$z^*=x-iy=e^{-i\theta}$};
% \end{tikzpicture}
% \]


Complex numbers not motivated by quantum computing. In the numbers system, we have the real numbers $\mathbb{N}$, the integer numbers $\mathbb{Z}$, the real numbers $\mathbb{R}$, and the complex numbers $\mathbb{C}$. the set incursions go this way $\mathbb{N} \subset \mathbb{Z} \subset \mathbb{R} \subset \mathbb{C}$, all supersets of the preceeding set. The set of all complex numbers $\mathbb{C}$ is closed over all algabraic operations, which include addition, substraction, multiplication, division, power, and root, and is considered the superset of all numbers. 

\begin{definition}
    The basic components of a complex number are defined as follows. 

    $$\real(z) = x, \imaginary(z) = y$$

    which are the real and imaginary components of a complex number $z$. Of course the complex number itself has a few interesting properties, such as $i^2 = -1$, $i^3 = -i$, and $i^4 = 1$. The complex conjugate of a complex number $z$ is defined as 

    $$z^* = x - iy$$

    defined as inverting the sign of the imaginary component. We can express the modulus (vector length) and argument (angle with respect to the real axis) (which are $r$ and $\theta$ in polar coordinates), as follows:

    $$r = \vert z \vert = \sqrt{zz^*} = \sqrt{x^2 + y^2}$$

    A very convenient property derived from algebra is that $zz^* = x^2 + y^2$.

    $$\theta = arg(z)$$

    For the angle, we note that

    $$\tan \frac{y}{x} \Longrightarrow \mathrm{arctan2}(x, y)$$

    where $\mathrm{arctan2}$ has been defined in the previous section. 
\end{definition}

\begin{example}
    Given $z = 1 + \sqrt{3}i$, we have

    $$z^* = 1-\sqrt{3}\,i.$$

    $$|z|=\sqrt{1^2+(\sqrt{3})^2}=2.$$

    $$zz^*=(1+\sqrt{3}\,i)(1-\sqrt{3}\,i)=1-(\sqrt{3}\,i)^2=1+3=4=|z|^2.$$

    $$\theta=\arctan\!\left(\dfrac{\sqrt{3}}{1}\right)=\dfrac{\pi}{3}.$$

\end{example}


\subsection{Exponential Form}

Now it is worth noting that while we commonly write $z = x + iy$ to represent a complex numnber, we like to use the following definitions of the complex number in polar form to represent a complex number itself, defined as $z = r (\cos \theta + i \sin \theta)$. However, multiplication and its inverse operation, division, becomes unnecessarily difficult givne the presence of another notation, namely \textbf{exponential form}. 


\begin{definition}
    The exponential/euler forms of the complex numbers can be thought of as a circular form of the function $z = x + iy$. In polar coordinates, we can rewrite this number as 

    $$
        z = r \cos \theta + i \sin \theta, \qquad r \in \mathbb{R}
    $$
    Conversely, the conversion between cartesian and polar are 
    
    $$
    x = r \cos \theta \qquad y = r \sin \theta
    $$
    
    The formula for $z$ above can be rewritten as 

    $$z = r e^{i\theta}$$
\end{definition}


\begin{theorem}
    Euler's formula states that for any complex number $z = r \cos \theta + i \sin \theta$, we have:

    $$
    e^{i \theta} = \cos \theta + i \sin \theta
    $$
\end{theorem}

\begin{proof}
    Euler's formula can be proven using the Taylor series expansion for the functions:

    \[
    e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots
    \]

    \[
    \sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots
    \]

    \[
    \cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \cdots
    \]

    if we replace $x$ in the formula for $e^x$ with $e^{ix}$, we then have 

    \begin{align*}
    e^{ix} &= 1 + (ix) + \frac{(ix)^2}{2!} + \frac{(ix)^3}{3!} + \frac{(ix)^4}{4!} + \frac{(ix)^5}{5!} + \frac{(ix)^6}{6!} + \frac{(ix)^7}{7!} + \cdots \\
    &= 1 + i x - \frac{x^2}{2!} - i \frac{x^3}{3!} + \frac{x^4}{4!} + i \frac{x^5}{5!} - \frac{x^6}{6!} - i \frac{x^7}{7!} + \cdots \\
    &= \left(1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \cdots \right) + \left(i x - i \frac{x^3}{3!} + i \frac{x^5}{5!}- i \frac{x^7}{7!} + \cdots\right) \\
    &= \cos \theta + i \sin \theta
    \end{align*}

    which sums up essentially what mathmaticians call the most beautiful proof man has known. If this proof were a female robot, mathematicians would compose harmonic waves and produce digital flowers in \LaTeX to please Euler's genius. 
\end{proof}

As noted before, we know that the set of all algebraic operations is well defined and closed on the set of all complex numbers $\mathbb{C}$. Addition will be easier in cartesian form, while multiplication will be considerably simpler in exponential form. Conversion between the two is also not difficult:

\includegraphics[width=\textwidth]{3_cn2}

% $$
% \begin{array}{l|l|l}
%  & \text{Cartesian} & \text{Exponential} \\
% \hline
% \text{} & z = x + iy & z = r e^{i\theta} \\
% \text{Conjugate} & z^{*} = x - iy & z^{*} = r e^{-i\theta} \\
% \text{Modulus} & |z| = \sqrt{x^{2}+y^{2}} & |z| = r \\
% \text{Conversion} & x = r\cos\theta,\; y = r\sin\theta & r = \sqrt{x^{2}+y^{2}},\; \theta=\operatorname{arctan2}(y,x) \\
% \end{array}
% $$


\subsection{Basic Operations}

As we touched upon earlier, the set of all complex numbers are closed on operations of addition and subtraction:

$$z_1+z_2=(x_1+x_2)+i(y_1+y_2).$$

$$z_1-z_2=(x_1-x_2)+i(y_1-y_2).$$

As well on multiplication and division. For 

$$z_1=r_1 e^{i\theta_1} \qquad z_2=r_2 e^{i\theta_2}$$ 

We have:

$$z_1\cdot z_2=r_1 r_2 e^{i(\theta_1+\theta_2)}.$$

$$\displaystyle \frac{z_1}{z_2}=\frac{r_1 e^{i\theta_1}}{r_2 e^{i\theta_2}}=\frac{r_1}{r_2}e^{i(\theta_1-\theta_2)}.$$

We also have the following properties for the conjugates of complex numbers

\begin{align*}
    \vert z \vert &= \vert z^* \vert \\
    (z_1 \pm z_2)^* &= z_1^* \pm z_2^* \\
    (z_1 \cdot z_2)^* &= z_1^* \cdot z_2^* \\
    (z_1 / z_2)^* &= z_1^* / z_2^* \\
    (z^x)^* &= (z^*)^x \quad x \in \mathbb{R}\\
    (x^z)^* &= x^{z^*} \quad x \in \mathbb{R}
\end{align*}

Where the last two are not immediately obvious. To prove that $(z^x)^* = (z^*)^x$, it is useful to write out $z$ using the complex notation $r e^{i\theta}$, and the last property is best proven using the identity $a^b = e^{b \ln a}$. As for powers and roots of complex numbers, we have 

\begin{theorem}
    De Moivre's theorem states that

    $$(\cos \theta + i \sin \theta)^s = \cos{s \theta} + i \sin{s \theta}$$

    which is conveniently derived from the fact that $z^s = r^s e^{is\theta}$.
\end{theorem}

In particular, an application of this theorem is where we describe the roots of unity. Any root of unity can be descibed by a power of the first root of unity, 

$$\omega_1 = e^\frac{2 \pi i}{n}$$

the $n$-th roots of unity ($n \in \mathbb{N}$) are given by

$$
\omega_k = e^{\frac{2\pi i}{n} k} = w_1^k \qquad k = 1, 2, \ldots, n-1
$$

which essentially says that there are $n$ roots to the complex polynomial. For $n=5$, we have the 5 roots of unity given by

\includegraphics[width=\textwidth]{3_roots_unity}

In general, we say that there are $n$ values of $k$ that satisfy the equation $\displaystyle \omega_1^n = e^{(\frac{2 \pi i}{n})^n} = 1$. From this, we can generalize what we know into the summations over $\omega_k$, which is any $k$-th root of unity except for $\omega = 1$. 

$$\sum_{k=0}^{n-1} \omega_k = \sum_{k=0}^{n-1} \omega^k_1 = 0$$

This formula can be conviniently proven by applying the formula for summing a geometric sequence to the summation. From this, we can conviniently derive a useful mathmatical condition, being

\begin{example}
    The DFT Orthonormality condition depends on two parameters $k$ and $l$, and is stated as follows

    $$
    \frac{1}{N} \sum_{n=0}^{N-1}e^{- \frac{2 \pi i}{N}kn} e^{\frac{2 \pi i}{N} ln} = \delta_{k-l \bmod N}
    $$
\end{example}

where $\delta_{k-l} \pmod{N} = 1$ if and only if $k \equiv l \pmod{N}$, else $0$. It is saying that when $k$ is congruent to $l$, equivalent to $k - l = mN$, where the difference between $k$ and $l$ is divisible by some integer $m$. The $\delta_{k-l \bmod N}$ term is a Kronecker delta of $k \equiv l \pmod{N}$, where the result is $1$ if $k \equiv l \pmod{N}$ holds and $0$ in the case of $k \ncong l \pmod{N}$. If we define $\omega = \displaystyle  e^{i \frac{2 \pi}{N}}$ as a primitive $N$th root of unity (satisfying $\omega^N = 1$), we have the derivation

$$
\begin{aligned}
\frac{1}{N}\sum_{n=0}^{N-1} e^{-\frac{2\pi i}{N}kn}\, e^{\frac{2\pi i}{N}ln}
&= \frac{1}{N}\sum_{n=0}^{N-1} \omega^{-kn}\omega^{ln} \\
&= \frac{1}{N}\sum_{n=0}^{N-1} \omega^{n(l-k)} \\
&=
\begin{cases}
\displaystyle \frac{1}{N}\sum_{n=0}^{N-1} 1 = 1, & \text{if } l \equiv k \pmod N,\\
\displaystyle \frac{1}{N}\,\frac{1-\omega^{(l-k)N}}{1-\omega^{(l-k)}} = 0, & \text{if } l \not\equiv k \pmod N
\end{cases}
\\
&= \delta_{k-l \bmod N},
\end{aligned}
$$

where we used the fact that $\omega^{n^N} = \left(\omega^N\right)^n = 1^n = 1$ for $n \in \mathbb{N}$.

\subsection{Advanced Operations}

It's probably best to illustrate more advanced operations on complex numbers with the help of some examples

\begin{example}
    Evaluating $\sqrt{i}$ or $\sqrt{\sqrt{1}}$ gives:

    $$\sqrt{i} = \left(e^{\frac{\pi i}{2}}\right)^{\frac{1}{2}} = e^{\frac{\pi i }{4}} = \cos \frac{\pi}{4} + i \sin \frac{\pi}{4} = \frac{1}{\sqrt{2}} (1 + i)$$

    The inverse is given by

    $$\left(\frac{1}{\sqrt{2}} (1 + i)\right)^2 = \frac{1}{2}(1 + 2i + i^2) = \frac{(2i)}{2} = i$$
\end{example}


\begin{example}
    Evaluating $$ \left(\frac{1}{2}+\frac{\sqrt{3}}{2}i\right)^{50} $$ gives

    $$
    \left(\frac{1}{2}+\frac{\sqrt{3}}{2}i\right)^{50}
    =\left(e^{\frac{\pi i}{3}}\right)^{50}
    =e^{\frac{50\pi i}{3}}
    =e^{\left(16+\frac{2}{3}\right)\pi i}
    =e^{\frac{2\pi i}{3}}
    =-\frac{1}{2}+\frac{\sqrt{3}}{2}i.
    $$
\end{example}


\begin{example}
    Evaluating $$2^{3+4i}$$ gives us
    $$
    2^{3+4i}=2^3\cdot 2^{4i}=8\cdot e^{4\ln(2)\,i}
    =8\cos\!\big(4\ln 2\big)+i\,8\sin\!\big(4\ln 2\big).
    $$

\end{example}

\begin{example}
    Evaluating $$\cos(3+4i)$$
    gives us
    \begin{align*}
        \cos(3+4i) &= \frac{1}{2}\!\left(e^{i(3+4i)}+e^{-i(3+4i)}\right) \\
        &= \frac{1}{2}\!\left(e^{-4+3i}+e^{4-3i}\right) \\
        &= \frac{1}{2}e^{-4}(\cos 3+i\sin 3)+\frac{1}{2}e^{4}(\cos 3-i\sin 3) \\
        &= \frac{1}{2}(e^{-4}+e^{4})\cos 3+i\,\frac{1}{2}(e^{-4}-e^{4})\sin 3 \\     
    \end{align*}

\end{example}

\begin{example}
    If we have the equation $z^5=\frac{1}{2}+\frac{\sqrt{3}}{2}i$, solving for $z$ gives

    $$
    z_k = e^{\frac{\pi i}{15}} e^{\frac{2k\pi i}{5}}
    \quad k=0,1,2,3,4.
    $$
\end{example}

It is worth noting that there exists a way to express the trigonometric functions $\sin$ and $\cos$ as a function of euler's number. We know that 

$$
e^{i\theta} = \cos \theta + i \sin \theta \qquad e^{-i\theta} = \cos \theta - i \sin \theta
$$

from this, we can derive that 

\begin{align*}
    e^{i\theta} + e^{-i\theta} &= \cos \theta + i \sin \theta + \cos \theta - i \sin \theta \\
    e^{i\theta} + e^{-i\theta} &= \cos \theta + \cos \theta\\
    \cos \theta &= \frac{1}{2}\left(e^{i\theta} + e^{-i\theta}\right)\\
\end{align*}

and that 

\begin{align*}
    e^{i\theta} + e^{-i\theta} &= \cos \theta + i \sin \theta - (\cos \theta - i \sin \theta) \\
    e^{i\theta} - e^{-i\theta} &= i \sin \theta - \left(i \sin \theta\right)\\
    \sin \theta &= \frac{1}{2i}\left(e^{i\theta} - e^{-i\theta}\right)\\
\end{align*}

Another way of expressing this is by saying

$$
\cos x = \real(e^{i\theta}) = \frac{e^{ix} + e^{-ix}}{2}
$$
$$
\sin x = \imaginary(e^{i\theta}) = \frac{e^{ix} - e^{-ix}}{2i}
$$


One final yet very important item to rememebr throughout the curriculum is that powers for complex numbers are \textbf{rotations}.

\break

\section{Sets, Groups, and Functions}

This chapter is mainly going to fly over the various mathmatical concepts that make up the backbone of many mathmatical fields, including those relevant to quantum computing. 

\subsection{Sets}

The concepts of sets are fundamental to many areas of mathmatics. A set is a well-defined collection of distinct objects, which is also an object in its own right. 

\begin{definition}[Set]
    A set is an (unordered) collection of objects, which are said to be elements or members of the set.
\end{definition}

Let $A = \{2, 4, 6, 8\}$ be the set of even numbers less than 10. The elements of this set are 2, 4, 6, and 8. We can also write sets using set-builder notation, such as $B = \{x \mid x \text{ is a vowel in the English alphabet}\} = \{a, e, i, o, u\}$.

\vspace{12pt}


\begin{definition}[Tuple]
    A tuple (or sequence) is an ordered list of elements. 
\end{definition}

Consider the tuple $T = (3, 1, 4, 1, 5)$. This is an ordered sequence where the first element is 3, the second is 1, the third is 4, the fourth is 1, and the fifth is 5. Note that the order matters and repetition is allowed, so $(3, 1, 4, 1, 5) \neq (1, 1, 3, 4, 5)$.


\vspace{12pt}


\begin{definition}[Cardinality]
    The cardinality of a set $A$, denoted $\vert A \vert$, is the number of elements in $A$.
\end{definition}

For the set $A = \{2, 4, 6, 8\}$, the cardinality is $|A| = 4$ since there are 4 elements. For the set $B = \{x, y, z\}$, we have $|B| = 3$. The empty set has cardinality $|\emptyset| = 0$.


We can categorize sets based on their cardinality into finite, countably infinite, or uncountably infinite. A set is said to be countably infinite if it can be bijectively mapped to the set of natural numbers $\mathbb{N}$ such as the set of integers $\mathbb{Z}$, while the set is said to be uncountably infinite if there is no one-to-one to the set of natural numbers, such as the set of real numbers $\mathbb{R}$.

\begin{example}
    Finite set: $A = \{1, 2, 3, 4, 5\}$ has $|A| = 5$.
    Countably infinite set: The set of integers $\mathbb{Z} = \{\ldots, -2, -1, 0, 1, 2, \ldots\}$ can be put in one-to-one correspondence with the natural numbers $\mathbb{N}$, so $|\mathbb{Z}| = |\mathbb{N}| = \aleph_0$.
    Uncountably infinite set: The set of real numbers $\mathbb{R}$ cannot be put in one-to-one correspondence with $\mathbb{N}$, so $|\mathbb{R}| > \aleph_0$.
\end{example}



\vspace{12pt}


\begin{definition}[Subset and Superset]
    We call $B$ a subset of $A$, denoted $B \subseteq A$ if $\forall b \in B, b \in A$. In this case, $A$ is a superset of $B$, denoted $B \supseteq A$. If $B$ is a subset of $A$ but not equal to $A$, then $B$ is called a proper subset of $A$, denoted $B \subset A$.
\end{definition}


Let $A = \{1, 2, 3, 4, 5\}$ and $B = \{2, 4\}$. Then $B \subseteq A$ since every element of $B$ is also in $A$. We can also say $A \supseteq B$. Since $B \neq A$, we have $B \subset A$ (B is a proper subset of A). Additionally, $A \subseteq A$ since every set is a subset of itself.


\vspace{12pt}


\begin{definition}[Union]
    The union of two sets $A$ and $B$, denoted by $A \cup B$, is the set containing all the elements in $A$, $B$, or both. 
\end{definition}

Let $A = \{1, 3, 5, 7\}$ and $B = \{2, 3, 6, 7, 8\}$. Then $A \cup B = \{1, 2, 3, 5, 6, 7, 8\}$, which contains all elements that appear in either set A or set B (or both).



\vspace{12pt}


\begin{definition}[Intersection]
    The intersection of two sets $A$ and $B$, denoted by $A \cap B$, is the set containing all the elements in both $A$ and $B$.
\end{definition}


Using the same sets $A = \{1, 3, 5, 7\}$ and $B = \{2, 3, 6, 7, 8\}$, we have $A \cap B = \{3, 7\}$, which contains only the elements that appear in both sets.



\vspace{12pt}


\begin{corollary}[Disjoint Sets]
    Two or more sets are said to be disjoint if they have no elements in common, that is, their intersection is the empty set: $A \cap B = \emptyset$
\end{corollary}


Let $A = \{1, 3, 5\}$ and $B = \{2, 4, 6\}$. These sets are disjoint because $A \cap B = \emptyset$ - they share no common elements.



\vspace{12pt}


\begin{definition}[Difference]
    The difference of two sets $A$ and $B$, denoted by $A - B$ or $A \ B$, is the set containing all the elements in $A$ but not in $B$.
\end{definition}


Let $A = \{1, 2, 3, 4, 5\}$ and $B = \{3, 4, 6, 7\}$. Then $A - B = \{1, 2, 5\}$, which contains the elements in A that are not in B. Similarly, $B - A = \{6, 7\}$.


\vspace{12pt}



\begin{definition}[Universal Set]
    The universal set, denoted by $U$, is the set that contains all elements under consideration, usually in relation to a particular problem or discussion. Every other set in that context is a subset of the universal set $U$.
\end{definition}

In a problem about students at a university, the universal set might be $U = $ all students at the university. If we're discussing card games, the universal set could be $U = $ all 52 cards in a standard deck.



\vspace{12pt}



\begin{definition}[Complement]
    The complement of a set $A$, denoted by $\overline{A}$ or $A^{c}$, is the set of all elements in the universal set $U$ that are not in set $A$. 
\end{definition}

Let $U = \{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\}$ be the universal set and $A = \{2, 4, 6, 8, 10\}$. Then the complement of A is $\overline{A} = \{1, 3, 5, 7, 9\}$, containing all elements in U that are not in A.



\vspace{12pt}


\begin{definition}[Cartesian Product]
    The cartesian product of two sets $A$ and $B$, denoted $A \cross B$, is the set of all ordered pairs $(a, b), a \in A, b \in B$. 
\end{definition}


Let $A = \{1, 2\}$ and $B = \{x, y, z\}$. Then $A \times B = \{(1,x), (1,y), (1,z), (2,x), (2,y), (2,z)\}$. The Cartesian product contains all possible ordered pairs where the first element comes from A and the second from B. Note that $|A \times B| = |A| \cdot |B| = 2 \cdot 3 = 6$.



\vspace{12pt}


\begin{definition}[Set Partitions]
    A partition of a set $A$ is a collection of disjoint subsets of $A$ such that every element in $A$ is included in exactly one subset. These subsets are called blocks of the partition. 
\end{definition}

Let $A = \{1, 2, 3, 4, 5, 6\}$. One partition of A could be $\{A_1, A_2, A_3\}$ where $A_1 = \{1, 2\}$, $A_2 = \{3, 5\}$, and $A_3 = \{4, 6\}$. These subsets are disjoint ($A_i \cap A_j = \emptyset$ for $i \neq j$) and their union gives the original set ($A_1 \cup A_2 \cup A_3 = A$).




\vspace{12pt}

\begin{theorem}[Set Partition]
    If $\{A_1, A_2, \ldots, A_n\}$ is a partition of set $A$, and $B \subseteq A$, then  $\{A_1 \cap B, A_2 \cap B, \ldots, A_n \cap B\}$ is a partition of set $B$.
\end{theorem}

Let $A = \{1, 2, 3, 4, 5, 6\}$ with partition $\{A_1, A_2, A_3\}$ where $A_1 = \{1, 2\}$, $A_2 = \{3, 5\}$, and $A_3 = \{4, 6\}$. If $B = \{2, 3, 4, 5\} \subseteq A$, then $\{A_1 \cap B, A_2 \cap B, A_3 \cap B\} = \{\{2\}, \{3, 5\}, \{4\}\}$ forms a partition of B.





\begin{definition}[Power Set]
    The power set of $A$ is the set of all subsets of $A$, denoted by $\mathcal{P}(A)$ is the set of all subsets of $A$.
\end{definition}


Let $A = \{1, 2\}$. The power set $\mathcal{P}(A)$ contains all possible subsets of A:
$\mathcal{P}(A) = \{\emptyset, \{1\}, \{2\}, \{1, 2\}\}$
Note that $|\mathcal{P}(A)| = 2^{|A|} = 2^2 = 4$. For any set with n elements, its power set has $2^n$ elements. For set operations in general, there are a few laws worth noting.

Identity Laws:
$$
A \cup \varnothing = A$$$$ A \cap U = A.
$$

Domination Laws:

$$
A \cup U = U$$$$ A \cap \varnothing = \varnothing.
$$

Idempotent Laws:
$$
A \cup A = A$$$$ A \cap A = A.
$$

Absorption Laws:
$$
A \cup (A \cap B) = A$$$$ A \cap (A \cup B) = A.
$$

Complement Laws:
$$
A \cup \overline{A} = U$$$$ A \cap \overline{A} = \varnothing$$$$ \overline{\overline{A}} = A.
$$

Commutative Laws:
$$
A \cup B = B \cup A$$$$ A \cap B = B \cap A.
$$

Associative Laws:
$$
A \cup (B \cup C) = (A \cup B)\cup C$$$$ A \cap (B \cap C) = (A \cap B)\cap C.
$$

Distributive Laws:
$$
A \cup (B \cap C) = (A \cup B)\cap (A \cup C)$$$$
A \cap (B \cup C) = (A \cap B)\cup (A \cap C).
$$

De Morgan's Laws:
$$
\overline{A \cap B}=\overline{A}\,\cup\,\overline{B}$$$$
\overline{A \cup B}=\overline{A}\,\cap\,\overline{B}.
$$


\begin{definition}[Totally Ordered Set]
A totally ordered set is a set in which every pair of elements is comparable; for any two elements $a$ and $b$, either $a \le b$ or $b \le a$ holds. 
\end{definition}

Examples include the real numbers and the integers with their usual orders.

\vspace{12pt}


\begin{definition}[Partially Ordered Set (Poset)]
A partially ordered set (poset) is a set equipped with a relation $\le$ that is reflexive, antisymmetric, and transitive; in a poset, not every pair of elements must be comparable, so some pairs may be incomparable. For example, the power set $\mathcal{P}(\mathbb{Z})$ ordered by inclusion $(\subseteq)$ contains subsets such as $\{1,2\}$ and $\{2,3\}$ that are not comparable, and the positive integers ordered by divisibility have incomparable primes like $5$ and $7$.
\end{definition}
\vspace{12pt}


\begin{definition}[Supremum]
Let $S$ be a nonempty subset of a partially ordered set $P$. An element $u \in P$ is the supremum of $S$, denoted by $u=\sup S$, if:
\begin{enumerate}
\item \textbf{Upper bound:} every $s\in S$ satisfies $s \le u$.
\item \textbf{Least upper bound:} if $v$ is any upper bound of $S$, then $u \le v$.
\end{enumerate}
For instance, $\sup\{-1,-2,-3,\dots\}=-1$, and $\sup\{\sin x: x\in[0,\pi]\}=1$.
\end{definition}
\vspace{12pt}

\begin{definition}[Infimum]
Let $S$ be a nonempty subset of a partially ordered set $P$. An element $\ell \in P$ is the infimum of $S$, denoted by $\ell=\inf S$, if:
\begin{enumerate}
\item \textbf{Lower bound:} every $s\in S$ satisfies $\ell \le s$.
\item \textbf{Greatest lower bound:} if $v$ is any lower bound of $S$, then $v \le \ell$.
\end{enumerate}
For example, $\inf\{e^{-x}: x>0\}=0$, even though $0$ is not an element of the set.
\end{definition}


\subsection{Groups}

Groups, rings, and fields lay the foundation in mathmatics that build upon sets with additional operations in algebra. These strucutres are ubiquitous in mathmatics and physics and lay the foundation for quantum computing as well. 

A group is a set equipped with a single binary operation that exhibits certain properties, much like addition and multiplication. A ring expands on this by incorporating two operations, typically referred to as multiplication and addition. A field is a more stringent structure where the set is a group under both operations, with multiplication also being commutative, and every non-zero element having a multiplicative inverse. 

\begin{definition}
    A group is a set $G$ which is \textit{closed} under an operation $\cdot$ (that is $\forall x, y \in G, x \cdot y \in G$) and satisfies the following properties:

    \begin{enumerate}
        \item \textbf{Identity:} $\exists e \in G$ where $\forall x \in G, x \cdot e = x = e \cdot x$. We define $e$ to be the identity element.
        \item \textbf{Inverse:} $\forall x \in G, \exists y \in G$ such that $x \cdot y = e = y \cdot x$, where $e$ is the identity element identified above. 
        \item \textbf{Associativity:} The operation $\cdot$ is associative for every $x, y, z \in G$, i.e. $$x\cdot(y\cdot z) = (x \cdot y)\cdot z$$
    \end{enumerate}
\end{definition}

In mathmatical contexts, it is common to omit the symbol $\cdot$ or $*$ for the group operation and simply write $x \cdot y$ as $xy$. Some common exmaples include the gorup of integers $\mathbb{Z}$ under addition, denoted by $(\mathbb{Z}, +)$, with $e = 0$, and the inverse of $x$ being $-x$. Another example would be the group of integers modulo $n$, denoted $\mathbb{Z}/n\mathbb{Z}$, with closure (the sum of any two integers modulo $n$ also forming a group under modulo $n$), identity, inverse, and associativity all holding under the subspace of $\mathbb{Z}/n\mathbb{Z}$.

\begin{definition}[Abelian Group]
   A group is said to be \textit{abelian} if the operation $\cdot$ is commutative $\forall x, y \in G$, that is, $$x \cdot y = y \cdot x$$ 
\end{definition}


While the examples so fare are all abelian groups, there are some of groups that are not abelian. For exmaple, the set of all symmetries of an equilateral triangle, known as the dihedral group $D_3$ is not abelian. As you can see, the table of operations is not symmetrical, rendering the group as a non-abelian group. However, the identity, inverse, and associative elements/properties are all present upon verification. 


\[
\begin{array}{c|cccccc}
   & e & a & b & c & r & s \\
\hline
e & e & a & b & c & r & s \\
a & a & e & r & s & b & c \\
b & b & s & e & r & c & a \\
c & c & r & s & e & a & b \\
r & r & c & a & b & s & e \\
s & s & b & c & a & e & r \\
\end{array}
\]



However, in the context of quantum computing, there are a few symmetry groups worth noting. They are

\begin{enumerate}
    \item $\mathbf{SO}(N):$ The orthogonal group in $N$ dimensions consists of all $N \cross N$ orthogonal matrices with determinant $1$, representing rotations in $\mathbb{R}^N$. These rotations preserver distance sand the orientation of objects in question. SO($2$) corresponds to a circle, and SO($3$) to a sphere. 
    \item $\mathbf{SU}(2):$ The special unitary group of degree 2 comprises of all $2 \cross 2$ unitary matrices with determinant $1$. It is closely related to SO($3$) and is commonly used to describe spins and qubit states. Each rotation in SO($3$) corresponds to two points in SU($2$). 
    \item $\mathbf{SU}(N):$ This represents the special unitary group of degree $N$, extending the concepts of SU($2$). These groups are useful in the study of quantum entanglement in quantum computing concerning $N$-level quantum systems. 
\end{enumerate}

\begin{definition}[Subgroups]
    A subgroup $H$ of group $G$ is a subset of $G$ that is a group in by itself, with the same group operation in $G$. You can think of this as a reduced version of $G$ where still 
    \begin{enumerate}
        \item The identity element of $G$ is in $H$.
        \item $\forall h_1, h_2 \in H, \quad h_1 \cdot h_2 \in H$.
        \item $\forall h \in H, \quad h^{-1} \in H$
    \end{enumerate}
\end{definition}

\begin{theorem}[Lagrange's Theorem]
    For any finite group $G$ and any subgroup $H$ of $G$, the order of $H$ divides the order of $G$, i.e. $$\vert G \vert \equiv 0 \mod \vert H \vert$$
\end{theorem}
\vspace{12pt}


\begin{definition}[Coset]
    Given a group $G$ and a subgroup $H$ of $G$, the \textbf{left coset} of $H$ with representative $g \in G$ is the set $gH = \{gh \vert h \in H\}$. Similarly, the right coset is $Hg = \{hg \vert h \in H\}$.
\end{definition}

\begin{theorem}[Parition Theorem for Cosets]
    The collection of all left cosets of a subgroup $H$ forms a partition of the group $G$. This means 
    \begin{enumerate}
        \item Every element of $G$ belongs to exactly one coset of $H$.
        \item Cosets are disjoint and have no elements in common.
    \end{enumerate}
\end{theorem}

\begin{definition}[Normal Subgroup]
    A subgroup $N$ of a group $G$ is called a normal subgroup if it is invariant under conjugation by elements of $G$. This means that $\forall n \in N, \quad g \in G, \quad gng^{-1} \in N$. In notation, $N \triangleleft G$ if $$gNg^{-1} = \{gng^{-1} \vert n \in N\} \subseteq N \forall g \in G.$$
\end{definition}

Now, note that this is not the same as an abelian group. While the abelian group imples that every subgroup is a normal subgroup, the latter does not imply the former.

\begin{example}[Quaternion Group $Q_8$]
Let $$Q_8=\{\pm 1,\pm i,\pm j,\pm k\}$$ with relations $$i^2=j^2=k^2=ijk=-1.$$ This group is non-abelian since, for instance, $ij=k$ while $ji=-k$, so $ij\neq ji$. Nevertheless, every subgroup of $Q_8$ is normal; the subgroups are $\{1\}$, $\{\pm 1\}$, $\langle i\rangle=\{\pm 1,\pm i\}$, $\langle j\rangle=\{\pm 1,\pm j\}$, $\langle k\rangle=\{\pm 1,\pm k\}$, and $Q_8$ itself, and each is invariant under conjugation by any element of $Q_8$. Thus $Q_8$ is a non-abelian group in which all subgroups are normal (a Hamiltonian group).
\end{example}



Also, it is worth noting that the left and right cosets of a normal subgroup $N$ are the same, allowing the group operations on cosets to be well-defined, which brings us to quotient groups:

\begin{definition}[Quotient Group]
    Let $G$ be a group, and $N \triangleleft G$. The quotient group $G / N$ is the set of cosets of $N \in G$ with the group operations defined by: $$(gN) (hN) (gh)N \quad \forall g, h \in G$$
\end{definition}

\begin{example}
    Consider the group $\mathbb{Z}$ of integers under addition and the subgroup $2\mathbb{Z}$ consisting of all even integers. The quotient group $\mathbb{Z}/2\mathbb{Z}$ is the set of cosets of $2\mathbb{Z}$ in $\mathbb{Z}$:

\begin{itemize}
\item The coset $0+2\mathbb{Z}=\{\ldots,-4,-2,0,2,4,\ldots\}$ represents the even integers.
\item The coset $1+2\mathbb{Z}=\{\ldots,-3,-1,1,3,5,\ldots\}$ represents the odd integers.
\end{itemize}

Thus, $\mathbb{Z}/2\mathbb{Z}$ has two elements: $0+2\mathbb{Z}$ and $1+2\mathbb{Z}$, corresponding to the even and odd integers, respectively. The group operation is addition modulo $2$.
\end{example}

This quotient group is \emph{isomorphic} to $\mathbb{Z}_2=\{0,1\}$ under addition modulo $2$, denoted as $\mathbb{Z}/2\mathbb{Z}\cong \mathbb{Z}_2$. Two groups are isomorphic if they have the same structure, meaning there is a one-to-one correspondence between the elements of the two groups that preserves the group operation.

\begin{definition}[Cyclic Group]
    A group $G$ is cyclic if $\exists g \in G$ such that $\forall x \in G$, $x$ can be expressed as powers (repeated operations) of $g$. 
\end{definition}

A good example of this would be $D_3$, which also has multiple generators. 
\vspace{12pt}

\begin{definition}[Ring]
    A ring is a set $R$ equipped with two operations $+$ and $\times$ satisfying the following properties:
    \begin{enumerate}
        \item $(R, +)$ forms an abelian group. 
        \item $(R, \times)$ is associative, that is $\forall a, b, c \in R$, $$a \times (b \times c) = (a \times b) \times c$$ 
        \item The distributive properties hold, that is $\forall a, b, c \in R$, $$a \times (b + c) = (a \times b) + (a \times c) $$$$ (b + c) \times a = (b \times a) + (c \times a)$$
    \end{enumerate}
\end{definition}

For example, both the set of integers $\mathbb{Z}$ and the group of integers modulo $n$, $\mathbb{Z}/n\mathbb{Z}$, are both rings. However, multiplication notably does not have an inverse under most groups of integers. For this to hold, we need to have fields. 

\begin{definition}[Field]
    A field is a set $F$ with two operations $+$ and $\times$, where
    \begin{enumerate}
        \item $(R, +)$ forms an abelian group. 
        \item $(R - \{0\}, \times)$ forms an abelian group. 
        \item The distributive properties holds as in rings. 
    \end{enumerate}
\end{definition}

Some common examples include the set of rational numbers $\mathbb{Q}$, the set of real numbers $\mathbb{R}$, and the set of complex numbers $\mathbb{C}$. Interestingly, $\mathbb{Z}/p\mathbb{Z}$ where $p$ is a prime is also a field.

\subsection{Functions}

Functions serve as mappings from one set to another. They assign each element in the domain to exactly one element in the codomain, and are essential for describing relationships using math. 

\begin{definition}[Function]
Let $f$ be a function from set $A$ to set $B$. \textit{A function from $A$ to $B$}, denoted $f : A \to B$, is an assignment of exactly one element of $B$ to each element of $A$. 
\end{definition}

We write $f(a) = b$ to denote the assignment of $b$ to an element $a$ of $A$ by the function $f$.

\begin{example}
$f(x) = x^2$ is a function $f : \mathbb{R} \rightarrow \mathbb{R}$. \quad $r = \sqrt{x^2 + y^2}$ is a function $f : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$.
\end{example}

\begin{definition}[Image, Range, and Domain]
Let $f$ be a function from $A$ to $B$.
    \begin{itemize}
        \item We say that $A$ is the \textit{domain} of $f$ and $B$ is the \textit{codomain} of $f$.
        \item If $f(a) = b$, $b$ is the \textit{image} of $a$ and $a$ is a \textit{pre-image} of $b$.
        \item The \textit{range} of $f$ (a subset of $B$) is the set of all images of elements of $A$.
        \item Let $S$ be a subset of $A$. The \textit{image of} $S$ is a subset of $B$ that consists of the images of the elements of $S$. We denote the image of $S$ by $f(S)$, so that $$f(S) = \{ f(s) \mid s \in S \}$$
    \end{itemize}
\end{definition}

\begin{center}
    \includegraphics[width=\textwidth/2]{4_func0}
\end{center}


There are also a few common function types that we will define:

\begin{definition}[Injective]
    A function $f: A \rightarrow B$ is injective if $\forall a_1, a_2 \in A$, we have $f(a_1) = f(a_2) \implies a_1 = a_2$.
\end{definition}


\begin{definition}[Surjective]
    A function $f: A \rightarrow B$ is surjective if the whole codomain is covered, meaning that $\forall b \in B, \quad \exists a \in A$ such that $f(a) = b$.
\end{definition}

Sometimes we call surjective functions \emph{onto} functions.

\begin{definition}[Bijective]
    A function is bijective if it is both \emph{injective} and \emph{surjective}.
\end{definition}


\includegraphics[width=\textwidth]{4_func1}


Only with bijectivity established can we define the inverse of a function.

\begin{definition}[Inverse Function]
    If we take the function $f: A \rightarrow B$, it's domain $A$ and codomain $B$, we can define an inverse function $f^{-1}: B \rightarrow A$ such that $\forall b \in B, \quad f^{-1}(b) = a$ if and only if $f(a) = b$. 
\end{definition}
 
Put plainly, $\forall a \in A, \quad f^{-1}(f(a)) = a$ and $\forall b \in B, \quad f(f^{-1}(b)) = b$.


\subsection{Common Functions and Asymptotic Behavior}

This section will be a brief review of real functions $(f: \mathbb{R} \rightarrow \mathbb{R})$ commonly used in quantum computing. 
\subsubsection*{Power Functions}

Power functions take the form $f(x) = x^p, \quad x \geq 0, p \in \mathbb{R}$. 

The behavior of the function varies significantly with the exponent $p$:

\begin{itemize}
    \item For $p > 0$, $f(x)$ increases as $x$ increases. $f(x)$ exhibits a more rapid growth with a larger $p$.
    
    \item For $p < 0$, $f(x)$ decreases as $x$ increases.
    
    \item When $p = 0$, $f(x) = 1$, regardless of $x$ (excluding $x = 0$), which is a constant function.
    
    \item For $p = 1$, $f(x) = x$, representing a linear relationship.
\end{itemize}

Key properties of power functions include the rules for exponentiation:

\begin{itemize}
    \item Multiplying powers with the same base: $x^a \cdot x^b = x^{a+b}$.
    
    \item Dividing powers with the same base: $x^a / x^b = x^{a-b}$.
\end{itemize}

\subsubsection*{Polynomial Functions}

A polynomial function is a sum of terms $a_i x^i$, where $i$ is a non-negative integer:

\[
f(x) = a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n.
\]

Its behavior for large $x$ values is predominantly determined by its highest power term, $x^n$, where $n$ is the degree of the polynomial.

An $n$-th degree polynomial has $n$ complex roots (counting multiplicities), and according to Vieta's formulas, the sum of these roots is equal to $-a_{n-1}/a_n$ and their product $(-1)^n a_0/a_n$.

\begin{example}
Consider the polynomial function $f(x) = x^3 - 7x^2 + 14x - 8$. It can be factored as:
\[
f(x) = (x - 1)(x - 2)(x - 4).
\]
\end{example}

The roots of this polynomial are $x = 1, x = 2$, and $x = 4$, which can be found by solving the equations $(x - 1) = 0, (x - 2) = 0$, and $(x - 4) = 0$. According to Vieta's formulas, the sum of the roots is:

\[
1 + 2 + 4 = 7 = -\frac{-7}{1},
\]

and the product is:

\[
1 \cdot 2 \cdot 4 = 8 = (-1)^3 \frac{-8}{1}.
\]

\subsubsection*{Exponential Functions}

Exponential functions are defined as $f(x) = b^x$, where $b$ is a positive constant (called the base) and $b \neq 1$. The variable $x$ is the exponent. The key characteristic is that the variable is in the exponent. Some important notes:

\begin{itemize}
    \item Growth and Decay:
    \begin{itemize}
        \item If $b > 1$, $f(x)$ exhibits exponential growth -- increasing rapidly as $x$ increases. Larger bases lead to faster growth.
        
        \item If $0 < b < 1$, $f(x)$ shows exponential decay -- decreasing towards zero as $x$ increases.
    \end{itemize}
    
    \item Always Positive: Exponential functions are always positive for any real-valued input $x$.
    
    \item Horizontal Asymptote: They approach zero for one direction of $x$ (negative infinity for growth, positive infinity for decay).
    
    \item Base $e$: The natural exponential function with $b = e$ (Euler's number, $\approx 2.718$), i.e, $e^x$, also denoted as $\exp(x)$, has special significance across mathematics.
\end{itemize}

\includegraphics[width=\textwidth]{4_func2}


\subsubsection*{Logarithmic Functions}

Logarithmic functions are the inverses of exponential functions. They are defined as $f(x) = \log_b(x)$, where $b$ is a positive constant $(b \neq 1)$ and $x > 0$. Some key points:

\begin{itemize}
    \item Reversing Exponentiation: If $b^y = x$ then $\log_b(x) = y$.
    
    \item Growth and Behavior
    \begin{itemize}
        \item For $b > 1$, $\log_b(x)$ increases as $x$ increases, but very slowly.
        
        \item For $0 < b < 1$, $\log_b(x)$ decreases as $x$ increases.
    \end{itemize}
    
    \item Vertical Asymptote: Logarithmic functions have a vertical asymptote at $x = 0$.
    
    \item Logarithms of 1 and the Base: $\log_b(1) = 0$ and $\log_b(b) = 1$.
    
    \item The natural logarithm, written as $\ln(x)$ has the base $e$.
\end{itemize}

Key Properties:

\begin{itemize}
    \item The Product Rule: $\log_b(xy) = \log_b(x) + \log_b(y)$
    
    \item Logarithms "Break" Exponents: $\log_b(x^y) = y \cdot \log_b(x)$
    
    \item Changing Bases: $\log_b(x) = \log_a(x) / \log_a(b)$
\end{itemize}

Scaling behavior, especially in the context of data structures and algorithm efficiency, is a significant topic for any computing related fields. It concerns itself with witht he evolution of function curves as the response becomes significantly large. We use the \emph{Big O notation} to describe the behavior of a function $f(x), \quad x \rightarrow \infty$. We say that $f(x)$ is $O(g(x))$ meaning that there is some positive constant $c$ such that the upper bound for complexity growth does not increase faster than $c \cdot g(x)$ for a sufficiently large $x$. 

\begin{example}
    For example, consider $$f(x) = 6x^3 + 2x + 1,$$ we say that $f(x)$ is $O(x^3)$ as $x \rightarrow \infty$, as $X^3$ is the dominant term.  
\end{example}


Most of the common limiting functions are illustrated in the following figure: 

\includegraphics[width=\textwidth]{4_func3}

\begin{enumerate}
    \item Log-log: $g(x) = \log \log(x)$
    \begin{itemize}
        \item Exhibits extremely slow growth. Algorithms within this complexity class increase their running time at a negligible rate with input size escalation.
        
        \item Applications include specialized computational geometry problems.
    \end{itemize}
    
    \item Log: $g(x) = \log(x)$
    \begin{itemize}
        \item Denotes high efficiency. The execution time grows much slower than the input size.
        
        \item Examples include binary search in sorted arrays and operations on certain balanced tree data structures.
    \end{itemize}
    
    \item Sublinear: $g(x) = x^p, 0 < p < 1$
    \begin{itemize}
        \item Exhibits growth slower than linear but faster than logarithmic.
        
        \item Common examples include the Grover's search algorithm in quantum computing, which has a complexity of approximately $O(\sqrt{x})$, and some algorithms that utilize probabilistic methods to achieve faster-than-linear performance on average.
    \end{itemize}
    
    \item Linear: $g(x) = x$
    \begin{itemize}
        \item Indicates direct proportionality. Doubling the input size doubles the running time.
        
        \item Common examples are searching in unsorted lists and identifying max/min elements in a list.
    \end{itemize}
    
    \item Polynomial: $g(x) = x^p, p > 1$
    \begin{itemize}
        \item The growth rate is influenced by the exponent $p$. Higher values lead to rapid increases in running time with input size.
        
        \item Examples: Bubble sort and insertion sort (quadratic complexity), matrix multiplication algorithms (cubic complexity or better).
    \end{itemize}
    
    \item Poly-log: $g(x) = x^p \log(x), p \geq 1$
    \begin{itemize}
        \item Less efficient than the corresponding poly (or linear for $p = 1$) but still considered scalable.
        
        \item Fast Fourier Transform (FFT) algorithms are a prime example of algorithms with linear-log complexity. Some fast sorting algorithms also approach this performance.
    \end{itemize}
    
    \item Exponential: $g(x) = b^x, b > 1$
    \begin{itemize}
        \item Characterized by rapid growth. Algorithms in this class quickly become impractical for moderate input sizes.
        
        \item Examples: Brute-force approaches to the Traveling Salesman Problem. Currently known classical algorithms for integer factorization.
    \end{itemize}
    
    \item Factorial: $g(x) = x!$
    \begin{itemize}
        \item Exhibits extremely rapid growth, surpassing even exponential functions in rate. Practical for only very small input sizes.
        
        \item Example: Generating all permutations of a set.
    \end{itemize}
    
    \item Hyper-exponential: $g(x) = x^x, g(x) = b^{a^x}, g(x) = b^{x!}$, and $g(x) = b^{x^x}$, etc., where $a, b > 1$
    \begin{itemize}
        \item Exhibits growth that is even more rapid than factorial functions.
        
        \item Example: Modeling scenarios with extremely high growth rates, beyond combinatorial complexity.
    \end{itemize}
\end{enumerate}

\break


\section{Vectors and Vector Spaces}

Vectors and the study of vector spaces are fundamental to the study of quantum computing, where they provide the mathmatical framework for representing and manipulating quantum states, commonly used to describe superpositions, entanglement, and other phenomena integral to quantum computing. This section serves as a systematic introduction to the realm of such concepets to lay the foundation for lies ahead. 

\subsection{Real Vectors and Complex Vectors}

A vector is an ordered sequence of numbers. For now, we will focus on vectors with numeric elements. 

\begin{definition}[Ordered $n$-Tuple]
    An ordered sequence of $n$ numbers $(v_1, v_2, \ldots, v_n)$ is called an ordered $n$-tuple. 
\end{definition}

\begin{definition}[Euclidean Space]
    The set comprising all $n$-tuples is called $n$-space or Euclidean space, and denoted as $\mathbb{R}^n$ space. The complex Euclidean space is denoted by $\mathbb{C}^n$ and comprises of all complex $n$-tuples. 
\end{definition}

Now in the context of quantum computing, $n$ is typically finite as we are usually considering finite quantum systems. A real vector is in $\mathbb{R}^n$, while a complex vector is in $\mathbb{C}^n$. Much like $\{\mathbf{0}\}$, we will begin with the defiition of the zero vector.

\begin{definition}[Zero Vector]
    The zero vector, in either $\mathbb{R}^n$ or $\mathbb{C}^n$, denoted by $\{\mathbf{0}\}$, is defined as the vector where all components are zero. $$\{\mathbf{0}\} = (0, 0, \ldots, 0)$$
\end{definition}

While it is difficult to visually represent vectors in higher dimensions, vectors in $\mathbb{R}^2$ and $\mathbb{R}^3$ are easier to represent. Generally, vectors both posess magnitude and direction, conviniently representing force, velocity, and heat flow. They carry weight and direction. 

In linear algebra, vectors with identical length and direction are considered equivalent, unlike vector fields which also consider position, which means that we generally don't draw a distinction between \emph{collinearity} and \emph{parallelism}.

\includegraphics[width=\textwidth]{5_vec0}

While seemingly simple, the intuition from representing vectors in $\mathbb{R}^2$ or $\mathbb{R}^3$ extends to $\mathbb{R}^n, \quad n > 3$ and even $\mathbb{C}^n$, for which geometric applications become exceedingly difficult or near impossible. 



\subsection{Basic Vector Algebra}

A few basic operations on vectors are defined in this subsection in order to establish the foundation of vector manipulation.

\begin{definition}[Vector Equality]
    Given two vectors $\mathbf{u}$ and $\mathbf{v}$ in $\Rn$ or $\Cn$, the two vectors are equal if and only if they are equal element wise. This is also denoted as $\mathbf{u} = \mathbf{v}$.
\end{definition}

For each complex vector, there exists a unique complex conjugate that is defined as follows:

\begin{definition}[Complex Vector Conjugate]
    The complex conjugate of a complex vector $\mathbf{v} = (v_1, v_2, \ldots, v_n)$ is denoted as $\mathbf{v}^*$ and is given by: 
    $$\mathbf{v}^* = (\conj{v_1}, \conj{v_2}, \ldots, \conj{v_n})$$
    Where each element is taken to its complex conjugate.
\end{definition}

Some other common operations also exist for vectors.

\begin{definition}[Vector Sum]
    The sum of two vectors $\mathbf{u}$ and $\mathbf{v}$ in $\Rn$ or $\Cn$ is given by element-wise addition:
    $$\mathbf{u} + \mathbf{v} = (u_1 + v_1, u_2 + v_2, \ldots, u_n + v_n)$$
\end{definition}

While not immediately obvious why this requires a proof, we have 

\begin{theorem}
    Vector addition satisfies the commutative, associative, and identity properties. Mathmatically,
    $$\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$$
    $$\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}$$
    $$\mathbf{v} + \mathbf{0} = \mathbf{v}$$
\end{theorem}


\begin{definition}[Vector Negation]
    The negation of a vector $\mathbf{u}$ in $\Rn$ or $\Cn$ is defined as the element wise negation of $\mathbf{u}$, denoted as $-\mathbf{u}$ and defined as:
    $$-\mathbf{u} = (-v_1, -v_2, \ldots, -v_n)$$
\end{definition}

Which allows us to define vector subtraction. 

\begin{definition}[Vector Subtraction]
    Vector subtraction is defined as adding the negated version of the second vector, also defined as $$\mathbf{u} - \mathbf{v} = \mathbf{u}+ (-\mathbf{v})$$
\end{definition}

Scalar multiplication is also very intuitive, defined as

\begin{definition}
    Given $\mathbf{v}$ in $\Rn$ or $\Cn$ and a scalar $k$, the scalar product is denoted by $k\mathbf{u}$, is defined as:
    $$k\mathbf{v} = (k v_1, k v_2, \ldots, k v_n)$$
\end{definition}

Note that the scalar in this case will usually be from the space in which the vector $\mathbf{v}$ resides. When we combine what we just defined, we have the linear combination. 

\begin{definition}[Linear Combination]
    Consider a vector $\mathbf{w}$ and a set of $r$ vectors $\{\mathbf{v_1}, \mathbf{v_2},$ $ \ldots, \mathbf{v_r}\}$ in $\Rn$ or $\Cn$, we say that $\mathbf{w}$ is a linear combination of $\{\mathbf{v_1}, \mathbf{v_2}, \ldots, \mathbf{v_r}\}$ if $\mathbf{w}$ can be expressed as:
    $$\mathbf{w} = k_1 \mathbf{v_1} + k_2 \mathbf{v_2} + \ldots + k_r \mathbf{v_r} = \sum_{i = 1}^{r} k_i \mathbf{v_i}$$ 
    where each $k_i$ is a scalar, referred to as the \textbf{coefficients} of the linear combination.
\end{definition}

\subsection{Vector Spaces, Subspaces, and Span}

After laying the groundwork for basic vector operations, we're now focusing on vector spaces, which are a collection of vectors satisfying specific axioms. This forms a foundational concept in linear algebra, and provides a structured way to handle and manipulate dobjects in multiple dimensions. 

\begin{definition}[Vector Space]
    A vector space (or linear space) is the combination of a certain set $V$ combined with a scalar field $F$ (either in $\R$ or $\C$) such that the following two operations are defined
    \begin{enumerate}
        \item \emph{Vector Addition:} Vector addition in $V$ remain in $V$.
        \item \emph{Scalar Multiplication:} Scalar multiplications remain in $V$ for scalars from $F$. 
    \end{enumerate}
    These operations must satisfy the following properties:
    \begin{itemize}
        \item Addition is associative and commutative as defined above. Addition also contains an identity element as well as an inverse.
        \item Multiplication is distributive, both for the scalar and the vector. 
        \item $(ab)\mathbf{v} = a(b\mathbf{v})$.
        \item The multiplicative identity is $1 \in F$.    
    \end{itemize}
\end{definition}

We define item 1 and 2 from the definition as \textbf{closure under addition} and \textbf{closure under multiplication}, which are two important properties to satisfy for the whole spiel to work. Unless otherwise specified, discussions over the vector space $\Rn$ assume field $\R$, discussions over vector space $\Cn$ assume field $\C$. It is evident that $\Cn$ is also a vector space over $\R$, depending on the choice of the scalar field. in the context of quantum computing, we consistently assume that $\Cn$ is treated as a complex vector space. 

While most of these definitions may be redundant, they are nevertheless necessary in order for later definitions.

\begin{example}
    Let $V$ be a vector space over the field $F$. Given a vector $v$ in $V$ and a scalar $k \in F$, use the previous axioms to prove that $k\mathbf{0}=\mathbf{0}$.
\end{example}

\begin{proof}
    Using the distributive property for scalar multiplication, we have:
    \[
    k(\mathbf{0}+\mathbf{0})=k\mathbf{0}+k\mathbf{0}.
    \]
    From the existence of the additive identity, we know $\mathbf{0}+\mathbf{0}=\mathbf{0}$, so:
    \[
    k\mathbf{0}=k\mathbf{0}+k\mathbf{0}.
    \]
    By the additive inverse property, there exists an additive inverse $-k\mathbf{0}\in V$. Adding $-k\mathbf{0}$ to both
    sides gives:
    \[
    k\mathbf{0}+(-k\mathbf{0})=(k\mathbf{0}+k\mathbf{0})+(-k\mathbf{0}).
    \]
    The left-hand side simplifies to $\mathbf{0}$, and using the fact that addition is commutative and contains an inverse on the right-hand side gives:
    \[
    \mathbf{0} = k\mathbf{0}+[k\mathbf{0}+(-k\mathbf{0})] = k\mathbf{0}+\mathbf{0} = k\mathbf{0}.
    \]
    This completes the proof. Note that the properties of associativity and identity in addition are also used.
\end{proof}

Next we will move on to subspaces. Subspaces of vector spaces are subsets that form a vector space of its own under the same vector operations. For example, the set of vectors in $\R^2$ with a positive $x$ component violate the properties of a subspace as it is not closed when multiplied with a negative scalar. More rigorously

\begin{definition}[Subspace]
    A non-empty subset $W$ of a vector space $V$ is called a subspace of $V$ if $W$ is a vector space under the \emph{same scalar field} $F$ and the same operations of vector addition and scalar multiplication as in $V$. 
\end{definition}

While the properties listed out in the bullet points are easy to verify as those properties are inherently preserved when defining a subspace. In order to verify that a subset is a subspace, it suffices to verify whether the points 1. and 2. hold.  

\begin{theorem}
    If $W$ is a non-empty subset of a vector space $V$, then $W$ is a subspace of $V$ if and only if $W$ satisfies closure under addition and scalar multiplication.
\end{theorem}

\begin{example}
    Prove that the empty set is a subspace of $\Rn/\Cn$. 
\end{example}

\begin{proof}
    We only need to verify that addition and scalar multiplication holds in the subspace. Since $$\zerovec + \zerovec = \zerovec, \qquad k\zerovec = \zerovec$$
    Therefore, $\{\zerovec\}$ is a subspace. 
\end{proof}

\begin{definition}[Zero Subspace]
    The subset $W = \{\zerovec\}$ is called the zero subspace of a vector space $V$ where $\zerovec$ is the zero vector in $V$.
\end{definition}

An informal corrolary is that any hyperplane spanned by vectors that does not pass through the origin will not form a subspace. 

\begin{example}
    Show that $S = \{\mathbf{x} \vert \mathbf{x} = (z, \conj{z}), z \in \C\}$ is not a subspace of $\C^2$
\end{example}

\begin{proof}
    Since this is the space of the vectors spanned by a complex number $z$ and its inverse $\conj{z}$, we see that it conflicts with a scalar multiplication is defined as $$k \vvec = (kz, k \conj{z}).$$
    The second element in the vector should be the complex conjugate of the first element. However, for every $\vvec \in S$ scaled by $k \in \C$, it becomes evident that the element is no longer in the set. Mathmatically: $$\conj{(kz)} = \conj{k}\conj{z} \neq k \conj{z} \Rightarrow k \vvec \notin S.$$
    Which violates closure under multiplication and proves that $S$ is not a subspace of $\C^2$.
\end{proof}

While the previous definitions can be called foundational or elementary, we are interested to see how subspaces can be constructed using a set of vectors rather than from the top down, i.e., from the reduction of another subspace. 

\begin{theorem}
    If $S = \{\vvec_1, \vvec_2, \ldots, \vvec_r\}$ is a set of vectors from vector space $V$, then all possible linear combinations of the vectors in $S$ form a subspace of $V$. 
\end{theorem}

\begin{proof}
    Let $W$ be the set of all possible linear combinations of the vectors in $S$ and take any two vectors $\mathbf{a}, \mathbf{b} \in W$. Then we can express $\mathbf{a}$ and $\mathbf{b}$ as a linear combination of all vectors in $S$ with coefficients $a_i, b_i \in F$. Then, for any scalar $k \in F$, we see that addition and scalar multiplication hold under $S$, making $W$ a subspace of $V$.
\end{proof}

The formal definition also is as follows:

\begin{definition}[Span]
    Given a set $S = \{\vvec_1, \vvec_2, \ldots, \vvec_r\}$ from the vector space $V$, and let $W$ be the subspoace of $V$ that contains all possible linear combinations of $S$. Here, we define $W$ as the span of $S$, and write $$W = \text{span}(S) = \text{span}\{\vvec_1, \vvec_2, \ldots, \vvec_r\}.$$ In other words, S \textbf{spans} W. 
\end{definition}



\subsection{Linear Independence, Basis, and Dimension}

We commonly use basis to describe the fundamental units on the cartesian coordiante and commonly use $\mathbf{\hat{i}} = (1, 0)$ and $\mathbf{\hat{j}} = (0, 1)$ in order to form a basis for $\R^2$. It is also easy to see that any vector in $\R^2$ can be expressed as a linear combination of these two vectors. However, it is not immediately mathmatically obvious why we must chose these to vectors.  to form the basis of these vectors. 

The first prerequisite for some combination of vectors to span a space, is for them to be linearly independent. This ensures that no vectors are redundant, which brings us to an associated property of subspaces

\begin{definition}[Linear Dependence]
    Let $S = \{\vvec_1, \vvec_2, \ldots, \vvec_r\}$ be a set of vectors from a vector space $V$. The set $S$ is said to be linearly dependent if there exists scalars $c_1, c_2, \ldots, c_n$ not all zero such that $$c_1 \vvec_1 + c_2 \vvec_2 + \ldots + c_n \vvec_n = \mathbf{0}.$$ If not such scalars exists, such that $\forall c_i = 0, i \in [1, n]$, then the set is linearly independent. 
\end{definition}

A minimal set of vectors that spans a vector space $V$ is a basis. Of course, there is also a rigorous definition.

\begin{definition}[Basis]
    A set of vectors $S = \{\vvec_1, \vvec_2, \ldots, \vvec_r\}$ from a vector space $V$ forms the basis for $V$ if and only if 
    \begin{itemize}
        \item $\text{span}(S) = V$
        \item $S$ is linearly independent
    \end{itemize}
\end{definition}

These two properties together construct the following theorem:

\begin{theorem}[Uniqueness of Basis Representation]
    If $S$ is a basis of a vector space $V$, then every vector $\mathbf{w} \in V$ has a unique representation as a linear combination of $S$. 
\end{theorem}

\begin{proof}
    Assuming that $\text{span}(S) = V$ and take 
    $$\mathbf{w} = \sum_{i=1}^{n}k_i \vvec_i$$
    and another representation of the same vector, 
    $$\mathbf{w} = \sum_{i=1}^{n}m_i \vvec_i$$
    then we have 
    $$\zerovec = \sum_{i=1}^{n} (k_i - m_i) \vvec_i$$
    since $S$ is linearly independent, we must have $$k_i - m_i = 0, \quad \forall k_i, m_i \Rightarrow k_i = m_i$$
    which renders the representation unique.
\end{proof}

From this, we can see that there is a well-defined and unique set of coefficients for every vector $\mathbf{w} \in V$ expressed as a linear combination of vector set $S$, where $\text{span}(S) = V$. There is also a special term that we assign to the coefficients:

\begin{definition}[Coordinate]
    Take the vector $\wvec$ expressed in the terms of a basis $S$ for from a vector space $V$ over a field $\R/\C$:
    $$\wvec = \sum_{i=1}^{n}k_i \vvec_i$$
    The unique vector $(k_1, k_2, \ldots, k_n)$ formed from the scalar coefficients in $\Rn/\Cn$ is said to be the coordinate vector, or the coordinate of $\wvec$ relative to $S$, denoted as $$(\wvec)_S = (k_1, k_2, \ldots, k_n)$$
\end{definition}

\begin{example}
    Any vector $(a, b) \in \R^2/\C^2$ in the standard basis $\mathbf{\hat{i}} = (1, 0)$ and $\mathbf{\hat{j}} = (0, 1)$ gives 
    $$(a, b) = a(1, 0) + b(0, 1)$$ which in turn gives us the coordinates for $(a, b)$, that is 
    $$(a, b)_{\{\ibase,\ \jbase\}} = (a, b)$$
\end{example}

This example goes to show that the coordinates in a Cartesian coordinate system uses the unit vectors along the $x$ and $y$ axis as its coordinate system. While this sentence sounds like a broken record, we can see that, while we prefer to use the unit vectors as the basis vectors to represent vectors in lower dimensional space, we can use alternative axes for representing the same points in $n$-dimensional space, and we'll introduce dimensions right off the bat with a theorem:

\begin{theorem}
    For a finite-dimensional vector space, all bases posess the same number of vectors. 
\end{theorem}

While the formal proof of this theorem requires extensive background in linear systems and matrix algebra, we nevertheless wish to develop an intuitive understanding. Consider a basis $S$ for a finite-dimensional vector space $V$. If we add an extra vector to this, the set would be linearly dependent. Conversely, removing any vector would reduce the span and fail to cover all of $V$. However, this theorem confirms that the dimension of a vector space is well defined.

\begin{definition}[Dimension]
    The dimensions of a finite-dimensional vector space $V$ is the number of vectors in its basis, denoted as $\dim(V)$. 
\end{definition}

\break

\section{Inner Product Spaces}

What lays the foundation for representing $n$-qubit systems is being able to mathmatically represent them as state vectors residing in $\C^{2^n}$, which must be orthonormal basis â€“ vectors mutually orthogonal of length 1. In order to rigorously define inner product spaces, we require a well-defined inner product â€“ a mathmatical structure that generalizes length and orthogonality to higher dimensions. This, in turn, serves as the foundation for Hilbert spaces. We will also introduce dirac (or bra-ket) notation in this chapter which presents itself as an elegant method to represent quantum states and operations. 


\subsection{Dirac Notation Basics}

\begin{definition}[Matrix]
    A matrix is a rectangular array of numbers, which can be either real or complex. A matrix with $m$ rows and $n$ columns is represented as:
    \[
    A = \begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{bmatrix}_{m \times n}
    \]
    where the entries $a_{11}, a_{12}, \ldots, a_{mn}$ are called \textbf{elements}. The matrix is referred to as an $m \times n$ matrix, indicating its size.
\end{definition}

When a matrix contains only one column (or one row), it is called a column vector (or a row vector):

\begin{definition}[Column and Row Vectors]
    A matrix with only one column ($n \times 1$ in size) is called a column vector, denoted as:
    \[
    \begin{bmatrix}
    v_1 \\
    v_2 \\
    \vdots \\
    v_n
    \end{bmatrix} .
    \]

    Similarly, a matrix with only one row ($1 \times m$ in size) is called a row vector, denoted as:
    \[
    \begin{bmatrix}
    v_1 & v_2 & \cdots & v_m
    \end{bmatrix} .
    \]
\end{definition}

The process of matrix transposition, which interchanges the rows and columns of a matrix, is a fundamental operation in matrix algebra, defined as follows.

\begin{definition}[Transpose]
    Given an $m \times n$ matrix $A$ as given in Eq.~6.1, the transpose of $A$, denoted as $A^T$, is an $n \times m$ matrix defined as:
    \[
    A^T = \begin{bmatrix}
    a_{11} & a_{21} & \cdots & a_{m1} \\
    a_{12} & a_{22} & \cdots & a_{m2} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{1n} & a_{2n} & \cdots & a_{mn}
    \end{bmatrix}_{n \times m}
    \]
    resulting from interchanging the rows and columns of $A$.
\end{definition}

From here on out, vectors such as $\vvec$ will represent column vectors and the transpose thereof will represent row vectors. In dirac notation, column vectors in $\Cn$ are usually represented by a symbol called ket. 

\begin{definition}[Ket]
    Given a general vector $v = (v_1, v_2, \ldots, v_n)$ in $\mathbb{C}^n$, we use $\ket{v}$ to denote its column vector form:
    \[
    \ket{v} \equiv \begin{bmatrix}
    v_1 \\
    v_2 \\
    \vdots \\
    v_n
    \end{bmatrix}
    \]
    where $\ket{v}$ is referred to as a ket.
\end{definition}

There are a few general and commmon methods of representing a state vector ket, which include:

\begin{itemize}
    \item $\ket{\psi}$ for a general state vector;
    \item $\ket{\lambda_i}$ for eigenvectors corresponding to the eigenvalue $\lambda_i$;
    \item $\ket{j}, \ket{k}$ with $j, k \in \{0, 1, 2, \ldots, n - 1 \}$ to denote the computational basis vectors in $\Cn$.
\end{itemize}

Therea realso certain symbols reserved for specific vectors of significance, much like how we reserve $\pi$ and $e$ in common arithmatic. These include

\begin{itemize}
    \item $\ketzero, \ketone$ for the computational basis vectors in single qubit systems;
    \item $\ket{V}, \ket{H}$ for rectilinear polarization states of a photon;
    \item $\bellpsi, \bellpsim, \bellphi, \bellphim$ for Bell states of two qubits. 
\end{itemize}

the standard basis vectors of $\Cn$ are indispensable in quantum computing, so it is necessary to define the computational basis for it

\begin{definition}[Computational Basis]
For a single-qubit system in $\C^2$, the standard basis vectors, commonly referred to as the \textbf{computational basis}, are defined as the following ket vectors:
\[
\ketzero \equiv \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \quad \ketone \equiv \begin{bmatrix} 0 \\ 1 \end{bmatrix}
\]

For a one-qudit system (with $d$ distinct levels) in $\C^d$, the computational basis includes vectors:
\[
\ketzero \equiv \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}, \quad \ketone \equiv \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}, \quad \cdots, \quad \ket{d-1} \equiv \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{bmatrix}
\]
\end{definition}

Therefore, a general vector can be expressed as 

$$
\begin{bmatrix}
    1 \\ 0
\end{bmatrix}
= 
\alpha \ketzero
+
 \beta \ketone, \quad \forall \alpha, \beta \in \C
$$

We conversely also have the hermitian adjoint of any vector $\ketpsi$, which is defined as 

\begin{definition}[Hermitian Adjoint]
    Consider a column vector in $\mathbb{C}^n$:
    \[
    v = \begin{bmatrix}
    v_1 \\
    v_2 \\
    \vdots \\
    v_n
    \end{bmatrix},
    \]

    its Hermitian adjoint, or simply \textit{adjoint}, is defined as its conjugate transpose, denoted as:
    \[
    v^\dagger \equiv (v^*)^T = (v^T)^* = \begin{bmatrix} v_1^* & v_2^* & \cdots & v_n^* \end{bmatrix}.
    \]
\end{definition}

We have a convinient method of encapsulating the concept of adjoint vectors using a bra in dirac notation, defined as:

\begin{definition}[Bra]
The bra, denoted as $\bra{v}$, represents the Hermitian adjoint of $\ket{v}$, formally defined as:
\[
\bra{v} \equiv \ket{v}^\dagger.
\]
\end{definition}

\begin{theorem}
    Given vectors $\bra{u}$, $\bra{v}$ in $\Cn$ and scalars $\alpha, \beta \in \mathbb{C}$, bras exhibit the following conjugate-linear properties:
        \begin{align}
        \bra{u + v} &= \bra{u} + \bra{v}\\
        \bra{\alpha v} &= \alpha^* \bra{v}\\
        \bra{\alpha u + \beta v} &= \alpha^* \bra{u} + \beta^* \bra{v}.
        \end{align}
\end{theorem}

Note that $\brapsi$ and $\ketpsi$ are fundamentally not in the same dimension, so that operations on them will not be very straightforward. 

\subsection{Norm and Unit Vectors}

We know that the squared distance of a line can be calculated by adding the squared values of their $x$ and $y$ componenets. In three dimensional space, it means adding the squared value of the $z$ component as well. This allows us to generalize to the $n$th dimension, where we have the following, rather intuitive, definition for norm. 

\begin{definition}[Norm of a Real Vector]
    For any real vector $\vvec \in \Rn$, its length is denoted $\left\lVert \vvec \right\rVert$ and defined by: $$\left\lVert \vvec \right\rVert = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}$$
\end{definition}


\begin{theorem}
    The norm of a vector in $\Rn/\Cn$ exhibits the following properties:
    \begin{enumerate}
        \item $\norm{\vvec} \geq 0$
        \item $\vnorm = 0$ if and only if $\vvec = 0$
        \item $\norm{k \vvec} = \abs{k} \norm{\vvec}$
    \end{enumerate} 
\end{theorem}

For complex numbers, we just need to use the complex conjugate into the equation. 

\begin{definition}[Norm of a Complex Number]
    For any complex vector $\vvec \in \Cn$, the norm is defined by the formula: 
    $$\vnorm = \sqrt{v_1 \conj{v_1} + v_2 \conj{v_2} + \cdots + v_n \conj{v_n} }$$
\end{definition}

We can also derive the following theorem:

\begin{theorem}
    For a complex number $\vvec \in \C$, the following identity holds:
    $$\vnorm^2 = \abs{v_1}^2 + \abs{v_2}^2 + \cdots + \abs{v_n}^2$$
\end{theorem}

\begin{proof}
    Since we know that $\abs{z} = \sqrt{z^* z}$, we can square both sides of the equation to give us:
    $$\vnorm^2 = z_1^* z_1 + z_2^* z_2 + \cdots + z_n^* z_n = \abs{v_1}^2 + \abs{v_2}^2 + \cdots + \abs{v_n}^2$$
\end{proof}

On a side note, we have the following equivalencies for a vector in dirac notation:

$$\norm{v} \equiv \norm{\ket{v}} = \norm{\bra{v}}$$

In quantum mechanics, most of the vectors must be represented as complex vectors of norm 1. 

\begin{theorem}
    For any non-zero vector $\vvec \in \R$, the following formula will give the normalized vector in the direction of $\vvec$, denoted by $\hat{\vvec}$: $$\unitv = \frac{1}{\vnorm}\vvec$$ 
\end{theorem}

\begin{proof}
    $$
    \norm{\unitv} = \norm{\frac{1}{\vnorm}\vvec} = \abs{\frac{1}{\vnorm}}\vnorm = \frac{1}{\vnorm}\vnorm = 1
    $$
\end{proof}

You might realize by this point, that the unit vectors reside on some version of a unit hypersphere. In 2 dimensions, this would be the unit circle. Consequently, rotations in hyperspace keep the unit vector as a unit vector. Distance is defined as:

\begin{definition}[Distance]
    Given two vectors $\vvec$ and $\uvec$ in $\Rn$ or $\Cn$, the distance between $\vvec$ and $\uvec$ is defined as:
    $$d(\uvec, \vvec) = \norm{\uvec - \vvec}$$
\end{definition}

\subsection{Complex Inner Product Spaces}

A vector space will only become an inner product space when it contains the inner product as a valid operation. In $\Rn$, this operation is simply the dot product. Generally speaking, the inner product is the operation defined for a vector space $V$, scalar field $F$, as $$\left\langle \cdot, \cdot \right\rangle: V \cross V \rightarrow F $$

The more familiar form of the inner product in $\Rn$, defined as:

\begin{definition}[Real Dot Product]
    For vectors $\vvec$ and $\uvec$ in $\Rn$, their dot product is deinfed as:
    $$\uvec \cdot \vvec = u_1 v_1 + u_2 v_2 + \ldots + u_n v_n$$
    Which is also known as the Euclidian inner product.
\end{definition}


The norm of a real vector and the distance between two vectors can also be expressed as a dot product as:

\begin{theorem}
    For $\uvec, \vvec \in \Rn$, we have
    \begin{align*}
        \norm{\vvec}^2 &= \vvec \cdot \vvec \\
        \norm{\uvec - \vvec}^2 &= (\uvec - \vvec) \cdot (\uvec - \vvec)
    \end{align*}
\end{theorem}

\begin{proof}
    Write the equation out and group it together using the definition of the real dot product. 
\end{proof}

At the same time, we have a few more intuitive and convinient algebraic properties of the dot product. 

\begin{theorem}
    For $\uvec, \vvec, \wvec \in \Rn$ and $k \in \R$, we have the following algebraic identities:
    \begin{align*}
        \uvec \cdot \vvec &= \vvec \cdot \uvec &\text{(Commutative Law)} \\
        \uvec \cdot (\vvec + \wvec) &= \uvec \cdot \vvec + \uvec \cdot \wvec &\text{(Left Distributive Law)} \\
        (\uvec + \vvec) \cdot \wvec &= \uvec \cdot \wvec + \vvec \cdot \wvec &\text{(Right Distributive Law)} \\
        (k \uvec) \cdot \vvec &= k(\uvec \cdot \vvec) &\text{(Left Homogeneity Law)} \\
        \uvec \cdot (k \vvec) &= k(\uvec \cdot \vvec) &\text{(Right Homogeneity Law)}
    \end{align*}
\end{theorem}

In the complex plane, things get more interesting. 

\begin{definition}[Complex Dot Product]
    Given $\vvec, \uvec \in \Cn$, we have the complex dot product:
    $$\uvec \cdot \vvec = u_1^* v_1 + u_2^* v_2 + \cdots + u_n^* v_n = \sum_{n}^{i=1}u_i^* v_i$$
\end{definition}

Generally speaking, the complex dot product will produce a complex scalar. In some other contexts, one might find the definition of the complex dot product to have the second element be the conjugate element. However, the previous definition is in line with the conventions of quantum mechanics and computing. The use of the complex conjugate is also designed to ensure that the norm defined previously is the dot product of a complex number with itself. 

\begin{theorem}
    Given a complex number $\vvec$, it's norm can be expressed as:
    $$\vnorm = \sqrt{\vvec \cdot \vvec} \quad \Longleftrightarrow \quad \vnorm^2 = \vvec \cdot \vvec$$
\end{theorem}

Complex inner product spaces are complex vector spaces "equipped" with a complex inner prodcut. You can think of them as spaces where the dot product (a resulting scalar from two vector multiplications) is a valid operation. We like using $\Rn/\Cn$ as examples of common inner product spaces, and in quantum computing we really like $\Cn$ as it is the norm for representing quantum systems. 

\begin{definition}[Complex Inner Product Space]
    A complex inner product space is a complex vector space $V$ equipped with an inner product, defined as: $\left\langle \cdot, \cdot \right\rangle: V \cross V \rightarrow \C. $ The inner product, for $\uvec, \vvec, \wvec \in V, \quad k \in \C$, must satisfy:
    \begin{enumerate}
        \item $\innerprod{\uvec}{\vvec} = \conj{\innerprod{\vvec}{\uvec}}$
        \item $\innerprod{\uvec}{\vvec + \wvec} = \innerprod{\uvec}{\vvec} + \innerprod{\uvec}{\wvec}$
        \item $\innerprod{\uvec}{k \vvec} = k \innerprod{\uvec}{\vvec}$
        \item $\innerprod{\vvec}{\vvec} \in \R$
        \item $\innerprod{\vvec}{\vvec} = 0 \quad \Longleftrightarrow \quad \vvec = 0$
    \end{enumerate}
\end{definition}

If we put the dot product and the complex vector space together, we have:

\begin{theorem}
    $\Cn$ is a complex inner product space when the complex dot product is used as the inner product.
\end{theorem}

\begin{proof}
    Take $V \equiv \Cn$. In order to prove that the dot product qualifies as an inner product space, we need to make sure that it satisfies the properties laid out in the previous definition. 
    \begin{enumerate}
        \item $$\uvec \cdot \vvec = \sumonen \conj{u_i} v_i = \sumonen \conj{(u_i \conj{v_i})} = \conj{\left(\sumonen u_i \conj{v_i}\right)} = \conj{(\vvec \cdot \uvec)}$$
        \item $$\uvec \cdot (\vvec + \wvec) = \sumonen \conj{u_i}(v_i + w_i) = \sumonen \conj{u_i} v_i + \sumonen \conj{u_i} w_i = \uvec \cdot \vvec + \uvec \cdot \wvec$$
        \item $$\uvec \cdot (k \vvec) = \sumonen \conj{u_i} (k v_i) = k \sumonen \conj{u_i} v_i = k (\uvec \cdot \vvec)$$
        \item $$\vvec \cdot \vvec = \sumonen \conj{v_i} v_i = \sumonen \abs{v_i}^2 \geq 0, \qquad \forall \vvec$$
        \item $$\forall i \in [1, n], \qquad \abs{v_i} = 0 \quad \Rightarrow \quad v_i = 0 \quad \Rightarrow \quad \vvec = \zerovec$$
    \end{enumerate}
\end{proof}

In addition to the previously defined identities, we also have:

\begin{theorem}
    Given a complex inner product space $V$ with an inner product $\innerprod{\cdot}{\cdot}$, with $\uvec, \vvec, \wvec \in V, \quad k \in \C$, the following identities hold:
    \begin{align*}
        \innerprod{\zerovec}{\vvec} &= \innerprod{\vvec}{\zerovec} = 0 \\
        \innerprod{\uvec + \vvec}{\wvec} &= \innerprod{\uvec}{\wvec} + \innerprod{\vvec}{\wvec} \\
        \innerprod{k \uvec}{\vvec} &= \conj{k} \innerprod{\uvec}{\vvec}
    \end{align*}
\end{theorem}

\begin{proof}
    Considering that the conjugate is taken for the left-handed element in the inner product:
    \begin{align*}
        \innerprod{k \uvec}{\vvec} &= \conj{\innerprod{\vvec}{k \uvec}} \\
        &= \conj{(k \innerprod{\vvec}{\uvec})} \\ 
        &= \conj{k} \conj{\innerprod{\vvec}{\uvec}}\\
        &= \conj{k} \innerprod{\uvec}{\vvec}
    \end{align*}
\end{proof}

Now as a subset of complex inner product spaces, we have the real inner product space. 

\begin{definition}[Real Inner Product Space]
    A real inner product space is a vector space $V$ equipped with the binary function (inner product) $\left\langle \cdot, \cdot \right\rangle: V \cross V \rightarrow \R.$  It must, for all $\uvec, \vvec, \wvec \in V, \quad k \in \R$, satisfy:
    \begin{enumerate}
        \item $\innerprod{\uvec}{\vvec} = \innerprod{\vvec}{\uvec}$
        \item $\innerprod{\uvec}{\vvec + \wvec} = \innerprod{\uvec}{\vvec} + \innerprod{\uvec}{\wvec}$
        \item $\innerprod{\uvec}{k \vvec} = k \innerprod{\uvec}{\vvec}$
        \item $\innerprod{\vvec}{\vvec} \in \R$
        \item $\innerprod{\vvec}{\vvec} = 0 \quad \Longleftrightarrow \quad \vvec = 0$
    \end{enumerate}
\end{definition}

The only key difference lies in the first property. We also have the following theorem for the real inner product:

\begin{theorem}
    The following identities hold for the real inner product space:
    \begin{align*}
        \innerprod{\uvec}{k\vvec + m\wvec} &= k\innerprod{\uvec}{\vvec} + m\innerprod{\uvec}{\wvec} \\
        \innerprod{k\uvec + m\vvec}{\wvec} &= k\innerprod{\uvec}{\wvec} + m\innerprod{\vvec}{\wvec} \\
    \end{align*}
\end{theorem}

Which, conviniently, leads us to the definition for norm and distance using the inner product.

\begin{definition}[Norm and Distance]
    For any $\vvec, \uvec \in V$, the length (norm) is defined by 
    $$\vnorm = \sqrt{\innerprod{\vvec}{\vvec}}$$
    The distance between two vectors is defined by:
    $$d(\vvec, \uvec) = \norm{\vvec - \uvec} = \sqrt{\innerprod{\vvec - \uvec}{\vvec - \uvec}}$$
\end{definition}

Using dirac notation, representing inner products become much simpler and straightforward.

\begin{definition}[Product of Row and Column Vectors]
    Given a row vector $\mathbf{r}$ and a column vector $\mathbf{c}$ in $\Cn$:
    $$\mathbf{r} = 
    \begin{bmatrix}
    r_1 &
    r_2 &
    \cdots &
    r_n
    \end{bmatrix}, \quad \mathbf{c} = 
    \begin{bmatrix}
    c_1 \\ c_2 \\ \vdots \\ c_m
    \end{bmatrix}
    $$
    their matrix product is defined as:
    $$\mathbf{rc} = \sumonen r_i c_i$$
\end{definition}

Now, noting that we can simplify the product betweenn a vector $\ketphi$ and a hermitian adjoint ($\psi^\dagger = \brapsi$) as $\brapsi \ketphi \equiv \braket{\psi}{\phi}$, we have the following definition:

\begin{definition}[Dirac Notation of Inner Product]
    Given two vectors $\ket{u} = \uvec, \ket{v} = \vvec$, we have the inner product defined as:
    $$\braket{u}{v} \equiv \innerprod{\uvec}{\vvec} = \uvec \cdot \vvec.$$
\end{definition}

For convinience, we have the following theorem:

\begin{theorem}
    For vectors $\ket{u}, \ket{v}, \ket{w} \in \Cn, \alpha \in \C$, the following identities hold:
    \begin{align*}
        \braket{u}{v} &= \conj{\braket{v}{u}} \\
        \braket{u + v}{w} &= \braket{u}{w} + \braket{v}{w} \\
        \braket{u}{v + w} &= \braket{u}{v} + \braket{u}{w} \\ 
        \braket{u}{\alpha v} &= \alpha \braket{u}{v} \\
        \braket{\alpha u}{v} &= \conj{\alpha} \braket{u}{v} \\
        \vnorm^2 &= \braket{v}{v} \\
    \end{align*}
\end{theorem}

We also have the following theorem for any unit vector:

\begin{theorem}
    Any unit vector $\ketphi$ satisfies:
    $$\braket{\phi}{\phi} = 1$$
\end{theorem}

\begin{proof}
    Since $\ketphi$ is a unit vector, $\norm{\phi} = 1$, which means that $\braket{\phi}{\phi} = \norm{\phi}^2 = 1$
\end{proof}

There are a host of computational examples that can be found in the textbook and it is highly recommended to look over them and compute for yourself. Now it is worth mentioning a quick note on \textbf{Hilbert Spaces}, which is the mathmatical framework under which quantum states reside. While such spaces are often infinite-dimensional, the complex inner product spaces $\Cn$ are often special cases of finite-dimensional Hilbert spaces. 

\subsubsection*{Infinite-Dimensional Complex Vector Spaces $\C^\infty$}

We first start with $\C^\infty$ where columns consist of infinite sequences of complex numbers. This forms the foundation for infinite dimensional Hilbert spaces. An inner product here is defined as:

$$\innerprod{\uvec}{\vvec} = \sumoneinf \conj{u_i} v_i$$

Notably, for the inner product to be well defined, the series must converge, which leads to the concept of square-summable sequences ($l^2$).

\subsubsection*{Functional Inner Spaces}

Functional Spaces, which are defined as the spaces of functions, consist of functions with specific properties. If we equip them with an inner product, these functional spaces become (you guessed it) functional inner spaces. For the inner product of functions $f(x), g(x)$ over the interval $[a, b]$, we have:

$$\innerprod{f}{g} = \int_{a}^{b} \conj{f}(x) g(x)\,dx. $$

One of the most important examples of functional inner spaces is $L^2([a, b])$, is the space of all square-intergrable functions on $[a, b]$, defined as:

$$L^2([a, b]) = \left\{f: [a, b] \longrightarrow \C \vert \int_{a}^{b} \abs{f(x)}^2 \, dx < \infty \right\}$$

The corresponding norm, (unironically) called the $L^2$ norm, is defined as:

$$\norm{f} = \sqrt{\int_{a}^{b} \abs{f(x)}^2 \,dx }$$

One will see later how concepts like orthogonality and projection naturally extends to functions. In the textbook, there is a brief tangent that connects this to the fourier series $\phi_k(x)$, which will not be covered at this time. 

\subsubsection*{Hilbert Space}

Building on the previous concepts of $\Cn$ and functional spaces, we are ready to define Hilber spaces. These are generalization of inner product spaces that is \textit{complete}. 

\begin{definition}[Hilbert Space]
    A Hilber space is an \textbf{inner product space} that is \textit{complete} with respect to the norm induced by its inner product. Completeness means that every Cauchy sequence in the space converges to a point within the space. 
\end{definition}

So what is a cauchy sequence? It is what Augustin Cauchy proposed as the solution to the question: how do you decide when to consider a sequence as convergent?

\begin{definition}[Cauchy Sequence]
    A $\left\{s_n\right\} = (s_1, s_2, \ldots)$ is called a \textbf{Cauchy Sequence} if:
    $$\forall N \in \N, \quad \epsilon > 0, \quad n > N \qquad \Longrightarrow \qquad \abs{s_n - s_{n+1}} < \epsilon$$
    \begin{center}
        \includegraphics[width=200px]{6_cauchy}
    \end{center}
\end{definition}

In relation to the Hilbert space, it means that every sequence in the Hilbert space will converge to a point in the Hilbert space, that it is complete. In the context of quantum computing and representing information, the $n$-qubit state space is a finite system constructed from an inner product space with dimension $2^n$. This is formalized by the following theorem:

\begin{theorem}
    Every finite-dimensional real or complex inner product space is a Hilbert space.
\end{theorem}


In quantum mechanics, Hilbert spaces will often extend to infinite dimensions. It ensures that any linear combination or superposition remains a valid quantum state, providing consistency to the space. In other words, this space is \textbf{complete}.

\subsection{Orthogonality and Projection}

We define orthogonality using the inner product, where, in close conjunction with the norm and establishing orthonormal bases, is a key concept. It entertains a close relationship with the projection problem, playing a crucial role in determining the measurement probabilities for different outcome states. From a geometric standpoint, the angle between two non-zero vectors in $\R^2/\R^3$ is defined as the intuitive angle between the two vectors. 

When we talk about the dot product, it is closely related to the two. Applying the law of cosines:

$$
    \norm{\uvec - \vvec}^2 = \unorm^2 + \vnorm^2 - 2 \unorm \vnorm \cos \theta
$$

and rearranging this equation gives us:

$$
    \unorm \vnorm \cos \theta = \frac{1}{2} (\unorm^2 + \vnorm^2 - \norm{\uvec - \vvec}^2 )
$$

We can see that the right hand side is equal to the dot product of $\uvec$ and $\vvec$, bringing us to the following theorem and definition.

\begin{center}
    \includegraphics[width=200px]{6_dot_prod_angle}
\end{center}

\begin{theorem}
    For vectors $\uvec, \vvec \in \R^2/\R^3$, we have the following identity:
    
    $$\uvec \cdot \vvec = \unorm \vnorm \cos \theta$$

    where $\theta$ is the geometric angle between $\uvec$ and $\vvec$.
\end{theorem}

In fact, while we say that this is true for $\uvec, \vvec \in \R^2/\R^3$, it is only because we mention that this is for the geometric angle $\theta$. It is very difficult to visualize $\theta$ in hyperspace, yet the following general definition holds for non-zero vectors in $\Rn$. 

\begin{definition}[Angle Between Two Real Vectors]
    For any $\uvec, \vvec \in \R^n$, the angle is said to lie within the interval [$0, \pi$] and is given by:
    $$\cos \theta = \frac{\uvec \cdot \vvec}{\unorm \vnorm}$$
\end{definition}

Note that for the previous definition to hold, the computed cosine value must lie within the interval of $[-1, 1]$. This condition is guaranteed to be satisfied given the \textbf{Cauchy-Schwarz Inequality}, stating that:

$$ - \unorm \vnorm \leq \vvec \cdot \uvec \leq \unorm \vnorm $$


For complex numbers, we say that $\uvec \cdot \vvec$ is generally a complex number. As a result, the previous definition for $\cos \theta$ no longer holds since the right side is generally complex. However, the condition $\vvec \cdot \uvec = 0$ still holds regardless and remains of great use even for complex vectors. 

\begin{definition}[Orthogonality in $\Cn$]
    Given two vectors $\uvec, \vvec \in \Cn$, we say that $\uvec$ and $\vvec$ are orthogonal if $\vvec \cdot \uvec = \uvec \cdot \vvec = 0$.
\end{definition}

\begin{definition}[Orthogonality in Inner Product Spaces]
    Generally, for $\uvec, \vvec \in V$, where $V$ is an inner product space, we say that $\uvec$ and $\vvec$ are orthogonal when $\innerprod{\uvec}{\vvec} = \innerprod{\vvec}{\uvec} = 0$
\end{definition}

\begin{definition}[Generalized Pythagorean Theorem]
    For $\uvec, \vvec \in \Cn$, if $\uvec$ and $\vvec$ are orthognal, then 
    $$\vnorm^2 + \unorm^2 = \norm{\uvec + \vvec}^2$$
\end{definition}

\begin{proof}
    Since we know that orthogonal vectors $\uvec, \vvec$ have the property $\innerprod{\uvec}{\vvec} = 0$, then:
    $$\innerprod{\uvec}{\vvec} = (\uvec + \vvec) \cdot (\uvec + \vvec) = \unorm^2 + \uvec \cdot \vvec + \vvec \cdot \uvec + \vnorm^2 =  \unorm^2 + \vnorm^2$$
\end{proof}

There is an intriguing property of orthogonal vectors that can be illustrated with the following example:

\begin{example}
    Given $\uvec = (1, 0)$, find a unit vector $\vvec \in \C^2$ that is orthogonal to $\uvec$. If we take $\vvec = (v_1, v_2)$, we know that $\innerprod{\uvec}{\vvec} = 0$, which means that $v_1 = 0$, and it will not matter what $v_2$ is equal to, as long as it is of length one. Therfore, we have
    $$\vvec = (0, e^{i \phi}), \quad \phi \in \R$$
\end{example}

This example illustrates a simple concept about complex vectors, whcih is that there exists an infinite number of unit vectors that are orthognoal to $(1, 0)$ in $\C^2$. This comes from the fact that multiplying, or scaling, by a factor $i \phi$ only maintains the magnitude of a complex number $z$. Therefore, we have the the following theorem:

\begin{theorem}
    Given two unit vectors $\mathbf{z}, \wvec \in \Cn$ that are orthogonal, there are an infinitely many unit vecros in the span of $\wvec$ which are ortogonal to $\mathbf{z}$ in the form of $e^{i \phi}\wvec, \phi \in \R$. 
\end{theorem}

\begin{proof}
    We rewrite $\wvec^\prime = e^{i \phi}\wvec, \phi \in \R$ as $\wvec^\prime = \lambda \wvec, \lambda \in \C$. The orthogonality operation then becomes:
    $$\mathbf{z} \cdot \wvec^\prime = \mathbf{z} \cdot (\lambda \wvec) = \lambda (\mathbf{z} \cdot \wvec) = \lambda \zerovec = \zerovec$$
    Furthermore, since $\norm{\wvec^\prime} = 1$, we see that:
    $$\norm{\wvec^\prime} = \norm{\lambda \wvec} = \abs{\lambda} \cdot \norm{\wvec} = \abs{\lambda}$$
    Implying that $\lambda$ is of the form $e^{i \phi}$.
\end{proof}

Since quantum states are represented by unit vectors in $\Cn$, we can see that multiplying any unit vector by a factor of $e^{i \phi}$ preserves its modulus, hence preserving it as a unit vector. We therefore call these vectors \textbf{global phase factors}, which highlights its effect across the quantum state. 

\begin{theorem}
    Given two orthogonal vectors $\ketphi, \ketpsi \in \Cn$, they satisfy:
    $$\braket{\phi}{\psi} = \braket{\psi}{\phi} = 0$$
\end{theorem}

Orthogonaliy is closely related to the concept of projections in the sense that it is commonly known as an orthogonal projection. An example of this in $\R^2$ is illustrated by the dashed line in the figure below. 

\begin{center}
    \includegraphics[width=\textwidth]{6_2d_proj}
\end{center}

Once you understand this conceptually in two dimensions, it becomes easier to see how $\mathbf{a}$ can be extended to a plane in three dimensions, and so on, bringing us to the general projection theorem in $\Rn/\Cn$. 

\begin{theorem}[Projection Theorem]
    Given $\vvec, \mathbf{a} \in \Rn/\Cn$, there is an \textit{unique} way to decompose $\vvec$ as $\vvec = \wvec + \wvec^\prime$ where $\wvec$ is in the span of $\mathbf{a}$ and $\wvec^\prime$ is orthogonal to $\mathbf{a}$. 

    Here, we denote $\wvec$ as the \textbf{orthogonal projection of $\vvec$ onto $\mathbf{a}$} and denote it as $\text{proj}_{\mathbf{a}}{\vvec}$ while $\wvec^\prime$ as the \textbf{complement of $\vvec$ orthogonal to $\mathbf{a}$}, denoted as $\vvec_{\perp \mathbf{a}}$. They are given by the following formulas:
    \begin{align*}
        \text{proj}_{\mathbf{a}}{\vvec} = \vvec_{\Vert \mathbf{a}} &= \frac{\mathbf{a} \cdot \vvec}{\norm{\mathbf{a}}^2}\mathbf{a} \\
        \vvec_{\perp \mathbf{a}} &= \vvec - \text{proj}_{\mathbf{a}}{\vvec} \\
    \end{align*}
\end{theorem}

\begin{proof}
    Since $\wvec$ is in the span of the vector $\mathbf{a}$, it can be writen as a scaled version of $\mathbf{a}$, i.e. $\wvec = \lambda \mathbf{a}, \lambda \in \C$. It is thus possible to expand the equation into:
    $$\mathbf{a} \cdot \vvec = \mathbf{a} \cdot (\wvec + \wvec^\prime) = \mathbf{a} \cdot \wvec + \mathbf{a} \cdot \wvec^\prime = \mathbf{a} \cdot \wvec = \mathbf{a} \cdot (\lambda \mathbf{a}) = \lambda (\mathbf{a} \cdot \mathbf{a})$$
    Solving for $\lambda$ gives us:
    $$\lambda = \frac{\mathbf{a} \cdot \vvec}{\mathbf{a} \cdot \mathbf{a}} = \frac{\mathbf{a} \cdot \vvec}{\norm{\mathbf{a}}^2}$$
    We therefore have:
    $$\wvec = \lambda \mathbf{a} = \frac{\mathbf{a} \cdot \vvec}{\norm{\mathbf{a}}^2} \mathbf{a}, \qquad \wvec^\prime = \vvec - \wvec = \vvec - \frac{\mathbf{a} \cdot \vvec}{\norm{\mathbf{a}}^2} \mathbf{a}$$
\end{proof}

Since in quantum computing the state vectors are unit vectors, the projection between unit complex vectors are of vital importance. We thus have:

\begin{theorem}[Projection Theorem Between Complex Unit Vectors]
    Consider $\ketphi, \ketpsi \in \Cn$. The projection of $\ketphi$ onto $\ketpsi$ is given by the formula:
    $$\text{proj}_{\ketpsi} \ketphi = \braket{\psi}{\phi} \ketpsi$$
    where $\abs{\braket{\psi}{\phi}} \leq 1$.
\end{theorem}

Now in order to prove this, we need an adaptation of the Cauchy-Schwarz Inequality for Dirac notation$$\abs{\braket{\phi}{\psi}}^2 \leq \braket{\phi}{\psi} \cdot \braket{\psi}{\psi} \quad \Longleftrightarrow \quad \abs{\innerprod{\phi}{\psi}} \leq \norm{\psi} \norm{\phi}$$.


\begin{proof}
    If we rewrite this equation into Dirac notation, it gives us 
    $$\text{proj}_{\ketphi} \ketpsi = \frac{\braket{\psi}{\phi}}{\braket{\phi}{\phi}} \ketpsi$$
    Since $\ketphi$ is a unit vector, $\braket{\phi}{\phi} = \norm{\phi}^2 = 1$, simplifying to the equation in the theorem above. 
    In order to prove that $\braket{\phi}{\psi} \leq 1$, we apply the Cauchy-Schwarz inequality and get:
    $$\abs{\braket{\phi}{\psi}}^2 \leq \braket{\phi}{\phi} \braket{\psi}{\psi} = 1$$
    since both $\braket{\phi}{\phi} = \braket{\psi}{\psi} = 1$.
\end{proof}

This is important for properties such as the Born Rule, which states that upon measuring an obvservable $M$ on a quantum state $\ketpsi$, the probability of obtaining each eigenvalue $\lambda_i$ is given by $\abs{\braket{\lambda_i}{\psi}}^2$, where $\ket{\lambda_i}$ is the corresponding eigenvector. Since the aforementioned theorem ensures that $\abs{\braket{\lambda_i}{\psi}}^2 \leq 1$, we can ensure that it is a valid probability. It is now necessary to reintroduce the general form of the following theorem:

\begin{theorem}[Cauchy-Schwarz Inequality]
    Consider any two vectors $\uvec, \vvec \in V$, where $V$ is an inner product space.Then we have the following inequality:
    $$\abs{\innerprod{\uvec}{\vvec}}^2 = \leq \innerprod{\uvec}{\uvec} \innerprod{\vvec}{\vvec}.$$ 
\end{theorem}

This theorem can be adapted into several other forms for various inner product spaces, such as $\Rn$, $\Cn$, etc. 

\subsection{Orthonormal Bases}

We know that bases of vector spaces are a set of linearly independent vectors that span a given space. In this subsection, we will extend this generalization to inner product spaces (which have additional properties such as norm and orthogonality). In these spaces, orthonormal bases are formed by sets of vectors that are both orthognoal and normalized, are an intuitive and funcamental concept for both quantum mechanics and quantum computing, simplifying the representation of quantum states and illustrating key concepts such as superposition, entanglement, state evoluytion, and measurement. Here, Dirac notation is the norm. 

\begin{definition}[Orthonormal Basis]
    A basis $S = \left\{\ket{b_1}, \ket{b_2}, \ldots, \ket{b_n}\right\}$ for an $n$-dimensional complex inner product space $V$ is said to be orthonormal when they have norm $1$ and are all orthogonal to each other. That is:
    $$\norm{b_1} = \norm{b_2} = \ldots = \norm{b_n} = 1$$
    $$\braket{b_i}{b_j} = 0 \quad \forall i \neq j$$
\end{definition}

This is very much intuitive and analogous to the definition of any orthonormal basis. However, extending this to complex inner product spaces, we have the following theorem:

\begin{theorem}[Fundamental Property of Orthonormal Bases]
    Given a basis $S$of a complex inner product space, $S$ is orthonormal if and only if all inner products between pairs of basis vectors satisfy:
    $$
    \braket{b_i}{b_j} = \delta_{ij}, \qquad \delta_{ij} = \begin{cases}
        0 \quad i \neq j \\
        1 \quad i = j
    \end{cases}
    $$
\end{theorem}

When expressing any unit vector in terms of orthonormal basses, the following key property holds true.

\begin{theorem}
    Let $\ketpsi \in \Cn$ be a \textit{unit vector}, and take a set of orthonormal basis $\left\{\ket{b_i}\right\}$. Suppose we can express $\ketpsi$ as a linear combination of the basis vectors as:
    $$\ketpsi = c_1 \ket{b_1} + c_2 \ket{b_2} + \cdots + c_n \ket{b_n}$$
    with $\left\{c_i\right\} \in \C$, then we have the squared magnitudes of the coefficients satisfying the following constraint:
    $$\sum_{i=1}^{n} \abs{c_i}^2 = 1$$ 
\end{theorem}

\begin{proof}
    Using rules of the complex dot product and the properties of an orthonormal basis, we can see that this holds:
    \begin{align}
        \braket{\psi}{\psi} &= \left(c_1^* \bra{b_1} + c_2^* \bra{b_2} + \cdots c_n^* \bra{b_n}\right) \left(c_1 \ket{b_1} + c_2 \ket{b_2} + \ldots c_n \ket{b_n}\right) \\
        &= \sum_{i=1}^{n} c_i^* c_i \braket{b_i}{b_i} + \sum_{i \neq j} c_i^* c_j \braket{b_i}{b_j} \\
        &= \sum_{i=1}^{n} c_i^* c_i 1 + \sum_{i \neq j} c_i^* c_j \mathbf{0} \\
        &= \sum_{i=1}^{n} \abs{c_i}^2
    \end{align}
    Since $\ketpsi$ is a unit vector, $\braket{\psi}{\psi} = \norm{\psi}^2 = 1$, so $\braket{\psi}{\psi} = \sum_{i=1}^{n} \abs{c_i}^2 = 1$.
\end{proof}

In quantum computing, we would take $\ketphi$ to be a \textbf{superposition state} within the basis $\left\{\ket{b_i}\right\}$, and when measured, the probability that this quantum state would reduce itself to $\left\{\ket{b_i}\right\}$ is $\abs{c_i}^2$. The previous theorem simply proves that the summation of these states is equal to one, consistent with the definition of probability. Now note that, starting at this point in the notes, that \textbf{all complex vectors expressed in Dirac notation are unit vectors}, unless stated otherwise. This is standard practice for quantum computing literature. 

In quantum computing, state vectors $\ketphi$ are typically expressed as superpositions within an orthonormal basis, rather than the standard basis themselves. For example, take the state $\ketplus = \frac{1}{\sqrt{2}}(\ketzero + \ketone)$, rather than taking the matrix form $\ketplus = \left[\frac{1}{\sqrt{2}} \quad \frac{1}{\sqrt{2}}\right]^T$. This greatly simplifies operations such as outer and tensor products, introduced in the later chapters. 

\begin{theorem}
    Given $\ketpsi$ and an orthonormal basis $S$ in an $n$-dimensional complex inner product space, $\ketpsi$ can be decomposed as a superposition of basis vectors:
    $$\ketpsi = c_1 \ket{b_1} + c_2 \ket{b_2} + \cdots + c_n \ket{b_n}$$
    where each coefficient $c_i$ is given by:
    $$c_i = \braket{b_i}{\psi}$$
\end{theorem}

\begin{proof}
    Since we are expressing $\ketpsi$ as a linear combination of the basis vectors $\left\{\ket{b_i}\right\}$, 
    $$\ketpsi = c_1 \ket{b_1} + c_2 \ket{b_2} + \cdots + c_n \ket{b_n}$$
    then in order to find $c_i$, we only need to take 
    \begin{align*}
        \braket{b_i}{\psi} &= \bra{b_i}(c_1 \ket{b_1} + c_2 \ket{b_2} + \cdots + c_n \ket{b_n}) \\
        &= c_1 \braket{b_i}{b_1} + c_2 + \cdots + c_n \braket{b_i}{b_n} \\
        &= c_1 \cdot 1 + \sum_{i \neq j} c_j \cdot 0 \\
        &= c_i \\
    \end{align*}
\end{proof}

This can also be rewritten as 

$$\ketpsi = \braket{b_1}{\psi} \ket{b_1} + \braket{b_2}{\psi} \ket{b_2} + \cdots + \braket{b_n}{\psi} \ket{b_n} = \sum_{i=1}^{n} \braket{b_i}{\psi} \ket{b_i}$$

where each component $\braket{b_i}{\phi} \ket{b_i}$ is just the projection of $\ketphi$ onto $\ket{b_i}$ or $\projection{\ket{b_i}}{\ketpsi}$.

\begin{center}
    \includegraphics[width=\textwidth]{6_orthonorm_decomp}
\end{center}

Now we know that generally vectors can be decomposed into some naturally existing subspaces and vectors defined in real linear algebra. This translates well to complex inner product spaces, where $c_i = \braket{b_i}{\psi}$ represents the $i$-th coordinate of $\ketpsi$ relative to the orthonormal basis $S = \left\{\ket{b_i}\right\}$, and obtained by projecting $\ketpsi$ onto the corresponding basis vector $\ket{b_i}$. 

We know that all finite-dimensional inner product spaces have an orthonormal basis. Actually, they have infinite orthonormal bases, and in some situations it is critical to create an orthonormal basis from a non-orthonormal one. The gram-schmidt process defines a methodical algorithm for htis task, which is especially beneficial when working with degenerate eigenvalues.

In order to fully understand this processs, we must first introduce a few theorems that lay the foundation for this. 

\begin{theorem}[Basis from Linearly Independent Vectors]
    Given a set $S$ with exactly $n$ vectors from a vector space $V$ with $\dim(V) = n$, $S$ forms a basis for $V$ if and only if $S$ is linearly independent. 
\end{theorem}

\begin{proof}
    Assume that $S$ forms a basis yet is linearly independent. Then it must be possible to remove at least one vector from $S$ without changing it's span. However, this means that the number of vectors will now be strictly less than $n$, which is in direct contradiction to the previous theorem that states the minimum number of vectors required to span a space is equal to its dimension. Therefore, $S$ must be linearly independent. 
\end{proof}

\begin{theorem}[Orthogonal Basis]
    Take $S = \left\{\vvec_1, \vvec_2, \ldots, \vvec_n\right\}$ is a set of nonzero vectors in an inner product space $V, \dim(V) = n$, and that these vectors are orthogonal to each other s.t. $\braket{\vvec_i}{\vvec_j} = 0, \forall i \neq j$. Thus, $S$ constitues an orthogonal basis of $V$. 
\end{theorem}

\begin{proof}
    Since $S$ is already defined as an orthogonal basis, we only need to demonstrate that $S$ is linearly independent. Consider the following equation:
    $$k_1 \vvec_1 + k_2 \vvec_2 + \cdots + k_n \vvec_n = \zerovec$$
    If we take the left dot product for each $\vvec_i$ with this equation, then we have 
    \begin{align*}
        \braket{\vvec_i}{k_1 \vvec_1 + k_2 \vvec_2 + \cdots + k_n \vvec_n} &= k_1 \braket{\vvec_i}{\vvec_1} + k_2 \braket{\vvec_i}{\vvec_2} + \cdots + k_n \braket{\vvec_i}{\vvec_n} \\
        &= k_i \braket{\vvec_i}{\vvec_i} \\
    \end{align*}
    We are left with:
    $$k_i \braket{\vvec_i}{\vvec_i} = 0$$
    Since $\braket{\vvec_i}{\vvec_i} = \norm{\vvec_i}^2 \neq 0$ for all non-zero vectors, it follows that $k_i = 0$. Hence, $S$ is linearly independent.
\end{proof}

Normalizing each basis vector in an orthogonal basis naturally leads to an orthonormal basis. This transformation is formalized in:

\begin{theorem}
    Given an orthogonal basis $S$ for an inner product space $V$, an orthonormal basis can be obtained by normalizing each basis vector in $S$:
    $$\hat{\vvec_1} = \frac{\vvec_1}{\norm{\vvec_1}}, \hat{\vvec_2} = \frac{\vvec_2}{\norm{\vvec_2}}, \ldots, \hat{\vvec_n} = \frac{\vvec_n}{\norm{\vvec_n}}$$
\end{theorem}

The Gram Schmidt process, 

\begin{theorem}[The Gram-Schmidt Process]
    The following computational steps, known as the Gram-Schmidt process, transform any basis \(\{\uvec_1, \uvec_2, \ldots, \uvec_n\}\) of an inner product space \(V\) into an orthogonal basis \(\{\vvec_1, \vvec_2, \ldots, \vvec_n\}\):

        \begin{enumerate}
            \item \(\vvec_1 = \uvec_1\)
            
            \item \(\vvec_2 = \uvec_2 - \projection{\vvec_1}{\uvec_2}\)
            
            \item \(\vvec_3 = \uvec_3 - \projection{\vvec_1}{\uvec_3} - \projection{\vvec_2}{\uvec_3}\)
            
            \item \(\vvec_4 = \uvec_4 - \projection{\vvec_1}{\uvec_4} - \projection{\vvec_2}{\uvec_4} - \projection{\vvec_3}{\uvec_4}\)
             
            \item[n.] \(\displaystyle \vvec_n = \uvec_n - \sum_{i=1}^{n-1} \projection{\vvec_i}{\uvec_n}\)
        \end{enumerate}
\end{theorem}

\begin{proof}
    In this proof, we utilize the complex dot product as the inner product. Given that \(\{\uvec_1, \uvec_2, \ldots, \uvec_n\}\) is a basis for \(V\), and hence \(\dim(V) = n\), we aim to demonstrate that \(\{\vvec_1, \vvec_2, \ldots, \vvec_n\}\) forms an orthogonal basis. This follows from the previous theorem where we showed that \(\vvec_i \cdot \vvec_j = 0, \forall i \neq j\).

    We establish the following relationships: that the inner product of a vector with another one onto itself is just the dot product with that vector, and the dot product of two vectors that are already orthogonal, then the projection of another vector onto the orthogonal vector is simply zero. Mathmatically:

    \begin{align}
        \vvec_i \cdot \projection{\vvec_i}{\uvec_j} &= \vvec_i \cdot \left(\frac{\vvec_i \cdot \uvec_j}{\vvec_i \cdot \vvec_i}\vvec_i\right) = \frac{\vvec_i \cdot \uvec_j}{\vvec_i \cdot \vvec_i}\vvec_i \cdot \vvec_i = \vvec_i \cdot \uvec_j,  \\
        \vvec_i \cdot \projection{\vvec_k}{\uvec_j} &= \vvec_i \cdot \left(\frac{\vvec_k \cdot \uvec_j}{\vvec_k \cdot \vvec_k}\vvec_k\right) = \frac{\vvec_k \cdot \uvec_j}{\vvec_k \cdot \vvec_k}\vvec_i \cdot \vvec_k = 0 \quad \text{if } \vvec_i \cdot \vvec_k = 0.
    \end{align}

    With these formulas, we proceed with the proof using mathematical induction.

    \textbf{Base Case:}

    For \(n = 2\), the vectors \(\vvec_1\) and \(\vvec_2\) are given by:
    \[
    \vvec_1 = \uvec_1, \quad \vvec_2 = \uvec_2 - \projection{\vvec_1}{\uvec_2}.
    \]

    Compute the inner product \(\vvec_1 \cdot \vvec_2\):
    \begin{align*}
        \vvec_1 \cdot \vvec_2 &= \vvec_1 \cdot (\uvec_2 - \projection{\vvec_1}{\uvec_2}) \\
        &= \vvec_1 \cdot \uvec_2 - \vvec_1 \cdot \projection{\vvec_1}{\uvec_2} \\
        &= \vvec_1 \cdot \uvec_2 - \vvec_1 \cdot \uvec_2 \\
        &= 0.
    \end{align*}

    Thus, \(\vvec_1\) and \(\vvec_2\) are orthogonal.

    \textbf{Inductive Step:}

    Assume that \(\vvec_1, \vvec_2, \ldots, \vvec_k\) are mutually orthogonal. We prove that \(\vvec_{k+1}\) is orthogonal to all previous \(\vvec_i\) for \(i = 1, 2, \ldots, k\). By definition:
    \[
        \vvec_{k+1} = \uvec_{k+1} - \sum_{i=1}^{k} \projection{\vvec_i}{\uvec_{k+1}}.
    \]

    Compute the inner product \(\vvec_j \cdot \vvec_{k+1}\) for \(j = 1, 2, \ldots, k\):
    \begin{align*}
        \vvec_j \cdot \vvec_{k+1} &= \vvec_j \cdot \left(\uvec_{k+1} - \sum_{i=1}^{k} \projection{\vvec_i}{\uvec_{k+1}}\right) \\
        &= \vvec_j \cdot \uvec_{k+1} - \sum_{i=1}^{k} \vvec_j \cdot \projection{\vvec_i}{\uvec_{k+1}}.
    \end{align*}

    For \(i \neq j\), \(\vvec_j \cdot \projection{\vvec_i}{\uvec_{k+1}} = 0\) because \(\vvec_j \cdot \vvec_i = 0\) by the induction hypothesis. For \(i = j\):
    \[
    \vvec_j \cdot \projection{\vvec_j}{\uvec_{k+1}} = \vvec_j \cdot \uvec_{k+1}.
    \]

    Thus:
    \[
    \vvec_j \cdot \vvec_{k+1} = \vvec_j \cdot \uvec_{k+1} - \vvec_j \cdot \uvec_{k+1} = 0.
    \]

    By induction, \(\vvec_1, \vvec_2, \ldots, \vvec_{k+1}\) are mutually orthogonal.
\end{proof}


An illustration of the Gram-Schmidt Process in $\R^2$ and $\R^3$ is depicted in the above figure a few pages back. We can generally define the projection of a vector onto a subspace as:

\begin{definition}[Projection]
    Let $V$ be an inner product space and $\uvec \in V$, and take $W \subseteq V$. If $\left\{\vvec_1, \vvec_2, \ldots, \vvec_n\right\}$ is an orthogonal basis for $W$, then we define $\projection{W}{\uvec}$ as:
    $$\projection{W}{\uvec} = \sum_{i=1}^{r}\projection{\vvec_i}{\uvec} = \sum_{i=1}^{r} \frac{\innerprod{\vvec_i}{\uvec}}{\innerprod{\vvec_i}{\vvec_i}} \vvec_i$$
    The component of $\uvec$ orthogonal to $W$, denoted by $\projection{W^\bot}{\uvec}$, is:
    $$\projection{W^\bot}{\uvec} = \uvec = \projection{W}{\uvec}$$
\end{definition}

Here, we take $W^\bot$ to be the \textbf{orthogonal complement} of $W$, where the vectors in $W^\bot$ are orthogonal to every vector in $W$. 

\begin{center}
    \includegraphics[width=\textwidth]{6_gram_schmidt}
\end{center}

Following the previous definition, the Gram-Schmidt Process can be succinctly summarized:

\begin{enumerate}
    \item \(\vvec_1 = \uvec_1\),
    
    \item \(\vvec_2 = \text{proj}_{W_1^{\perp}} \uvec_2\), where \(W_1 = \text{span}\{\vvec_1\}\),
    
    \item \(\vvec_3 = \text{proj}_{W_2^{\perp}} \uvec_3\), where \(W_2 = \text{span}\{\vvec_1, \vvec_2\}\),
    
    \item \(\vvec_4 = \text{proj}_{W_3^{\perp}} \uvec_4\), where \(W_3 = \text{span}\{\vvec_1, \vvec_2, \vvec_3\}\),
    
    \vdots
    
    \item[n.] \(\vvec_n = \text{proj}_{W_{n-1}^{\perp}} \uvec_n\), where \(W_{n-1} = \text{span}\{\vvec_1, \vvec_2, \ldots, \vvec_{n-1}\}\),
\end{enumerate}

where \(W_i\) represents the subspace spanned by \(\vvec_1, \vvec_2, \ldots, \vvec_i\).

\subsection*{Practical Implementation in Python}

Standard "textbook" implementations of Gram-Schmidt often fail in quantum computing contexts because they do not correctly handle complex conjugation (using \texttt{np.dot} instead of \texttt{np.vdot}) or suffer from numerical instability. Below are three verified approaches for quantum states.

\subsubsection*{1. The Production Approach: NumPy QR}
\textit{Recommended for actual work.} This approach is significantly faster, numerically stable, and utilizes optimized C routines.

\begin{lstlisting}[language=Python]
import numpy as np

def quantum_gram_schmidt(state_vectors):
    # Orthonormalizes a set of quantum state vectors using QR decomposition.
    # 'reduced' returns Q with dimensions matching the input columns
    Q, _ = np.linalg.qr(state_vectors, mode='reduced')
    return Q
\end{lstlisting}

\subsubsection*{2. The Pedagogical Approach: Modified GS}
\textit{Recommended for understanding.} This is the correct manual implementation for quantum computing. It explicitly handles complex conjugation ($\braket{\psi}{\phi}$) and uses the \textbf{Modified} algorithm to stabilize against rounding errors.

\begin{lstlisting}[language=Python]
def modified_gram_schmidt(A):
    # Ensure complex type to prevent casting errors
    A = A.astype(complex)
    n_rows, n_cols = A.shape
    Q = np.zeros((n_rows, n_cols), dtype=complex)
    
    for j in range(n_cols):
        v = A[:, j]
        # Subtract projections onto ALL previous vectors
        for i in range(j):
            # np.vdot conjugates the first argument: <Q_i | v>
            projection = np.vdot(Q[:, i], v) 
            v = v - projection * Q[:, i]
            
        norm = np.linalg.norm(v)
        if norm < 1e-10: 
            Q[:, j] = np.zeros_like(v)
        else:
            Q[:, j] = v / norm
    return Q
\end{lstlisting}

\subsubsection*{3. The Robust Approach: Handling Linear Dependence}
\textit{Recommended for redundant sets.} If your input vectors might be linearly dependent (common when generating overcomplete bases), this version explicitly checks for dependence and skips redundant vectors to avoid division by zero.

\begin{lstlisting}[language=Python]
def robust_gram_schmidt(vectors, tol=1e-10):
    basis = []
    for v in vectors:
        w = np.array(v, dtype=complex)
        for b in basis:
            w -= np.vdot(b, w) * b # Orthogonalize
            
        norm = np.linalg.norm(w)
        if norm > tol:
            basis.append(w / norm)
            
    return np.array(basis).T # Return as columns
\end{lstlisting}


\break

\section{Fundamentals of Matrix Algebra}

Matrices play a fundamental role across various fields, such as physics, engineering, data science, and especially quantum computing. Here, they represent quantum measurements, evolution, and mixed quantum states. A solid understanding of matrix algebra as well as the relationship between vectors and matrices is profound and essential for advanced topics in quantum computing. This chapter introduces the fundamental principles of matrix algebra, which the proceeding chapters will build upon. 

\subsection{Matrix Basics}

After introducing matrices and vectors in the previous sections, we will provide a thorough exploration of all fundamental concepts essential for the subsequent sections and chapters.

\begin{definition}[Matrix]
    A matrix $A$ is defined as a rectangular array with $m$ rows and $n$ columns, represented as follows:
    \[
    A = \begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{bmatrix}_{m \times n},
    \]
    where the numbers in the matrix are called entries or elements. The matrix is said to be an $m \times n$ matrix, or $m \times n$ in size.
\end{definition}

In linear algebra, matrices are conventionally denoted by capitalized Latin or Greek letters, such as $A, B, U, V, \Sigma, \Delta$. This convention largely holds in quantum computing with a notable exception: the density matrix $\rho$. Additionally, the notation $[a_{ij}]$ might be used to denote the matrix $A$, and $(A)_{ij}$ is used to refer specifically to the entries of $A$, with $i,j$ referring to the individual entries of $A$.

\begin{definition}[Column and Row Vectors]
    A matrix with only one column ($n \times 1$ in size) is called a column vector, denoted as
    \[
    \vvec =
    \begin{bmatrix}
    v_{1} \\
    v_{2} \\
    \vdots \\
    v_{n}
    \end{bmatrix}.
    \]
    Similarly, a matrix with only one row ($1 \times m$ in size) is called a row vector, denoted as
    \[
    \vvec^{T} =
    \begin{bmatrix}
    v_{1} & v_{2} & \cdots & v_{m}
    \end{bmatrix}.
    \]
\end{definition}

In matrix algebra, a scalar can be considered as a $1 \times 1$ matrix when this interpretation fits the context of the operations. This allows scalars to be seamlessly incorporated into matrix operations, especially when dealing with matrices of varying dimensions.

\begin{definition}[Transpose]
    Given an $m \times n$ matrix $A$, the transpose of $A$, denoted by $A^{T}$, is the $n \times m$ matrix
    \[
    A^{T} =
    \begin{bmatrix}
    a_{11} & a_{21} & \cdots & a_{m1} \\
    a_{12} & a_{22} & \cdots & a_{m2} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{1n} & a_{2n} & \cdots & a_{mn}
    \end{bmatrix}_{n \times m},
    \]
    obtained by interchanging the rows and columns of $A$.
\end{definition}

\begin{definition}[Conjugate]
    For a complex matrix $A$, its conjugate is the matrix $A^{*}$ obtained by taking the complex conjugate of each entry:
    \[
    A^{*} =
    \begin{bmatrix}
    a_{11}^{*} & a_{12}^{*} & \cdots & a_{1n}^{*} \\
    a_{21}^{*} & a_{22}^{*} & \cdots & a_{2n}^{*} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1}^{*} & a_{m2}^{*} & \cdots & a_{mn}^{*}
    \end{bmatrix}.
    \]
\end{definition}

We remind that the complex conjugate is obtained through inverting the sign of the complex component of the individual entries. 

\begin{definition}[Adjoint]
    For a complex matrix $A$, its Hermitian adjoint, or adjoint, denoted by $A^{\dagger}$, is defined as the conjugate transpose:
    \[
    A^{\dagger} = (A^{*})^{T} = (A^{T})^{*} =
    \begin{bmatrix}
    a_{11}^{*} & a_{21}^{*} & \cdots & a_{m1}^{*} \\
    a_{12}^{*} & a_{22}^{*} & \cdots & a_{m2}^{*} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{1n}^{*} & a_{2n}^{*} & \cdots & a_{mn}^{*}
    \end{bmatrix}_{n \times m}.
    \]
\end{definition}

For a real matrix, it's complex conjugate is just the matrix itself, while the adjoint is equal to the transpose. Just some matrix manipulation here, nothing else. While some textbooks might use $\overline{A}$ as the conjugate of a matrix, $A^*$ as the conjugate transpose (adjoint) of the matrix. 

\begin{theorem}
    For any matrix $A$, the following property holds: 
    $$
    (A^T)^T = A, \quad (A^\dagger)^\dagger = A
    $$
\end{theorem}

\begin{definition}[Square Matrix]
    A matrix that has an equal number of rows and columns is termed a square matrix, or an $n \times n$ matrix.
\end{definition}

\begin{definition}[Main Diagonal]
    The main diagonal of a square matrix consists of the entries where the row and column indices are the same. In an $n \times n$ matrix $A$, the entry $a_{ij}$ is on the main diagonal if $i = j$.
\end{definition}

\begin{definition}[Matrix Equality]
    Two matrices of the same size, $A_{m \times n}$ and $B_{m \times n}$, are said to be equal if all corresponding entries are identical:
    \[
    a_{ij} = b_{ij}, \quad \text{for all } i = 1,2,\ldots,m \text{ and } j = 1,2,\ldots,n.
    \]
    If two matrices $A$ and $B$ are identical in this sense, this relationship is denoted simply by
    \[
    A = B.
    \]
\end{definition}

\begin{definition}[Symmetric Matrix]
    A square matrix $A$ is termed a symmetric matrix if $A = A^T$. 
\end{definition}

While symmetric matrices are more imporant in the analysis of real matrices, Hermetian matrices are more pertient especially as they remain invariant when taking the Hermitian adjoint. 

\begin{definition}[Hermitian Matrix]
    A square complex matrix $A$ is said to be Hermitian or a Hermitian matrix if $A = A^\dagger$.
\end{definition}

Hermitian matrices are of crucial importance in quantum mnechanics and computing since they represent operators for quantum measurements. A matrix is Hermitian if it is square with entries that are conjugate symmetric about the main diagonal. Any real symmetric matrix is Hermitian. 

\begin{example}
    The following matrix is a Hermitian Matrix:

    $$
    A = 
    \begin{bmatrix}
    0 & a - ib & c - id \\
    a + ib & 1 & m - in \\
    c + id & m + in & 2
    \end{bmatrix}
    $$
\end{example}

Now similar to vectors, the fundamental algebraic operations for matrices include addition and scalar multiplication, which are defined as follows. 


\begin{definition}[Matrix Addition]
    Matrix addition is defined only for two matrices of the same size, $A_{m \times n}$ and $B_{m \times n}$. Their sum, denoted by $A + B$, is an $m \times n$ matrix computed entrywise as
    \[
    A + B =
    \begin{bmatrix}
    a_{11} + b_{11} & a_{12} + b_{12} & \cdots & a_{1n} + b_{1n} \\
    a_{21} + b_{21} & a_{22} + b_{22} & \cdots & a_{2n} + b_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} + b_{m1} & a_{m2} + b_{m2} & \cdots & a_{mn} + b_{mn}
    \end{bmatrix}.
    \]
\end{definition}

\begin{definition}[Scalar Multiplication of Matrices]
    Let $A_{m \times n}$ be a matrix and $k \in \R$ a scalar. The scalar multiple $kA$ is the $m \times n$ matrix obtained by multiplying each entry of $A$ by $k$:
    \[
    kA =
    \begin{bmatrix}
    k a_{11} & k a_{12} & \cdots & k a_{1n} \\
    k a_{21} & k a_{22} & \cdots & k a_{2n} \\
    \vdots   & \vdots   & \ddots & \vdots   \\
    k a_{m1} & k a_{m2} & \cdots & k a_{mn}
    \end{bmatrix}.
    \]
\end{definition}

\begin{definition}[Negative of a Matrix]
    The negative of a matrix $A_{m \times n}$, denoted by $-A$, is defined as the scalar multiple $(-1)A$. Explicitly,
    \[
    -A =
    \begin{bmatrix}
    -a_{11} & -a_{12} & \cdots & -a_{1n} \\
    -a_{21} & -a_{22} & \cdots & -a_{2n} \\
    \vdots  & \vdots  & \ddots & \vdots  \\
    -a_{m1} & -a_{m2} & \cdots & -a_{mn}
    \end{bmatrix}.
    \]
\end{definition}

\begin{definition}[Matrix Subtraction]
    For two matrices $A_{m \times n}$ and $B_{m \times n}$, matrix subtraction $A - B$ is defined as the addition of $A$ and the negative of $B$:
    \[
    A - B = A + (-B) =
    \begin{bmatrix}
    a_{11} - b_{11} & a_{12} - b_{12} & \cdots & a_{1n} - b_{1n} \\
    a_{21} - b_{21} & a_{22} - b_{22} & \cdots & a_{2n} - b_{2n} \\
    \vdots          & \vdots          & \ddots & \vdots          \\
    a_{m1} - b_{m1} & a_{m2} - b_{m2} & \cdots & a_{mn} - b_{mn}
    \end{bmatrix}.
    \]
\end{definition}

While these are visually appealing methods for notation, they are quite cumbersome in notation. 

\begin{theorem}
    Given two matrices $A_{m \times n} = [a_{ij}]$ and $B_{m \times n} = [a_{ij}]$ of the same size and a scalar $k$, the basic algebraic operations can be computed element wise as follows:

    \begin{align*}
        (A + B)_{ij} &= a_{ij} + b_{ij} \\
        (kA)_{ij} &= ka_{ij} \\
        (-A)_{ij} &= -a_{ij} \\
        (A - B)_{ij} &= a_{ij} - b_{ij}\\
    \end{align*}
\end{theorem}

For transpose and Hermitian of matrices, the element-wise formula is as follows: 

\begin{theorem}
    Given $A_{m \times n} = [a_{ij}]$, the entries of $A^T$ and $A^\dagger$ are given element-wise by:
    \begin{align*}
        (A^T)_{ji} &= a_{ij} \\
        (A^\dagger)_{ji} &= a^*_{ij} \\
    \end{align*}
\end{theorem}

Since matrices adhere to the same fundamental operations as vectors, they naturally form vecor spaces. In order to complete the definition, we first need to define the zero matrix. 

\begin{definition}[Zero Matrix]
    Among all matrices of a specific size $m \times n$, the unique zero matrix, denoted by $0$, is defined as the matrix whose entries are all zero:
    \[
    0_{m \times n} =
    \begin{bmatrix}
    0 & 0 & \cdots & 0 \\
    0 & 0 & \cdots & 0 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 0 & \cdots & 0
    \end{bmatrix}_{m \times n}
    \]
    or in element-wise notation,
    \[
    (0)_{ij} = 0, \quad \forall i = 1,2,\ldots,m \text{ and } j = 1,2,\ldots,n.
    \]
\end{definition}

\begin{theorem}
    The following properties hold true for any matrices $A, B, C, 0$ of the same size and scalars $k, m$:

    \begin{align*}
        A + B &= B + A \\
        A + 0 &= 0 + A = A \\
        (A + B) + C &= A + (B + C) \\
        k(A + B) &= kA + kB \\
        (k + m)A &= kA + mA \\
        (km)A &= k(mA) \\
        0A &= 0 \\
        1A &= A\\
    \end{align*}
\end{theorem}

While this might seem familiar to you, it is worth reiterating given that there are some key differences between vectors and matrices. At the same time, all of the aformentioned properties remain true if addition is replaced with subtraction. It's rather easy to verfiy that matrices of a specific size $m \times n$ satisfy the all of the axioms required for a vector spaces given in the previous theorem. Therefore, we have:

\begin{theorem}
    All matrices of size $m \times n$ constitute a vector space. For real matrices over $\R$, this is defined as $\Rmn$, nad for complex matrices over $\C$, this is denoted as $\Cmn$. 
\end{theorem}

For the matrix space $\R^{2 \times 2}$, we have the following basis: 

\[
B_{11} =
\begin{bmatrix}
1 & 0 \\
0 & 0
\end{bmatrix},
\quad
B_{12} =
\begin{bmatrix}
0 & 1 \\
0 & 0
\end{bmatrix},
\quad
B_{21} =
\begin{bmatrix}
0 & 0 \\
1 & 0
\end{bmatrix},
\quad
B_{22} =
\begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix}.
\]


In general, there for a vector space in $\Rmn/\Cmn$, there are going to be $mn$ basis matrices in the $mn$ dimensional space. This warrants the introduction of a more broader concept, namely tensors. If we take vectors to be one-dimensional arrays in $\Cn$, and matrices to be two-dimensional arrays in $\C^{n \times n}$, and three dimensional arrays in $\C^{n \times n \times n}$, the mathmatical term given to such objects in higher dimensions is defined as tensors. By this definition, vectors are 1D tensors, matrices are 2D tensors, and then generally $k$-dimensional tensors will reside in $\C^{n \times n \times \cdots \times n}$. 

\subsection{Matrix Multiplication}

While the previous subsection established the foundational principles of matrix algebra, behavior of matrices largely seemed like vectors. However, matrix multiplication significantly differs from vector multiplication, which you will be able to see in this chapter. At the same time, we note that this will mainly focus on square matrices as they are of particular importance in quantum computing. 

Before diving into matrix multiplication, it is crucial to understand how to partition a matrix into its row and column vectors, which are the two fundamental partitions of a matrix. 

\begin{definition}[Partition into Row and Column Vectors]
    Given a matrix $A_{m \times n}$, it can be partitioned into its row vectors as
    \[
    A =
    \begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    \hline
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \hline
    \vdots & \vdots & \ddots & \vdots \\
    \hline
    a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{bmatrix}
    =
    \begin{bmatrix}
    \mathbf{r}_{1} \\
    \mathbf{r}_{2} \\
    \vdots \\
    \mathbf{r}_{m}
    \end{bmatrix},
    \]
    where $\mathbf{r}_{1}, \mathbf{r}_{2}, \ldots, \mathbf{r}_{m}$ represent the row vectors of $A$. The matrix can also be partitioned into its column vectors as
    \[
    A =
    \begin{bmatrix}
    a_{11} & \vline & a_{21} & \vline & \cdots & \vline & a_{m1} \\
    a_{12} & \vline & a_{22} & \vline & \cdots & \vline & a_{m2} \\
    \vdots & \vline & \vdots & \vline & \ddots & \vline & \vdots \\
    a_{1n} & \vline & a_{2n} & \vline & \cdots & \vline & a_{mn}
    \end{bmatrix}
    =
    \begin{bmatrix}
    \mathbf{c}_{1} & \mathbf{c}_{2} & \cdots & \mathbf{c}_{n}
    \end{bmatrix},
    \]
    where $\mathbf{c}_{1}, \mathbf{c}_{2}, \ldots, \mathbf{c}_{n}$ are the column vectors of $A$.
\end{definition}

Matrix partitioning will be essential to what is to come in a moment. We will adopt the convention that $\mathbf{r}_i$ and $\mathbf{c}_j$ represent row and column vectors of a given matrix. We previously introduced the dot product as:

\[
\rvec \cvec
= \begin{bmatrix} r_{1} & r_{2} & \cdots & r_{n} \end{bmatrix}
  \begin{bmatrix}
  c_{1} \\
  c_{2} \\
  \vdots \\
  c_{n}
  \end{bmatrix}
= r_{1} c_{1} + r_{2} c_{2} + \cdots + r_{n} c_{n}
= \rvec \cdot \cvec.
\]

Now we will the dot product to matrix products. For the matrices $A_{m \times p}$ and $B_{p \times n}$, note that the number of columns in $A$ must be equal to the number of rows in $B$. While you are multiplying the rows in $A$ with the columns in $B$, the entries in a row from $A$ and entries in a column from $B$ must be the same. Therefore, the elements in a column of $A$ and the number of elements in a row of $B$ must add up. 

\begin{definition}[Matrix Product]
    The matrix product $AB$ is defined when the number of columns in $A_{m \times p}$ matches the number of rows in $B_{p \times n}$. Partition $A$ into its row vectors and $B$ into its column vectors:
    \[
    A =
    \begin{bmatrix}
    \rvec_{1} \\
    \rvec_{2} \\
    \vdots   \\
    \rvec_{m}
    \end{bmatrix},
    \qquad
    B =
    \begin{bmatrix}
    \cvec_{1} & \cvec_{2} & \cdots & \cvec_{n}
    \end{bmatrix}.
    \]
    The product $AB$ is consequently defined as an $m \times n$ matrix, computed as
    \[
    (AB)_{m \times n} =
    \begin{bmatrix}
    \rvec_{1} \cdot \cvec_{1} & \rvec_{1} \cdot \cvec_{2} & \cdots & \rvec_{1} \cdot \cvec_{n} \\
    \rvec_{2} \cdot \cvec_{1} & \rvec_{2} \cdot \cvec_{2} & \cdots & \rvec_{2} \cdot \cvec_{n} \\
    \vdots                    & \vdots                    & \ddots & \vdots                    \\
    \rvec_{m} \cdot \cvec_{1} & \rvec_{m} \cdot \cvec_{2} & \cdots & \rvec_{m} \cdot \cvec_{n}
    \end{bmatrix}.
    \]
\end{definition}

This relationship can be succinctly expressed using element-wise notation:

\begin{theorem}
    Given two matrices $A_{m \times p}$ and $B_{p \times n}$, and $C_{m \times n} = AB$, the entries of $C$ are given by the rowâ€“column rule for matrix multiplication:
    \[
    c_{ij} = a_{i1} b_{1j} + a_{i2} b_{2j} + \cdots + a_{ip} b_{pj}
           = \sum_{k=1}^{p} a_{ik} b_{kj},
    \]
    where $i = 1,2,\ldots,m$ and $j = 1,2,\ldots,n$.
\end{theorem}

\begin{center}
    \includegraphics[width=350px]{7_row_col}
\end{center}

Now this brings up an imporant aspect of matrix multiplication, which is that matrix multiplication generally does not commute, or that $AB \neq BA$. This takes place when either one is not defined, or that they differ in size, or that they are simply not equal. This property stands in sharp contrast with scalar multiplication and the dot product of vectors, which are commutative or conjugate commutative. In quantum computing, the special case of two matrices commuting holds significant importance. To formally express this relationship, we have the following definition:

\begin{definition}[Commutator]
    Given two square matrices of equal dimension, their commutator is denoted as $[A, B]$ and defined by:
    $$[A, B] = AB - BA$$
    When $[A, B] = 0$ or equivalently $AB = BA$, the matrices $A$ and $B$ are said to commute. 
\end{definition}

Building on this concept of commutation, where matrix multiplication is symmetric, we explore its counterpart, namely the anti-commutator. This operator provides an alternative:

\begin{definition}[Anti-Commutator]
    Given two square matrices of the same size, their anti-commutator is denoted as ${A, B}$ and defined by:
    $${A, B} = AB + BA$$
    When ${A, B} = 0$ or equivalently $AB = -BA$, the matrices $A$ and $B$ are said to anticommute. 
\end{definition}

Commutation and anticommutation relations are particularly relevant when working with Pauli matrices, which are going to be thouroughly covered in Section 12. Despite the non-commutativity of matrix multiplication, it shares many properties with scalar multiplication. For instance:

\begin{theorem}
    Assuming all matrix multiplications are valid, the following identities hold:
    \begin{align*}
        A(B + C) &= AB + AC \\
        (A + B)C &= AC + BC \\
        (AB)C &= A(BC)\\
    \end{align*}
\end{theorem}

\begin{proof}
    For the operation $A(B + C)$ to be valid, consider matrices
    $A_{m \times p} = [a_{ik}]$, $B_{p \times n} = [b_{kj}]$, and
    $C_{p \times n} = [c_{kj}]$ for $i = 1,2,\ldots,m$, $k = 1,2,\ldots,p$, and
    $j = 1,2,\ldots,n$.

    Define $D = A(B + C) = [d_{ij}]$ and $F = AB + AC = [f_{ij}]$
    (both are $m \times n$ matrices). Compute the entries of $D$ and $F$
    using element-wise notation for matrix multiplication:
    \[
        d_{ij} = \sum_{k=1}^{p} a_{ik} (b_{kj} + c_{kj}), \qquad
        f_{ij} = \sum_{k=1}^{p} a_{ik} b_{kj} + \sum_{k=1}^{p} a_{ik} c_{kj}.
    \]

    By applying the distributive laws of scalars within the summation,
    \[
        d_{ij}
        = \sum_{k=1}^{p} a_{ik} (b_{kj} + c_{kj})
        = \sum_{k=1}^{p} (a_{ik} b_{kj} + a_{ik} c_{kj})
        = \sum_{k=1}^{p} a_{ik} b_{kj} + \sum_{k=1}^{p} a_{ik} c_{kj}
        = f_{ij},
    \]
    for all $i = 1,2,\ldots,m$ and $j = 1,2,\ldots,n$. Hence, by matrix
    equality, $D = F$. 
\end{proof}

The proofs of the two other properties follow a similar pattern and are omitted for brevity. At the same time, when we are dealing with transpose or adjoint operations, the following identities are very useful:

\begin{theorem}
    Assuming all matrix multiplications are valid, the following identities hold:
    \begin{align*}
        (AB)^* &= A^* B^* \\
        (AB)^T &= B^T A^T \\
        (AB)^\dagger &= B^\dagger A^\dagger
    \end{align*}
\end{theorem}

\begin{proof}
    To prove the part of the above theorem, consider $A_{m \times p} = [a_{ik}]$ and
    $B_{p \times n} = [b_{kj}]$ for $i = 1,2,\ldots,m$, $k = 1,2,\ldots,p$, and
    $j = 1,2,\ldots,n$.

    Let $C_{n \times m} = (AB)^{T} = [c_{ji}]$ and
    $D_{n \times m} = B^{T} A^{T} = [d_{ji}]$. Using the rowâ€“column rule, compute
    the entries of $C$ and $D$ as follows:
    \[
        c_{ji} = \bigl((AB)^{T}\bigr)_{ji}
        = (AB)_{ij}
        = \sum_{k=1}^{p} a_{ik} b_{kj},
    \]
    \[
        d_{ji} = (B^{T} A^{T})_{ji}
        = \sum_{k=1}^{p} (B^{T})_{jk} (A^{T})_{ki}
        = \sum_{k=1}^{p} b_{kj} a_{ik}.
    \]

    Since multiplication of scalar entries is commutative,
    $a_{ik} b_{kj} = b_{kj} a_{ik}$ for all $i,j,k$, it follows that
    $c_{ji} = d_{ji}$. Therefore, $C = D$, proving that
    \[
        (AB)^{T} = B^{T} A^{T}.
    \]
    The third equation builds upon the previous logic, where
    \[
    (AB)^{\dagger}
    = \bigl((AB)^{*}\bigr)^{T}
    = (A^{*} B^{*})^{T}
    = (B^{*})^{T} (A^{*})^{T}
    = B^{\dagger} A^{\dagger}.
    \]
\end{proof}

Now, matrix multiplication can also be performed using a technique known as the column-row expansion. This is an alternative way of computing the product of matrices by expanding it into a sum of individual matrices. 

\begin{theorem}[Columnâ€“Row Expansion]
    Consider matrices $A_{m \times p}$ and $B_{p \times n}$. Partition $A$ into its column vectors and $B$ into its row vectors:
    \[
        A =
        \begin{bmatrix}
        \cvec_{1} & \cvec_{2} & \cdots & \cvec_{p}
        \end{bmatrix},
        \qquad
        B =
        \begin{bmatrix}
        \rvec_{1} \\
        \rvec_{2} \\
        \vdots   \\
        \rvec_{p}
        \end{bmatrix}.
    \]
    Then, the product $AB$ can be calculated by summing the products of these vectors:
    \[
        AB = \cvec_{1} \rvec_{1} + \cvec_{2} \rvec_{2} + \cdots + \cvec_{p} \rvec_{p}
           = \sum_{i=1}^{p} \cvec_{i} \rvec_{i}.
    \]
\end{theorem}

Note that in this each product term become full-sized matrices, and thus the entire product is express as a sum of such matrices. However, we can't have a little fun without Dirac notation, and in the field of quantum computing, the matrices are primarily square matrices and column vectors. In this subsection, we will delve into some of the most frequently encountered matrix multiplication scenarios in quantum computing utilizing bra-ket notation. For example, we know that the inner product (essentially a dot product) effectively produces a scalar and is defined as:

\[
\innerprod{\uvec}{\vvec}
= \braket{\uvec}{\vvec}
= \begin{bmatrix} u_{1}^{*} & u_{2}^{*} & \cdots & u_{n}^{*} \end{bmatrix}
  \begin{bmatrix}
  v_{1} \\
  v_{2} \\
  \vdots \\
  v_{n}
  \end{bmatrix}
= u_{1}^{*} v_{1} + u_{2}^{*} v_{2} + \cdots + u_{n}^{*} v_{n}.
\]

Hereforth, we also define a matrix-vector multiplication as multiplying an $n \times n$ matrix by an $n \times 1$ column vector, yielding another $n \times 1$ column vector. In dirac notation, this operation is expressed as:

$$A \ket{u} = \ket{v}$$

In this context, $A$ is referred to as a transformation matrix as it transforms the vector $\ket{u}$ into $\ket{v}$. The matrix representation of this operation is as follows: 

\[
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{bmatrix}
\begin{bmatrix}
u_{1} \\
u_{2} \\
\vdots \\
u_{n}
\end{bmatrix}
=
\begin{bmatrix}
v_{1} \\
v_{2} \\
\vdots \\
v_{n}
\end{bmatrix}.
\]

This operation is fundamental in quantum computing, particularly in the context of quantum gates, as we can simply take the vector $\ket{u}$ as the input quantum gate, and $\ket{v}$ as the output quantum gate. $A$ embodies the matrix representation of the quantum gate, which is mathmatical candor at its finest. Next we will introduce the outer product of two vectors, which is defined as follows: 

\begin{definition}[Outer Product of Two Vectors]
    Given two vectors $\ket{u}$ and $\ket{v}$ in $\Cn$, their outer product is an $n \times n$ matrix defined as
    \[
        \ket{u}\bra{v}
        =
        \begin{bmatrix}
        u_{1} \\
        u_{2} \\
        \vdots \\
        u_{n}
        \end{bmatrix}
        \begin{bmatrix}
        v_{1}^{*} & v_{2}^{*} & \cdots & v_{n}^{*}
        \end{bmatrix}
        =
        \begin{bmatrix}
        u_{1} v_{1}^{*} & u_{1} v_{2}^{*} & \cdots & u_{1} v_{n}^{*} \\
        u_{2} v_{1}^{*} & u_{2} v_{2}^{*} & \cdots & u_{2} v_{n}^{*} \\
        \vdots          & \vdots          & \ddots & \vdots          \\
        u_{n} v_{1}^{*} & u_{n} v_{2}^{*} & \cdots & u_{n} v_{n}^{*}
        \end{bmatrix}.
    \]
\end{definition}

A pretty useful formula for outer products is as follows: 

$$\ket{v}\bra{u} = (\ket{u}\bra{v})^\dagger$$

This can be demonstrated by applying the conjugate transpose prooperty:

$$(\ket{u}\bra{v})^\dagger = (\bra{v})^\dagger (\ket{u})^\dagger = \ket{v}\bra{u}$$

\begin{example}
    Compute $\ket{0}\bra{0}$, $\ket{0}\bra{1}$, $\ket{1}\bra{0}$, and $\ket{1}\bra{1}$ in matrix representation. 
    
    Using the computational basis, we find:
    \[
        \ket{0}\bra{0}
        =
        \begin{bmatrix}
        1 \\
        0
        \end{bmatrix}
        \begin{bmatrix}
        1 & 0
        \end{bmatrix}
        =
        \begin{bmatrix}
        1 & 0 \\
        0 & 0
        \end{bmatrix},
        \qquad
        \ket{0}\bra{1}
        =
        \begin{bmatrix}
        1 \\
        0
        \end{bmatrix}
        \begin{bmatrix}
        0 & 1
        \end{bmatrix}
        =
        \begin{bmatrix}
        0 & 1 \\
        0 & 0
        \end{bmatrix},
    \]
    \[
        \ket{1}\bra{0}
        =
        \begin{bmatrix}
        0 \\
        1
        \end{bmatrix}
        \begin{bmatrix}
        1 & 0
        \end{bmatrix}
        =
        \begin{bmatrix}
        0 & 0 \\
        1 & 0
        \end{bmatrix},
        \qquad
        \ket{1}\bra{1}
        =
        \begin{bmatrix}
        0 \\
        1
        \end{bmatrix}
        \begin{bmatrix}
        0 & 1
        \end{bmatrix}
        =
        \begin{bmatrix}
        0 & 0 \\
        0 & 1
        \end{bmatrix}.
    \]
\end{example}

Now in this example, we see that it is possible to represent a general matrix $A_{2 \times 2}$ in terms of outer products. This approach can be generalized to any matrix using the following theorem. 

\begin{theorem}[Matrix Basis Decomposition]
    Let $\{\ket{0}, \ket{1}, \ldots, \ket{d-1}\}$ be the computational basis of $\C^{d}$. For a general matrix $A \in \C^{d \times d}$ given by
    \[
        A =
        \begin{bmatrix}
        a_{00}     & a_{01}     & \cdots & a_{0(d-1)} \\
        a_{10}     & a_{11}     & \cdots & a_{1(d-1)} \\
        \vdots     & \vdots     & \ddots & \vdots     \\
        a_{(d-1)0} & a_{(d-1)1} & \cdots & a_{(d-1)(d-1)}
        \end{bmatrix},
    \]
    it can be decomposed into a sum of outer products:
    \[
        A = \sum_{i,j} a_{ij} \ket{i}\bra{j},
        \qquad \text{for } i,j = 0,1,\ldots,d-1.
    \]
\end{theorem}

\begin{example}
    Given the matrix $A$ as described above, demonstrate that:
    $$A \ket{k} = \sum_i a_{ik} \ket{i}$$
\end{example}

\begin{proof}
    \begin{align*}
        A \ket{k} &= \left(\sum_{i, j} a_{ij} \ket{i} \bra{j}\right) \ket{k} \\
        &= \sum_{i, j} a_{ij} \ket{i} \braket{j}{k} \\
        &= \sum_{i, j} a_{ij} \ket{i} \delta_{jk} \\ 
        &= \sum_{i} a_{ij} \ket{i}
    \end{align*}
\end{proof}

Where the equailty $\braket{j}{k} = \delta_{jk}$ (the Koronecker delta) simplifies the expression by collapsing the sum over $j$ to only the term where $j = k$. Next we will consider basis change matrices based on the previous example. In quantum computing, basis changes are often expressed as sums of outer products between two basis vectors. For example, consider changing from the set of vectors ${\ket{0}, \ket{1}}$ to the basis ${\ketplus, \ketminus}$. We take the matrix $U$ that performs this mapping to be 

$$
U = \ketplus \bra{0} + \ketminus \bra{1}
$$

We can verify that $U$ completes this mapping by showing, for example, that $(\ketplus \bra{0})\ket{0} = \ketplus \braket{0}{0}$. This result can be generalized to the following formula:

\begin{theorem}
    Given a matrix $U$ as an outer product of $U = \ket{u} \bra{v}$ and a vector $\ket{w}$, the product $U \ket{w}$ is computed as:
    $$U \ket{w} = \braket{v}{w} \ket{u}$$
    which is just a scalar multiple of $\ket{u}$
\end{theorem}

\begin{proof}
    We have 
    $$U \ket{w} = (\ket{u} \bra{v}) \ket{w} = \ket{u} (\bra{v} \ket{w}) = \ket{u} \braket{v}{w} = \braket{v}{w} \ket{u}$$
    Since $\braket{v}{w}$ is a scalar, the result is just a scalar multiple of $\ket{u}$. 
\end{proof}

This provides you with an effective yet clean change of basis method. A more comprehensive discussion on changing basis will be presented in the next section. Sometimes, it is also necessary to compute the inner product between two vectors, wherein one or both vectors are the results of a matrix-vector product. For instance in classical linear algebra, we have:

$$\braket{Ax}{y} = (A\mathbf{x}) \cdot \mathbf{y}$$
$$\braket{x}{Ay} = \mathbf{x} \cdot (A\mathbf{y})$$
$$\braket{Ax}{Ay} = (A\mathbf{x}) \cdot (A\mathbf{y})$$

For brevity, we denote $\ket{Ax} \equiv A \ket{x}$ and $\bra{Ax} \equiv (A \ket{x})^\dagger$, and adopt the following identities when dealing with these forms:

\begin{theorem}
    The inner product on the left-hand side can be expressed as matrix products on the right-hand side:
    \begin{align*}
        \braket{x}{Ay} &= \bra{x} A \ket{y} \\
        \braket{Ax}{y} &= \bra{x} A^\dagger \ket{y} \\
        \braket{Ax}{Ay} &= \bra{x} A^\dagger A \ket{y}\\
    \end{align*}
\end{theorem}

\begin{proof}
    These relationships can be defined using the rules of bra-ket notation:
    \begin{align*}
        \braket{x}{Ay} &= \bra{x}\ \ket{Ay} = \bra{x} A \ket{y} \\
        \braket{Ax}{y} &= \bra{Ax}\ \ket{y} = (\ket{Ax})^\dagger \ket{y} = \ket{x}^\dagger A^\dagger \ket{y} = \bra{x} A^\dagger \ket{y} \\
        \braket{Ax}{Ay} &= \bra{Ax}\ \ket{Ay} = (\ket{Ax})^\dagger A \ket{y} = \bra{x} A^\dagger A \ket{y} \\
    \end{align*}
\end{proof}

For the given matrix $A$, the expression $\bra{x} A \ket{y}$ defines the mapping $\Cn \times \Cn \rightarrow \C$, which is linear in the second argument $\ket{y}$ and conjugate linear in the first argument $\ket{x}$, which refers to $\bra{x} A \ket{y}$ as something in sesquilinear form, a generalization of inner products in complex vector spaces. What does this mean? 

When we say the mapping is linear in $\ket{y}$, we mean it behaves exactly as you would expect standard multiplication to behave. It preserves addition and scalar multiplication without changing the scalar. Mathematically, if you replace $\ket{y}$ with a linear combination $c_1\ket{y_1} + c_2\ket{y_2}$ (where $c_1, c_2$ are complex numbers), the expression splits cleanly:

$$
\bra{x} A (c_1\ket{y_1} + c_2\ket{y_2}) = c_1 \bra{x} A \ket{y_1} + c_2 \bra{x} A \ket{y_2}
$$

Conjugate linear (sometimes called antilinear) is where complex spaces differ from real ones. It means that the mapping preserves addition, but when you pull out a scalar, it comes out as its complex conjugate.

$$
(c_1\ket{x_1} + c_2\ket{x_2})^\dagger A \ket{y} = c_1^* \bra{x_1} A \ket{y} + c_2^* \bra{x_2} A \ket{y}
$$

Recall that in Dirac notation (bra-ket notation), the "bra" $\bra{x}$ corresponds to the Hermitian conjugate (adjoint) of the column vector $x$. If $\ket{x}$ is the column vector $x$, then $\bra{x}$ is the row vector $x^\dagger$ (conjugate transpose). If you scale the vector $x$ by a complex number $c$, the conjugate transpose scales by $c^*$.

$$
(c x)^\dagger = c^* x^\dagger
$$


So therefore the term Sesquilinear literally means "one-and-a-half linear." It is "one" linear because of the second argument ($\ket{y}$). It is "half" linear (antilinear) because of the first argument ($\bra{x}$). 

Sesquilinearity ensures that the norm of a vector (the inner product of a vector with itself) is always a real number. If it were linear in both arguments, $\braket{cx}{cx}$ would equal $c^2 \braket{x}{x}$, which could be a complex number. Because it is conjugate linear in the first slot, we get:

$$
\braket{cx}{cx} = c^* c \braket{x}{x} = |c|^2 \braket{x}{x}
$$

This guarantees that lengths/probabilities remain real and non-negative. Now going along with the previous theorems, if $A$ is Hermitian and both vectors on either side are equal, we call this a \textbf{Hermitian quadratic form} for the expression:

$$\brapsi M \ketpsi$$

where $M$ is Hermitian ($M = M^\dagger$). This form is critical in quantum computing as it represents the statistical average of an observable $M$ on the state $\ketpsi$ always yielding a real scalar. This is explored further below. 

\begin{theorem}
    Given a Hermitian matrix $M \in \Cnn$ and a complex vector $\ketpsi \in \Cn$, $\brapsi M \ketpsi$ is always real. 
\end{theorem}

\begin{proof}
    Taking $\ket{x} = \ket{y} = \ketpsi$ from previous equations, we have:

    $$\brapsi M^\dagger \ketpsi = (\brapsi M \ketpsi)^*$$
    
    Since $M$ is Hermitian, it holds that $M = M^\dagger$, and thus:

    $$\brapsi M \ketpsi = \brapsi M^\dagger \ketpsi$$
    
    This implies that:

    $$\brapsi M \ketpsi = (\brapsi M \ketpsi)^* \quad \Longrightarrow \quad \brapsi M \ketpsi \in \R$$
\end{proof}

\subsection{Matrix Inverses}

Very similar to how real numbers have inverses, matrices have them too. 

\begin{definition}[Identity Matrix]
    The identity matrix, denoted by $I_{n}$, is an $n \times n$ square matrix characterized by ones on its main diagonal and zeros in all off-diagonal positions:
    \[
        I_{n} =
        \begin{bmatrix}
        1      & 0      & 0      & \cdots & 0 \\
        0      & 1      & 0      & \cdots & 0 \\
        0      & 0      & 1      & \cdots & 0 \\
        \vdots & \vdots & \vdots & \ddots & \vdots \\
        0      & 0      & 0      & \cdots & 1
        \end{bmatrix}.
    \]
\end{definition}


The identity matrix can also be defined using the Kronecker delta function as:

$$
I_n = [\delta_{ij}], \qquad \delta_{ij} = \begin{cases}
    0 \quad i \neq j \\
    1 \quad i = j \\
\end{cases}
$$

\begin{theorem}
    For any matrices $A_{m \cross n}$, the identity matrix $I_n$ and $I_m$ satisfies the properties $A I_n = A$ and $I_m A = A$. 
\end{theorem}

\begin{proof}
    Let $A = [a_{ij}]$ and the product $C = AI_n = [c_{ij}]$:
    $$c_{ij} = \sum_{k = 1}^{n} a_{ik} \delta_{kj}. $$
    The only non-zero term in the summation occurs when $k = j$, yielding:
    $$c_{ij} = a_{ij}\delta_{jj} = a_{ij}, \qquad \forall i, j,$$
    affirming that $A I_n = A$. We can apply a similar method for validating $I_m A = A$.
\end{proof}

Note that we can omit the subscript when the context clearly identifies the size of the identity matrix $I$. Next, we will define the inverse of a matrix, which is conceptually analogous to the inverse of a scalar, but entails a few more hoops that you need to jump through.

\begin{definition}[Inverse of a Matrix]
    Consider a square matrix $A_{n \times n}$. If there exists a matrix $B_{n \times n}$ such that $BA = AB = I$, then we consider $A$ \textbf{invertible}, $B$ as the \textbf{inverse} of $A$, denoted by $A^{-1}$. If there exists no such $B$, then $A$ is described as \textbf{singular}. 
\end{definition}

We know that only square matrices are invertible. However, we also know that the inverse of a matrix is unique. 

\begin{theorem}
    The inverse of a matrix, if it exists, is unique. 
\end{theorem}

\begin{proof}
    Assume $B$ and $C$ are the inverse of a matrix $A$. By definition:
    $$AB = BA = I = AC = CA.$$
    So if we take 
    $$BAC = B (AC) = BI = B$$
    which is by defnition equal to 
    $$BAC = (BA)C = IC = C$$
    it is easy to see that $B = C$ and that the inverse of a matrix is unique.
\end{proof}

\begin{theorem}
    For two square matrices $A$ and $B$, 
    $$AB = I \Leftrightarrow BA = I \quad \Longleftrightarrow \quad A = B^{-1} \Leftrightarrow B = A^{-1}.$$
\end{theorem}

It is out of the scope of this text to compute the inverse of matrices. However, not all matrices are invertible. The zero matrix is an obvious example of a singular matrix. In general, you can think of singular matrices having some form of information reduced from it so that compression and expansion is not able to recreate in inverted image of it. More formally: 

\begin{theorem}
    A square matrix $A$ is singular if its row vectors or column vectors are linearly independent. 
\end{theorem}

\begin{proof}
    Consider a square matrix $A$ with $n$ linearly dependent column vectors. Let $\spanop(S) = A$. Since the column vectors are linearly dependent, that implies that $\dimop(S) < n$. Next, we consider a matrix $B$ partitioned into its column vectors. If we define 
    $$C = AB = \sum_{i=1}^{n} A \mathbf{b}_i,$$
    then each product is going to be linearly dependent since $A$ is linearly dependent, implying that 
    $$\forall \mathbf{b}_i, \quad A \mathbf{b}_i \in \spanop(S) \Longrightarrow C \neq I.$$
    It is easy to see that this holds true for all matrices $B$ such that $C$ will never be the identity matrix $I$, and hence $A$ must be singular. A similar method can be used to prove that the same holds for linearly independent row vectors. 
\end{proof}

As a brief note, while invertibility is a fundamental property of square matrices and it does come up time and time again, the need to examine invertibility is not necessary in quantum computing. This is primarily so becuase unitary matrices, the mathmatical representation for quantum gates, are by definition invertible. As a result, the emphasis lies on unitary matrices and their properties, which are central to quantum computing. 

In practice, it is often more desirable to represent solutions of problems as a vector. Such problems are often formulated using matrix equations. 

\begin{theorem}
    Given an invertible matrix $A \in \Cnn, \ketphi \in \Cn$, the matrix equation
    $$A \ketphi = \ketpsi$$
    admits a unique solution:
    $$\ketphi = A^{-1} \ketpsi.$$
\end{theorem}

\begin{proof}
    Multiplying the inverse on the right hand side of the equation yields:
    $$\inverse{A} A \ketphi = \inverse{A} \ketpsi \qquad \Rightarrow \qquad I \ketphi = \inverse{A} \ketpsi \qquad \Rightarrow \qquad \ketphi = \inverse{A} \ketpsi$$
\end{proof}

\begin{theorem}
    Given a series of invertible matrices $A_{1}, A_{2}, \ldots, A_{n-1}, A_{n}$ of the same dimensions, the following identity holds true:
    \[
        (A_{1} A_{2} \cdots A_{n-1} A_{n})^{-1} = A_{n}^{-1} A_{n-1}^{-1} \cdots A_{2}^{-1} A_{1}^{-1}.
    \]
\end{theorem}

\begin{proof}
    We verify the identity by showing that the product of the sequence of matrices and their inverses results in the identity matrix, $I$:
    \begin{align*}
        (A_{n}^{-1} A_{n-1}^{-1} \cdots A_{2}^{-1} A_{1}^{-1})(A_{1} A_{2} \cdots A_{n-1} A_{n})
        &= A_{n}^{-1} A_{n-1}^{-1} \cdots A_{2}^{-1} I A_{2} \cdots A_{n-1} A_{n} \\
        &= A_{n}^{-1} A_{n-1}^{-1} \cdots A_{3}^{-1} I A_{3} \cdots A_{n-1} A_{n} \\
        &= \cdots \\
        &= A_{n}^{-1} A_{n-1}^{-1} A_{n-1} A_{n} \\
        &= A_{n}^{-1} I A_{n} = A_{n}^{-1} A_{n} = I.
    \end{align*}
    Therefore we conclude that
    \[
        (A_{1} A_{2} \cdots A_{n-1} A_{n})^{-1} = A_{n}^{-1} A_{n-1}^{-1} \cdots A_{2}^{-1} A_{1}^{-1}.
    \]
\end{proof}

\begin{theorem}
    If $A$ is an invertible matrix, then it's adjoint ($A^\dagger$) is also invertible. The inverse of $A^\dagger$ is given by:
    $$\inverse{(A^\dagger)} = (\inverse{A})^\dagger$$ 
\end{theorem}

\begin{proof}
    Multiplying the left side of the equation with $A^\dagger$ yields:
    $$I = A^\dagger (\inverse{A})^\dagger \qquad \Rightarrow \qquad I^\dagger = A \inverse{A} = I$$
\end{proof}

Next we will move on to diagonal matrices, defined as square matrices with non-zero entries only on the main diagonal, play a signifcant role as a source of convinience in linear algebra. 

\begin{definition}[Diagonal Matrix]
    A diagonal matrix is a square matrix where all off-diagonal entries are zero. A general $n \times n$ diagonal matrix is represented as
    \[
        \Lambda =
        \begin{bmatrix}
        \lambda_{1} & 0           & \cdots & 0 \\
        0           & \lambda_{2} & \cdots & 0 \\
        0           & 0           & \ddots & 0 \\
        0           & 0           & \cdots & \lambda_{n}
        \end{bmatrix}.
    \]
\end{definition}

While you might be used to using $D$ to represent diagonal matrices from real linear algebra, $\Lambda$ is used just the same. We also like to use the notation $\text{diag}(\lambda_1, \lambda_2, \ldots, \lambda_n)$ to represent diagonal matrices. A key advantage of using diagonal matrices is that matrix algebra operations are exceptionally straightforward. 

\begin{theorem}
    Given an $n \times n$ diagonal matrix $\Lambda = \operatorname{diag}(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n})$ and an $m \times n$ matrix $A$ partitioned into column vectors $A = \begin{bmatrix} \cvec_{1} & \cvec_{2} & \cdots & \cvec_{n} \end{bmatrix}$, the matrix product $A\Lambda$ is obtained by multiplying each column of $A$ by the corresponding $\lambda_{i}$ in $\Lambda$:
    \[
        A\Lambda = \begin{bmatrix} \lambda_{1}\cvec_{1} & \lambda_{2}\cvec_{2} & \cdots & \lambda_{n}\cvec_{n} \end{bmatrix}.
    \]
    Similarly, for an $n \times p$ matrix $B$ partitioned into row vectors $B = \begin{bmatrix} \rvec_{1} \\ \rvec_{2} \\ \vdots \\ \rvec_{n} \end{bmatrix}$, the matrix product $\Lambda B$ is obtained by multiplying each row vector of $B$ by the corresponding $\lambda_{i}$ in $\Lambda$:
    \[
        \Lambda B =
        \begin{bmatrix}
        \lambda_{1}\rvec_{1} \\
        \lambda_{2}\rvec_{2} \\
        \vdots \\
        \lambda_{n}\rvec_{n}
        \end{bmatrix}.
    \]
\end{theorem}

\begin{theorem}
    A diagonal matrix $\Lambda = \operatorname{diag}(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n})$ is invertible if and only if $\lambda_{i} \neq 0$ for all $i=1,2,\ldots,n$. The inverse of such a matrix is given by:
    \[
        \Lambda^{-1} = \operatorname{diag} \left( \frac{1}{\lambda_{1}}, \frac{1}{\lambda_{2}}, \ldots, \frac{1}{\lambda_{n}} \right) =
        \begin{bmatrix}
        \frac{1}{\lambda_{1}} & 0 & \cdots & 0 \\
        0 & \frac{1}{\lambda_{2}} & \cdots & 0 \\
        0 & 0 & \ddots & 0 \\
        0 & 0 & \cdots & \frac{1}{\lambda_{n}}
        \end{bmatrix}.
    \]
\end{theorem}


\begin{theorem}
    Let $\{\ket{0}, \ket{1}, \ldots, \ket{d-1}\}$ be the computational basis of $\C^{d}$. A general diagonal matrix $\Lambda = \operatorname{diag}(k_{0}, k_{1}, \ldots, k_{d-1})$ can be expressed as
    \[
        \begin{bmatrix}
        k_{0} & 0 & \cdots & 0 \\
        0 & k_{1} & \cdots & 0 \\
        0 & 0 & \ddots & 0 \\
        0 & 0 & \cdots & k_{d-1}
        \end{bmatrix}
        = k_{0}\ket{0}\bra{0} + k_{1}\ket{1}\bra{1} + \cdots + k_{d-1}\ket{d-1}\bra{d-1}.
    \]
\end{theorem}

\begin{definition}[Matrix Power]
    For a square matrix $A$, non-negative integer powers are defined as follows:
    \[
        A^{0} = I, \qquad A^{n} = \underbrace{AA \cdots A}_{n \text{ factors}}.
    \]
    For invertible matrices, negative integer powers can also be defined:
\end{definition}

\begin{definition}[Negative Power]
    For an invertible matrix $A$, negative integer powers are defined as
    \[
        A^{-n} = \underbrace{A^{-1}A^{-1} \cdots A^{-1}}_{n \text{ factors}}.
    \]
\end{definition}

The two above definitions yields the following properties:

\begin{theorem}
    For any matrix $A$, the following properties hold:
    $$A^p A^q = A^{p+q}, \qquad (A^p)^q = A^{pq}$$
    provided all powerers involved are defined. 
\end{theorem}

A special type of matrix related to matrix powers are idempotent matrices, derived from the latin words "idem," meaning "the same," and "potent," meaning "power."

\begin{definition}[Idempotent Matrix]
    A square matrix $A$ is called idempotent if $A^2 = A$.
\end{definition}

Idenpotent matrices frequently arise in the context of projection matrices. 

\begin{definition}[Projection Matrix]
    A projection matrix onto a $n \times 1$ vector $\ketpsi$ is defined as:
    $$P_{\psi} = \ketpsi \brapsi,$$
    such that $P_{\psi}$ projects any $n \times 1$ vector onto $\ketpsi$. In this context, $\ketpsi$ and $\ketphi$ are normalized vectors. 
\end{definition}

\begin{proof}
    Consider the matrix multiplication:
    $$P_{\psi} \ketphi = \ketpsi \brapsi \ketphi = \braket{\psi}{\phi} \ketpsi$$
    which precisely follows the definition from the previous section. 
\end{proof}

\begin{theorem}
    The projection matrix $P_{\psi} = \ketpsi \brapsi$ is idempotent. 
\end{theorem}

\begin{proof}
    To verify that $P_{\psi}$ is idempotent, we can simply compute $P_{\psi}^2$:
    $$P_{\psi}^2 = \ketpsi \brapsi \ketpsi \brapsi = \ketpsi \braket{\psi}{\psi} \brapsi = \ketpsi \brapsi = P_{\psi},$$
    considering that $\braket{\psi}{\psi} = 1$ given that they are normalized vectors. 
\end{proof}

Conceptually, this follows since repeated powers of $P_\psi$ is simply the same projection over and over again. 

\begin{theorem}
    Given a diagonal matrix $\Lambda = \operatorname{diag}(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{n})$, its $d$-th power is given by:
    \[
        \Lambda^{d} =
        \begin{bmatrix}
        \lambda_{1}^{d} & 0 & \cdots & 0 \\
        0 & \lambda_{2}^{d} & \cdots & 0 \\
        0 & 0 & \ddots & 0 \\
        0 & 0 & \cdots & \lambda_{n}^{d}
        \end{bmatrix},
    \]
    where $d \in \mathbb{Z}$. If $d$ is negative, $\Lambda$ must be invertible.
\end{theorem}

Another interesting note before the end of this subsection is that we can also have polynomials for matrices. 

\begin{definition}[Matrix Polynomial]
    Given a polynomial function
    \[
        p(x) = c_{0} + c_{1}x + c_{2}x^{2} + \cdots + c_{n}x^{n},
    \]
    the polynomial function of a square matrix $A$ is defined as:
    \[
        p(A) = c_{0}I + c_{1}A + c_{2}A^{2} + \cdots + c_{n}A^{n},
    \]
    where $I$ is the identity matrix of the same size as $A$, and $p(A)$ is a square matrix of the same size as $A$.
\end{definition}

Since matrices and their powers are associative under multiplication, matrix polynomials, also following the same rules. For instance, algebraic identities such as 

$$1 - x^{n} = (1 - x)(1 + x + x^{2} + \cdots + x^{n-1})$$

translates directly over to any square matrix $A$ such as 

$$I - A^{n} = (I - A)(I + A + A^{2} + \cdots + A^{n-1}).$$

Matrix polynomials are a powerful tool for analzying square matrices, particularly in the context of spectral decomposition, diagonalization, and the Cayley-Hamilton theorem, covered in section 11. 

\subsection{Trace and Determinant}

Square matrices also contain two important scalar quantities, which are the trace and the determinant. Both quantities are foundational for linear algebra and quantum computing. While this section will briefly introduce their basic properties, it will lay the groundwork for understanding their fundamental roles and implications in both classical and quantum contexts. 

\begin{definition}[Trace]
    The trace of an $n \times n$ square matrix $A = [a_{ij}]$, denoted as $\operatorname{tr}(A)$, is defined as:
    \[
        \operatorname{tr}(A) = a_{11} + a_{22} + \cdots + a_{nn} = \sum_{i=1}^{n} a_{ii}.
    \]
\end{definition}

\begin{theorem}
    The following identities demonstrate the basic properties of the trace function for any square matrices $A$, $B$ and scalar $k$:
    \begin{align*}
        \operatorname{tr}(A^{T}) &= \operatorname{tr}(A),\\
        \operatorname{tr}(A^{\dagger}) &= \operatorname{tr}(A^{*}) = \operatorname{tr}(A)^{*},\\
        \operatorname{tr}(kA) &= k \operatorname{tr}(A) \\
        \operatorname{tr}(A + B) &= \operatorname{tr}(A) + \operatorname{tr}(B)\\
        \operatorname{tr}(AB) &= \operatorname{tr}(BA).
    \end{align*}
\end{theorem}

\begin{proof}
    Let $A = [a_{ij}]$ and $B = [b_{ij}]$, where $i,j = 1,2,\ldots,n$. Using the rowâ€“column rule, compute the entries of $AB$ and $BA$ as follows:
    \[
        (AB)_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}, \qquad
        (BA)_{ij} = \sum_{k=1}^{n} b_{ik} a_{kj}.
    \]

    The traces of $AB$ and $BA$ are then:
    \[
        \operatorname{tr}(AB)
        = \sum_{i=1}^{n} (AB)_{ii}
        = \sum_{i=1}^{n} \sum_{k=1}^{n} a_{ik} b_{ki},
    \]
    \[
        \operatorname{tr}(BA)
        = \sum_{i=1}^{n} (BA)_{ii}
        = \sum_{i=1}^{n} \sum_{k=1}^{n} b_{ik} a_{ki}.
    \]

    Since summations are symmetric with respect to $i$ and $k$, swapping $i$ and $k$ in the summation for $\operatorname{tr}(BA)$ yields:
    \[
        \operatorname{tr}(BA)
        = \sum_{i=1}^{n} \sum_{k=1}^{n} b_{ik} a_{ki}
        = \sum_{i=1}^{n} \sum_{k=1}^{n} b_{ki} a_{ik}
        = \operatorname{tr}(AB).
    \]
\end{proof}

Now the extension of the last theorerm leads to the following theorem:

\begin{theorem}
    The following identities hold true for square matrices $A, B, C$ and an intertible matrix $P$ we have the properties of \textbf{cyclic invariance}:
    $$\trop(ABC) = \trop(BCA) = \trop(CAB)$$
    and \textbf{simiarity invariance}:
    $$\trop(\inverse{P} A P) = \trop(A)$$
\end{theorem}

A common misconception upon first glance is that the product of the matrices can be shuffled at will and they will still remain the same. However, this property actually borrows from the previous definition of how $\trop(AB) = \trop(BA)$.

\begin{proof}
    Using the cyclic property of the trace, we can rearrange the product inside the trace function:
    \[
    \operatorname{tr}(ABC) = \operatorname{tr}(A(BC)) = \operatorname{tr}((BC)A) = \operatorname{tr}(BCA),
    \]
    \[
    \operatorname{tr}(ABC) = \operatorname{tr}((AB)C) = \operatorname{tr}(C(AB)) = \operatorname{tr}(CAB),
    \]
    \[
    \operatorname{tr}(P^{-1} A P) = \operatorname{tr}(A P P^{-1}) = \operatorname{tr}(A I) = \operatorname{tr}(A).
    \]
\end{proof}

The cyclic invariance property holds for the trace of a product of square matrices of any length. For example:

$$\trop(ABCD) = \trop(BCDA) = \trop(CDAB) = \trop(DABC).$$

Generally speaking, however, $\trop(ABC) \neq \trop(CAB)$ as this is not cyclic. 

\begin{theorem}
    For matrices $A \in \Rmn, B \in \C_{nm}$ it holds that:
    $$\trop(AB) = \trop{BA}.$$
\end{theorem}

This theorem leads to several quite unique and useful results in quantum computing. 

\begin{theorem}
    The following identities hold for a matrix $A \in \Cnn$ and vectors $\ketpsi, \ketphi \in \Cn$:
    $$\trop(\ketpsi \braphi) = \braket{\phi}{\psi}$$ 
    $$\trop(A\ketpsi \braphi) = \braphi A \ketpsi$$
\end{theorem}

This theorm is fundamental to describing density matrices, also known as density operators, in quantum computing. In quantum mechanics, a \textbf{pure state} is represented by a unit vector $\ketpsi \in \Cn$, or more generally in a Hilbert space. A \textbf{mixed state} represents a statistical ensemble of such pure states. A mixed state can be described by a density matrix defined as:

$$\rho = \sum_{i} = p_i \ket{\phi_i} \bra{\phi_i},$$

where $p_i \geq 0$ and $\sum_{i} p_i = 1$. Here, $\ket{\phi_i}$ represents a pure state, and the state is mixed when the sum contains many terms, with $p_i$ represents the probability of observing the pure state $\ket{\phi_i}$. 

A pure state is a special case of the mixed state, which can be expressed as a single outer product: $\rho = \ketphi \braphi$. Density matrices exhibit some very special properties: $\trop(\rho) = \trop(\rho^2) = 1$. We know that all traces of pure states are of unit trace using the above theorem. Additionally, we know that pure states have $\rho^2 = \rho$, implying that $\trop(\rho^2) = \trop(\rho) = 1$. 

For mixed states, we know that $\trop(\rho) = 1$ and that $0 < \trop(\rho^2) \leq 1$. The proof of the first property is as follows:

\begin{align*}
    \trop\left(\sum_{i} p_i \ket{\psi_i} \bra{\psi_i}\right) &= \sum_{i} \trop\left( p_i \ket{\psi_i} \bra{\psi_i}\right) \\
    &= \sum_{i} p_i \trop\left(  \ket{\psi_i} \bra{\psi_i}\right) \\
    &= \sum_{i} p_i \\
    &= 1.\\
\end{align*}


In order to demonstrate that $0 < \trop(\rho^2) \leq 1$, consider the case where ${\ket{\psi_i}}$ are the orthonormal pure states with $\braket{\psi_i}{\psi_j} = \delta_{ij}$. Since $\rho$ is Hermitian, it can always be decomposed with orthonormal pure states according to the spectral decomposition theorem. We derive:

\begin{align*}
\operatorname{tr}(\rho^{2})
&= \operatorname{tr}\!\left( \sum_{i} p_{i} \ket{\psi_{i}}\bra{\psi_{i}} \sum_{j} p_{j} \ket{\psi_{j}}\bra{\psi_{j}} \right) \\
&= \operatorname{tr}\!\left( \sum_{i} \sum_{j} p_{i} p_{j} \ket{\psi_{i}}\bra{\psi_{i}} \ket{\psi_{j}}\bra{\psi_{j}} \right) \\
&= \operatorname{tr}\!\left( \sum_{i} \sum_{j} p_{i} p_{j} \delta_{ij} \ket{\psi_{i}}\bra{\psi_{j}} \right) \\
&= \operatorname{tr}\!\left( \sum_{i} p_{i}^{2} \ket{\psi_{i}}\bra{\psi_{i}} \right)
\qquad (\because \delta_{ij} = 0 \text{ when } i \neq j) \\
&= \sum_{i} p_{i}^{2} \operatorname{tr}\!\left( \ket{\psi_{i}}\bra{\psi_{i}} \right) \\
&= \sum_{i} p_{i}^{2}.
\end{align*}

since we know that $p_i \geq 0$ and $\sum_{i} p_i = 1$ it follows that $0 < \sum_{i} p^2_i \leq \sum_{i} p_i= 1$. The trace function is also critical for expressing exptected values. In a quantum state represented by density matrix $\rho$, we can extend the expected value of an observable state $A$ from its Hermitian quadratic form, 

$$\expectation{A} = \brapsi A \ketpsi, $$

to its reformatted version

$$\expectation{A} = \brapsi A \ketpsi = \trop(A \ketpsi \brapsi) = \trop(A \rho).$$

This naturally also applies to mixed states, represented by $\rho = \sum_i p_i \ket{\psi_i} \bra{\psi_i}$, the expected value $\expectation{A}$ is a weighted average of the expected value of its pure states.

$$\expectation{A} = \sum_{i} p_i \bra{\psi_i} A \ket{\psi_i} = \sum_{i} p_i \trop(A \ket{\psi_i} \bra{\psi_i}) = \trop(A \sum_{i} p_i \ket{\psi_i} \bra{\psi_i}) = \trop(A\rho).$$

This illustrates how the trace function is instrumental in bridging the pure and mixed states in facilitating the calculation of expected values in quantum measurements. It also plays a defining role in defining the inner products, as outlined below. 

\begin{definition}[Matrix inner product]
    Given two arbitrary matrices of the same size, their inner product is defined as:
    $$\innerprod{A}{B} \equiv \trop(A^\dagger B).$$
    This inner product, also known as the Hilber-Schmidt or Frobenius inner product, is sometimes denoted as $\innerprod{A}{B}_F$ to distinguish it from other types of inner products. 
\end{definition}

This matrix inner product generalizes the vector dot product by aggregating the products of corresponding elements across all indices. This concept, manifesting as simply as the inner product between two matrices such as:

$$\innerprod{A}{B}
= \operatorname{tr}(A^{\dagger} B)
= a_{11}^{*} b_{11} + a_{12}^{*} b_{12} + a_{21}^{*} b_{21} + a_{22}^{*} b_{22}, $$

generalizes to the following theorem:

\begin{theorem}
    For matrices $A_{m \times n} = [a_{ij}]$ and $B_{m \times n} = [b_{ij}]$, the Hilbert-Schmidt inner product is given by:
    $$\trop(A^\dagger B) = \sum_{i=1}^{m} \sum_{j=1}^{n} a^*_{ij} b_{ij}.$$
\end{theorem}

\begin{proof}
    If we take the product $C = A^\dagger B$ to be an $n \times n$ matrix with elements $C_{jk}$ given by:
    $$c_{jk} = \sum_{i=1}^{m} (A^\dagger)_{ji} b_{ik} = \sum_{i=1}^{m} a^*_{ij} b_{ik}.$$
    Then, the trace of $C$ is then simply the sum of its diagonal entries:
    $$\trop(A^\dagger B) = \trop(C) = \sum_{j=1}^{n} c_{jj} = \sum_{i=1}^{n}\left( \sum_{j=1}^{m} a^*_{ij} b_{ij}\right)  = \sum_{i=1}^{m} \sum_{j=1}^{n} a^*_{ij} b_{ij}.$$
\end{proof}

This inner product also induces a narual norm for the matrices, analogous to the euclidian norm for vectors.

\begin{definition}[Frobenius Norm]
    The Frobenius norm of a matrix $A_{m \times n}$, denoted as $\norm{A}_F$, is defined as:
    $$\norm{A}_F = \sqrt{\innerprod{A}{A}_F} = \sqrt{\trop(A^\dagger A)} = \sqrt{\sum_{i=1}^{m} \sum_{j=1}^{n} \abs{a_{ij}}^2}.$$
\end{definition}


\end{document}