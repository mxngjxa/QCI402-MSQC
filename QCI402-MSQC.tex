\documentclass[12pt]{article}

% Geometry
\usepackage[legalpaper, portrait, margin=1in]{geometry}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{physics}
% \usepackage{braket}

% Graphics and drawing
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{quantikz} % For quantum circuit drawing
\usepackage{pgfplots}

% Code formatting
\usepackage{minted}

% Layout and formatting
\usepackage{titling}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{multicol}
\usepackage{xcolor}

% Tables
\usepackage{multirow}
\usepackage{array}
\usepackage{booktabs}
\usepackage{tabularx}

% Code

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}

\lstset{style=mystyle}


\usemintedstyle{manni}
\graphicspath{ {./figures/} }
\newcommand{\Rlogo}{\protect\includegraphics[height=1.8ex,keepaspectratio]{Rlogo.png}}

\input{config.tex}
% \input{quantum-commands.tex}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{postulate}[theorem]{Postulate}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

% % alt styling: theorem is separate from rest
% % Theorem: its own counter, resets each section
% \newtheorem{theorem}{Theorem}[section]

% % Create an independent base counter for the "definition group"
% \newtheorem{fact}{\!\!}[section] % invisible title; serves only as a counter

% % Make these share the "fact" counter (not the theorem counter)
% \newtheorem{definition}[fact]{Definition}
% \newtheorem{postulate}[fact]{Postulate}
% \newtheorem{corollary}[fact]{Corollary}
% \newtheorem{lemma}[fact]{Lemma}

\newtheorem*{example}{Example}

\newtheorem*{solution}{Solution}

\title{\COURSECODE\ - \FULLCOURSENAME\ - \UNIVERSITY}
\author{\PROFESSORNAME\ - \STUDENTNAME}
\date{\SEMESTER\ Semester \YEAR}

\begin{document}

\maketitle

\hfill

The underpinnings of all scientific advancements is the ability to express natural phenomena with the art of Mathmatics; this is no different for the subject of Quantum Computing. While the boundaries of quantum computing have been pushed beyond limits in theoretical terms on university blackboards, it has become of great interest to realize the theoretical computational power with the advances of hardware and technology. 

However, these notes mainly concerns itself with the mathmatical underpinnings of quantum computing that the course surrounds itself with. \FULLCOURSENAME takes a scaffolding approach designed to efficiently convey the required theoretical understanding of mathmatics in order to able to learn quantum computing. As of writing, we are basing the notes on verison one of the textbook published in March 2025. In this text, we will primarily be using dirac notation for the expression of vectors, operators, and their interactions. 

\tableofcontents

\break


\section{Summation and Product Notations}

This section primarily focuses on the common notations applied across mathmatics to denote and shorten addition and product notation. 

\subsection{Summation over a single Variable}

The sigma notation is defined as follows

$$\sum_{i=1}^{n}f(i)$$

where we use sigma $\sum$ to represent the sum of a series. For example, the sum of all numbers in a series beginning with $m$ and ending at index $n$ is written as:

$$\sum_{i=m}^{n} a_i = a_m + a_{m+1} + a_{m+2} + \cdots + a_{n-1} + a_n$$

Sums can also be infinite, commonly seen when Sigma looks as follows: $\sum^{\infty}_{i=m}$. Infinite sums are either convergent or divergent. A few of the most common converging infinite sums are as follows:

$$\sum_{i=0}^{\infty} \frac{1}{2^i} = 1 + \frac{1}{2} + \frac{1}{4} + \cdots = 2$$
$$\sum_{i=0}^{\infty}  \frac{1}{i^2} = \frac{1}{1^2} + \frac{1}{2^2} + \frac{1}{3^2} + \cdots = \frac{\pi^2}{6}$$

The first example is an infinite geometric series, and the sum of the first $n$ terms is given by:

$$S_n = \sum_{i=0}^{n} \frac{1}{2^i} = \frac{1 - \frac{1}{2^n}}{1 - \frac{1}{2}}$$

As $n \rightarrow \infty, \frac{1}{2^n} \rightarrow 0$. Consequently, $S_n \rightarrow \frac{1}{1 - \frac{1}{2}} = 2$. A rigorous proof of the second example requires extensive calculus and is not immediately obvious. While any mathmatical symbol can be used for the index of a summation, it is more practical to use something other than $i$ as in the context of complex numbers, $i$ commonly denotes the complex number$\sqrt{-1}$. moreover, sume can also be specified using descriptions. For example, 

$$\sum_{p \in P} f(p) \qquad P \in \mathbb{N^\prime}$$

where $\mathbb{N^\prime}$ is the set of all prime numbers. Summations can also contain parameters other than the index, which results in functions of those parameters. For example the discrete Fourier transform (DFT) is given by

$$\tilde{x}_k = \frac{1}{\sqrt{N}} \sum_{n=0}^{N - 1} x_n e^{- \frac{2 \pi i}{N} kn}, \quad k = 0, 1, \cdots N-1$$

where $x_n$ represents the $N$ values index by $n$ and $\tilde{x}_k$ are the Fourier coefficients. Here, $i$ is the imaginary numebr and $N$ is a positive integer representing the dimension fo the DFT, of which we will cover in greater depth in Chapter 3. The following are some useful summation forumae commonly encountered in quantum computing:

\[
\sum_{i=1}^{n} i = \frac{n(n+1)}{2}
\]

\[
\sum_{i=1}^{n} i^2 = \frac{n(n+1)(2n+1)}{6}
\]

\[
\sum_{i=1}^{n} i^3 = \left(\frac{n(n+1)}{2}\right)^2
\]

\[
\sum_{i=0}^{n} \left(a_0 + id\right) = (n+1)\left(a_0 + \frac{nd}{2}\right) \quad \text{(arithmetic series)}
\]

\[
\sum_{i=0}^{n} a^i = \frac{1 - a^{n+1}}{1 - a} \quad \text{(geometric series)}
\]

\[
(a + b)^n = \sum_{i=0}^{n} \binom{n}{i} a^{n-i} b^i \quad \text{(binomial theorem)}
\]

\[
\frac{1}{1 - x} = \sum_{n=0}^{\infty} x^n = 1 + x + x^2 + x^3 + \cdots \quad (\lvert x \rvert < 1)
\]

\[
\frac{1}{(1-x)^2} = \sum_{n=1}^{\infty} n x^{n-1} = 1 + 2x + 3x^2 + 4x^3 + \cdots \quad (\lvert x \rvert < 1)
\]

\[
\ln(1 + x) = \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n} x^n = x - \frac{x^2}{2} + \frac{x^3}{3} - \cdots \quad (\lvert x \rvert < 1)
\]

\[
e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots
\]

\[
\sin x = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n+1)!} x^{2n+1} = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots
\]

\[
\cos x = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!} x^{2n} = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \cdots
\]

Below are also a list of the common summations rules and manupulations:

\[
\sum_{i=m}^{n} a_i = \sum_{j=m}^{n} a_j \quad \text{(change of index variable)}
\]

\[
\sum_{i=s}^{t} f(i) = \sum_{n=s}^{t} f(n) \quad \text{(change of index variable)}
\]

\[
\sum_{n=s}^{t} f(n) = \sum_{n=s}^{j} f(n) + \sum_{n=j+1}^{t} f(n) \quad \text{(splitting a sum)}
\]

\[
\sum_{n=s}^{t} f(n) = \sum_{n=0}^{t-s} f(t-n) \quad \text{(reverse order)}
\]

\[
\sum_{n=s}^{t} f(n) = \sum_{n=s+p}^{t+p} f(n-p) \quad \text{(index shift)}
\]

\[
\sum_{n=s}^{t} a \cdot f(n) = a \cdot \sum_{n=s}^{t} f(n) \quad \text{(distributivity)}
\]

\[
\sum_{n=s}^{t} f(n) \pm \sum_{n=s}^{t} g(n) = \sum_{n=s}^{t} \left(f(n) \pm g(n)\right) \quad \text{(commutativity)}
\]


\subsection{Products and other Notations}

Similar to the $\sum$ notation for addition, the $\prod$ (Pi) symbol is also more commonly used to dentoe the product of a series of terms. In this 

$$\prod_{i=m}^{n} a_i = a_m \cdot a_{m+1} \cdot a_{m+2} \cdot \cdots \cdot a_{n-1} \cdot a_n$$ 

for example, the factorial of $n$ is expressed as 

$$\prod_{i=0}^{n} i = n!$$

and the relationship between $\sum$ and $\prod$, which are 

$$b^{\sum_{n=s}^{t} f(n)} = \prod_{n=s}^{t} b^{f(n)}$$

$$\sum_{n=s}^{t} \log_b f(n) = log_b \prod_{n=s}^{t} f(n)$$

It is worth noting that in quantum computing and linear algebra, there are a few special notations such as the modulo-2 sum (bitwise XOR), or in other contexts the direct sum of linear spaces, represented by $\oplus$, and the tensor product represented by $\otimes$.

\subsection{Summation over Multiple Variables}

The double summation over a rectangular array is given by 

\begin{align*}
    \sum_{i=1,j=1}^{n_1, n_2} a_{i,j} &= \sum_{i=1}^{n_1} \sum_{j=1}^{n_2} a_{i, j} = \sum_{j=1}^{n_2} \sum_{i=1}^{n_1} a_{i, j} \\
    &= a_{1, 1} + a_{1, 2} + a_{1, 3} + a_{1, 4} + \cdots + a_{1, n_2} \\
    &+ a_{2, 1} + a_{2, 2} + a_{2, 3} + a_{2, 4} + \cdots + a_{2, n_2} \\
    & + a_{3, 1} + a_{3, 2} + a_{3, 3} + a_{3, 4} + \cdots + a_{3, n_2} \\
    & + a_{4, 1} + a_{4, 2} + a_{4, 3} + a_{4, 4} + \cdots + a_{4, n_2} \\
    & + \cdots \\
    & + a_{n_1, 1} + a_{n_1, 2} + a_{n_1, 3} + a_{n_1, 4} + \cdots + a_{n_1, n_2}
\end{align*}

Here, $\sum_{i=1}^{n_1} \sum_{j=1}^{n_2}$ represents summing over each row first and then summing the results, while $\sum_{j=1}^{n_2} \sum_{i=1}^{n_1}$ will represent summing over the columns and then summing those results. The term $\sum_{i=1,j=1}^{n_1, n_2} a_{i,j}$ represents the summation over the rectangular array, irrespecive of the order. The product of two sums can be expanded into a double sum as follows:

\begin{align*}
\left( \sum_{i=1}^{m} a_i \right) \left( \sum_{j=1}^{n} b_j \right)
    &= (a_1 + a_2 + \cdots + a_m)(b_1 + b_2 + \cdots + b_n) \\
    &=\; a_1b_1 + a_1b_2 + a_1b_3 + a_1b_4 + \cdots + a_1b_n \\
    &+ a_2b_1 + a_2b_2 + a_2b_3 + a_2b_4 + \cdots + a_2b_n \\
    &+ a_3b_1 + a_3b_2 + a_3b_3 + a_3b_4 + \cdots + a_3b_n \\
    &+ \cdots \\
    &+ a_mb_1 + a_mb_2 + a_mb_3 + a_mb_4 + \cdots + a_mb_n \\
    &= \sum_{i=1}^{m} \sum_{j=1}^{n} a_i b_j = \sum_{i=1}^{m} a_i \sum_{j=1}^{n} b_j
\end{align*}

which is actually rather intuitive given how the expansion of the standard expansion of the term $(a+b)^2$ plays out, a more elementary application of the distributive property which the above equation generalizes over. For a triangular matrix, in this case the lower triangular matrix, the sum is given by

\begin{align*}
    \sum_{1 \leq j \leq n}^{} a_{i,j} &= \sum_{i=1}^{n} \sum_{j=1}^{i} a_{i, j} = 
    \sum_{j=1}^{n} \sum_{i=j}^{n} a_{i, j} = 
    \sum_{j=0}^{n-1} \sum_{j=1}^{n-j} a_{i+j, i} \\
    &= a_{1, 1} \\
    & + a_{2, 1} + a_{2, 2}  \\
    & + a_{3, 1} + a_{3, 2} + a_{3, 3} \\
    & + a_{4, 1} + a_{4, 2} + a_{4, 3} + a_{4, 4} \\
    & + \cdots \\
    & + a_{n, 1} + a_{n, 2} + a_{n, 3} + a_{n, 4} + \cdots + a_{n, n}
\end{align*}

where the term $\sum_{1 \leq j \leq n}^{} a_{i,j}$ denotes the summation over all elements in a lower triangular array including the diagonal. The first notation variation will sum up each row to the $i$th element then aggregate while the second notation sums each column starting from the $j$th element downwards then aggregate the sums. The final expression will sum along the diagonal where $j=0$ represents the main diagonal and $j=n-1$ is the first off-diagonal, which is a single term. 

\begin{example}
    Say we would like to expand the product of $\left(1 + x_i\right)$ from $1$ to $n$. We have 

    $$\prod_{i=1}^{n} \left(1 + x_i\right) = 1 + \sum_{k=1}^{n} \left(\sum_{1 \leq i_1 < \cdots < i_k \leq n} \prod_{j=1}^{k} x_{i_j}\right)$$
\end{example}

This formula represents the \textit{multinomial expansion} of a product. When you expand the equation by hand, you get the product

$$\prod_{i=1}^{n} \left(1 + x_i\right) = (1 + x_1)(1 + x_2)\cdots(1 + x_n)$$

If we break this down, we see that the outer summation $\sum_{k=1}^{n}$ will go through each possible summation size in terms of the variables in question, and that the inner summation $\sum_{1 \leq i_1 < \cdots < i_k \leq n}$ will iterate through each possible unique product of the variables. while ensuring that they are unique. Not sure how this works, but if all $x_i$ are the same, then wesee that the equation actually simplifies to a subset of the binomial theorem 

$$(1 + x)^n = \sum_{k=0}^{n} \binom{n}{k} x^k$$

where $\binom{n}{k}$ is the binomial coefficient representing the numebr of ways to choose $k$ elements from a set of $n$ distinct elements.

\break

\section{Trigonometry}

You can't escape this.

\subsection{Definitions}

I mean, where do I start? The basic trigonometric functions are defined as the ratios between the angles of a right triangle. I will not show how these ratios remain the same given the same angle, nor will I go into great mathmatical detail of how to prove these items. However, we still have to go over this. Don't ask me why. 

\includegraphics[width=\textwidth]{2_trig0}

The functions of trigonometric functions can also be plotted out as follows:

\includegraphics[width=\textwidth]{2_trig1}

\subsection{Basic Properties and Inverse Functions}

\includegraphics[width=\textwidth]{2_trig2}

% --- 1. Trig Functions ---
\begin{center}
\begin{tikzpicture}[scale=1.5]
\begin{axis}[
    axis lines=middle,
    xlabel={}, ylabel={},
    xmin=-pi, xmax=pi,
    ymin=-5, ymax=5,
    restrict y to domain=-5:5,
    samples=300,
    legend style={at={(1.05,1)}, anchor=north west},
    domain=-2*pi:2*pi
]
\addplot[blue, thick] {sin(deg(x))};
\addlegendentry{$\sin(x)$}

\addplot[red, thick] {cos(deg(x))};
\addlegendentry{$\cos(x)$}

\addplot[green!70!black, thick] {tan(deg(x))};
\addlegendentry{$\tan(x)$}

\addplot[orange, thick, domain=-2*pi:-0.1] {1/sin(deg(x))};
\addplot[orange, thick, domain=0.1:2*pi] {1/sin(deg(x))};
\addlegendentry{$\csc(x)$}

\addplot[yellow, thick, domain=-2*pi:-pi/2-0.1] {1/cos(deg(x))};
\addplot[yellow, thick, domain=-pi/2+0.1:pi/2-0.1] {1/cos(deg(x))};
\addplot[yellow, thick, domain=pi/2+0.1:2*pi] {1/cos(deg(x))};
\addlegendentry{$\sec(x)$}

\addplot[brown, thick, domain=-2*pi:-0.1] {cos(deg(x))/sin(deg(x))};
\addplot[brown, thick, domain=0.1:2*pi] {cos(deg(x))/sin(deg(x))};
\addlegendentry{$\cot(x)$}

\end{axis}
\end{tikzpicture}
\end{center}

We can also see that there are certain useful symmetric properties of the trigonometric functions

\begin{align*}
\sin(-\theta) &= - \sin(\theta)           &  \sin(\pi-\theta) &= \sin(\theta)              &  \sin(\pi+\theta) &= - \sin(\theta)\\
\cos(-\theta) &= \cos(\theta)         &  \cos(\pi-\theta) &= - \cos(\theta)   &  \cos(\pi+\theta) &= - \cos(\theta)\\
\tan(-\theta) &= - \tan(\theta)   &  \tan(\pi-\theta) &= - \tan(\theta)         &  \tan(\pi+\theta) &= \tan(\theta)
\end{align*}

There are also some common inverse functions associated with the functions.

\includegraphics[width=\textwidth]{2_trig4}

But also some interesting extensions of the commonly known inverse functions, with the example being $\mathrm{arctan2}$, a function that effectively doubles the domain of the function while preserving its properties for the purpose of, say, converting values from cartesian to spherical coordinates for the azimuthal angle $\phi$.

\[
\mathrm{arctan2}(y, x) = 
\begin{cases}
  \arctan\left(\dfrac{y}{x}\right) & \text{if } x > 0, \\[2ex]
  \arctan\left(\dfrac{y}{x}\right) + \pi & \text{if } x < 0 \text{ and } y \geq 0, \\[2ex]
  \arctan\left(\dfrac{y}{x}\right) - \pi & \text{if } x < 0 \text{ and } y < 0, \\[2ex]
  +\dfrac{\pi}{2} & \text{if } x = 0 \text{ and } y > 0, \\[2ex]
  -\dfrac{\pi}{2} & \text{if } x = 0 \text{ and } y < 0, \\[2ex]
  0 & \text{if } x = 0 \text{ and } y = 0.
\end{cases}
\]

\includegraphics[width=\textwidth]{2_Arctangent2}

\subsection{Special Angles and Function Values}

There are a few special angles that are worth remembering for the trigonometric functions mentioned above, given by the wheel below.

\includegraphics[width=\textwidth]{2_trig6}

\subsection{Trigonometric Identities}

\subsubsection*{Reciprocal and Quotient Identities}

\begin{align*}
\csc \theta &= \frac{1}{\sin \theta} &
\sec \theta &= \frac{1}{\cos \theta} &
\cot \theta &= \frac{1}{\tan \theta} \\
\end{align*}

\begin{align*}
    \tan \theta &= \frac{\sin \theta}{\cos \theta}, &
\cot \theta &= \frac{\cos \theta}{\sin \theta}
\end{align*}

\subsubsection*{Cofunction Identities}
\begin{align*}
\sin\!\left(\frac{\pi}{2} - \theta \right) &= \cos \theta, &
\cos\!\left(\frac{\pi}{2} - \theta \right) &= \sin \theta \\
\tan\!\left(\frac{\pi}{2} - \theta \right) &= \cot \theta, &
\cot\!\left(\frac{\pi}{2} - \theta \right) &= \tan \theta \\
\sec\!\left(\frac{\pi}{2} - \theta \right) &= \csc \theta, &
\csc\!\left(\frac{\pi}{2} - \theta \right) &= \sec \theta
\end{align*}

\subsubsection*{Pythagorean Identities}
\begin{align*}
\sin^2 \theta + \cos^2 \theta &= 1 \\
1 + \tan^2 \theta &= \sec^2 \theta \\
1 + \cot^2 \theta &= \csc^2 \theta
\end{align*}

\subsubsection*{Evenâ€“Odd Symmetry}
\begin{align*}
\sin(-\theta) &= -\sin \theta, &
\cos(-\theta) &= \cos \theta, &
\tan(-\theta) &= -\tan \theta \\
\csc(-\theta) &= -\csc \theta, &
\sec(-\theta) &= \sec \theta, &
\cot(-\theta) &= -\cot \theta
\end{align*}

\subsubsection*{Sum and Difference Formulas}
\begin{align*}
\sin(\alpha \pm \beta) &= \sin \alpha \cos \beta \pm \cos \alpha \sin \beta \\
\cos(\alpha \pm \beta) &= \cos \alpha \cos \beta \mp \sin \alpha \sin \beta \\
\tan(\alpha \pm \beta) &= \frac{\tan \alpha \pm \tan \beta}{1 \mp \tan \alpha \tan \beta}
\end{align*}

\subsubsection*{Double Angle Formulas}
\begin{align*}
\sin(2\theta) &= 2\sin \theta \cos \theta \\
\cos(2\theta) &= \cos^2 \theta - \sin^2 \theta \\
&= 2\cos^2 \theta - 1 \\
&= 1 - 2\sin^2 \theta \\
\tan(2\theta) &= \frac{2\tan \theta}{1 - \tan^2 \theta}
\end{align*}

\subsubsection*{Half Angle Formulas}
\begin{align*}
\sin^2\!\left(\frac{\theta}{2}\right) &= \frac{1 - \cos \theta}{2} \\
\cos^2\!\left(\frac{\theta}{2}\right) &= \frac{1 + \cos \theta}{2} \\
\tan\!\left(\frac{\theta}{2}\right) &= \frac{\sin \theta}{1 + \cos \theta} 
= \frac{1 - \cos \theta}{\sin \theta}
\end{align*}

\subsubsection*{Product-to-Sum Identities}
\begin{align*}
\sin \alpha \sin \beta &= \frac{1}{2}\big[\cos(\alpha - \beta) - \cos(\alpha + \beta)\big] \\
\cos \alpha \cos \beta &= \frac{1}{2}\big[\cos(\alpha - \beta) + \cos(\alpha + \beta)\big] \\
\sin \alpha \cos \beta &= \frac{1}{2}\big[\sin(\alpha + \beta) + \sin(\alpha - \beta)\big]
\end{align*}

\subsubsection*{Sum-to-Product Identities}
\begin{align*}
\sin \alpha + \sin \beta &= 2 \sin\!\left(\frac{\alpha + \beta}{2}\right) 
                           \cos\!\left(\frac{\alpha - \beta}{2}\right) \\
\sin \alpha - \sin \beta &= 2 \cos\!\left(\frac{\alpha + \beta}{2}\right) 
                           \sin\!\left(\frac{\alpha - \beta}{2}\right) \\
\cos \alpha + \cos \beta &= 2 \cos\!\left(\frac{\alpha + \beta}{2}\right) 
                           \cos\!\left(\frac{\alpha - \beta}{2}\right) \\
\cos \alpha - \cos \beta &= -2 \sin\!\left(\frac{\alpha + \beta}{2}\right) 
                           \sin\!\left(\frac{\alpha - \beta}{2}\right)
\end{align*}

A rather nice photo to sum the secion up is by relating the angles to each other using the following image

\includegraphics[width=\textwidth]{2_trig7.png}

\subsection{The Spherical Coordinate System}

While the expansion of the cartesian coordinate system into three dimensions is the logical linear expansion to take, some interesting basis begin to form if we consider the angles as a unit of measurement in three dimensions

\includegraphics[width=\textwidth]{2_trig8.png}

The cylindrical coordinate system elevates the polar coordinate system into three dimensions with the addition of the $z$ axis, while the spherical system is a more organic translation of the polar coordinate system into three dimensions. While the former is best suited for describing not only cylinder-like structures yet also helices, while the latter is best suited for rotations in three-dimensional space, which is particularly potent in the field of quantum computing. 

For example, the spherical coordinate system is commonly used to represent qubit states on the Bloch sphere, employing a radius $(r)$, a polar angle $(\theta)$, and an azimuthal angle $(\phi)$ to represent a point in three-dimensional space. The azimuthal angle $\phi$ is measured in the $xy$-plane from the positive $x$-axis with common values ranging from $(-\pi, \pi]$ or $(0, 2\pi]$. The polar angle is commonly measured from the positive $z$ axis towards the $xy$-plane, with values ranging from $[0, \pi]$. Note that $r \in \mathbb{R}$, meaning that we can cover the other half of the range simply by flipping the sign around. 

Conversion is relatively simple, with conversion to and from spherical to cartesian being as follows

\begin{align*}
    x &= r \sin \theta \cos \phi &     r &= \sqrt{x^2 + y^2 + z^2} \\
    y &= r \sin \theta \sin \phi &     \phi &= \mathrm{arctan2} (y, x) \\
    z &= r \cos \theta &         \theta &= \arccos \frac{z}{r}
\end{align*}

where $\mathrm{arctan2}$ was previously defined as a optimal inverse mapping onto the range of $[- \pi, \pi]$. We also have a few definitions

\begin{definition}
    The Law of Sines is defined as 

    $$\frac{\sin A}{a} = \frac{\sin B}{b} = \frac{\sin C}{c}$$
\end{definition}

\begin{definition}
    The Law of Cosines is defined as 

    $$a^2 = b^2 + c^2 - 2bc \cos A$$

    which can be rewritten as 

    $$\cos A = \frac{b^2 + c^2 - 2bc}{a^2}$$
\end{definition}

\begin{definition}
    The Law of Tangents is defined as 

    $$\frac{a - b}{a + b} = \frac{\tan \frac{1}{2}(A - B)}{\tan \frac{1}{2}(A + B)}$$
\end{definition}


\break


\section{Complex Numbers}

We consider numbers to be complex when they compose of a real and imaginary part, and they are not only fundamental to a complete understanding of algebra and mathmatics as a whole, but also form the backbone of quantum mechanics, and, by extension, quantum computing. Mastering complex numbers is like Rosie mastering the rivet gun, so we have to study it. 

\subsection{Cartesian Form}


\begin{definition}
    A complex number $z$ is defined as 

    $$
        z = x + iy, \qquad x, y \in \mathbb{R}, \quad i^2 = -1
    $$
\end{definition}

This is called the \textbf{cartesian form} of the complex number $z$ and corresponds to a point in the two-dimensional complex plane. We commonly refer to $i$ as the imaginary unit. It may seem ironic that we need imaginary numbers in quantum computing, or that we really need the imaginary number. Take it as you may. 

\includegraphics[width=\textwidth]{3_cn0.png}

% \[
% \begin{tikzpicture}[scale=1.2]
%   % Axes
%   \draw[->] (-0.2,0) -- (6,0) node[below right] {$\mathrm{Re}$};
%   \draw[->] (0,-3.2) -- (0,3.2) node[above left] {$\mathrm{Im}$};

%   % Points and projections
%   \coordinate (O) at (0,0);
%   \coordinate (Z) at (5,2.4);
%   \coordinate (Zc) at (5,-2.4);

%   \draw[dashed] (Z) -- (5,0) node[below] {$x$};
%   \draw[dashed] (Z) -- (0,2.4) node[left] {$y$};

%   \draw[dashed] (Zc) -- (5,0);
%   \draw[dashed] (Zc) -- (0,-2.4) node[left] {$-y$};

%   % Vectors
%   \draw[line width=1.6pt,orange] (O) -- (Z) node[midway,above right] {$r$};
%   \draw[line width=1.6pt,orange,dashed] (O) -- (Zc) node[midway,below right] {$r$};

%   % Angles
%   \draw (0.9,0) arc (0:25:0.9);
%   \node at (1.2,0.35) {$\theta$};

%   \draw (0.9,0) arc (0:-25:0.9);
%   \node at (1.3,-0.35) {$-\theta$};

%   % Dots
%   \fill (Z) circle (2pt);
%   \fill (Zc) circle (2pt);

%   % Labels near points
%   \node[above right] at (Z) {$z=x+iy=e^{i\theta}$};
%   \node[below right] at (Zc) {$z^*=x-iy=e^{-i\theta}$};
% \end{tikzpicture}
% \]


Complex numbers not motivated by quantum computing. In the numbers system, we have the real numbers $\mathbb{N}$, the integer numbers $\mathbb{Z}$, the real numbers $\mathbb{R}$, and the complex numbers $\mathbb{C}$. the set incursions go this way $\mathbb{N} \subset \mathbb{Z} \subset \mathbb{R} \subset \mathbb{C}$, all supersets of the preceeding set. The set of all complex numbers $\mathbb{C}$ is closed over all algabraic operations, which include addition, substraction, multiplication, division, power, and root, and is considered the superset of all numbers. 

\begin{definition}
    The basic components of a complex number are defined as follows. 

    $$\real(z) = x, \imaginary(z) = y$$

    which are the real and imaginary components of a complex number $z$. Of course the complex number itself has a few interesting properties, such as $i^2 = -1$, $i^3 = -i$, and $i^4 = 1$. The complex conjugate of a complex number $z$ is defined as 

    $$z^* = x - iy$$

    defined as inverting the sign of the imaginary component. We can express the modulus (vector length) and argument (angle with respect to the real axis) (which are $r$ and $\theta$ in polar coordinates), as follows:

    $$r = \vert z \vert = \sqrt{zz^*} = \sqrt{x^2 + y^2}$$

    A very convenient property derived from algebra is that $zz^* = x^2 + y^2$.

    $$\theta = arg(z)$$

    For the angle, we note that

    $$\tan \frac{y}{x} \Longrightarrow \mathrm{arctan2}(x, y)$$

    where $\mathrm{arctan2}$ has been defined in the previous section. 
\end{definition}

\begin{example}
    Given $z = 1 + \sqrt{3}i$, we have

    $$z^* = 1-\sqrt{3}\,i.$$

    $$|z|=\sqrt{1^2+(\sqrt{3})^2}=2.$$

    $$zz^*=(1+\sqrt{3}\,i)(1-\sqrt{3}\,i)=1-(\sqrt{3}\,i)^2=1+3=4=|z|^2.$$

    $$\theta=\arctan\!\left(\dfrac{\sqrt{3}}{1}\right)=\dfrac{\pi}{3}.$$

\end{example}


\subsection{Exponential Form}

Now it is worth noting that while we commonly write $z = x + iy$ to represent a complex numnber, we like to use the following definitions of the complex number in polar form to represent a complex number itself, defined as $z = r (\cos \theta + i \sin \theta)$. However, multiplication and its inverse operation, division, becomes unnecessarily difficult givne the presence of another notation, namely \textbf{exponential form}. 


\begin{definition}
    The exponential/euler forms of the complex numbers can be thought of as a circular form of the function $z = x + iy$. In polar coordinates, we can rewrite this number as 

    $$
        z = r \cos \theta + i \sin \theta, \qquad r \in \mathbb{R}
    $$
    Conversely, the conversion between cartesian and polar are 
    
    $$
    x = r \cos \theta \qquad y = r \sin \theta
    $$
    
    The formula for $z$ above can be rewritten as 

    $$z = r e^{i\theta}$$
\end{definition}


\begin{theorem}
    Euler's formula states that for any complex number $z = r \cos \theta + i \sin \theta$, we have:

    $$
    e^{i \theta} = \cos \theta + i \sin \theta
    $$
\end{theorem}

\begin{proof}
    Euler's formula can be proven using the Taylor series expansion for the functions:

    \[
    e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots
    \]

    \[
    \sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots
    \]

    \[
    \cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \cdots
    \]

    if we replace $x$ in the formula for $e^x$ with $e^{ix}$, we then have 

    \begin{align*}
    e^{ix} &= 1 + (ix) + \frac{(ix)^2}{2!} + \frac{(ix)^3}{3!} + \frac{(ix)^4}{4!} + \frac{(ix)^5}{5!} + \frac{(ix)^6}{6!} + \frac{(ix)^7}{7!} + \cdots \\
    &= 1 + i x - \frac{x^2}{2!} - i \frac{x^3}{3!} + \frac{x^4}{4!} + i \frac{x^5}{5!} - \frac{x^6}{6!} - i \frac{x^7}{7!} + \cdots \\
    &= \left(1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \cdots \right) + \left(i x - i \frac{x^3}{3!} + i \frac{x^5}{5!}- i \frac{x^7}{7!} + \cdots\right) \\
    &= \cos \theta + i \sin \theta
    \end{align*}

    which sums up essentially what mathmaticians call the most beautiful proof man has known. If this proof were a female robot, mathematicians would compose harmonic waves and produce digital flowers in \LaTeX to please Euler's genius. 
\end{proof}

As noted before, we know that the set of all algebraic operations is well defined and closed on the set of all complex numbers $\mathbb{C}$. Addition will be easier in cartesian form, while multiplication will be considerably simpler in exponential form. Conversion between the two is also not difficult:

\includegraphics[width=\textwidth]{3_cn2}

% $$
% \begin{array}{l|l|l}
%  & \text{Cartesian} & \text{Exponential} \\
% \hline
% \text{} & z = x + iy & z = r e^{i\theta} \\
% \text{Conjugate} & z^{*} = x - iy & z^{*} = r e^{-i\theta} \\
% \text{Modulus} & |z| = \sqrt{x^{2}+y^{2}} & |z| = r \\
% \text{Conversion} & x = r\cos\theta,\; y = r\sin\theta & r = \sqrt{x^{2}+y^{2}},\; \theta=\operatorname{arctan2}(y,x) \\
% \end{array}
% $$


\subsection{Basic Operations}

As we touched upon earlier, the set of all complex numbers are closed on operations of addition and subtraction:

$$z_1+z_2=(x_1+x_2)+i(y_1+y_2).$$

$$z_1-z_2=(x_1-x_2)+i(y_1-y_2).$$

As well on multiplication and division. For 

$$z_1=r_1 e^{i\theta_1} \qquad z_2=r_2 e^{i\theta_2}$$ 

We have:

$$z_1\cdot z_2=r_1 r_2 e^{i(\theta_1+\theta_2)}.$$

$$\displaystyle \frac{z_1}{z_2}=\frac{r_1 e^{i\theta_1}}{r_2 e^{i\theta_2}}=\frac{r_1}{r_2}e^{i(\theta_1-\theta_2)}.$$

We also have the following properties for the conjugates of complex numbers

\begin{align*}
    \vert z \vert &= \vert z^* \vert \\
    (z_1 \pm z_2)^* &= z_1^* \pm z_2^* \\
    (z_1 \cdot z_2)^* &= z_1^* \cdot z_2^* \\
    (z_1 / z_2)^* &= z_1^* / z_2^* \\
    (z^x)^* &= (z^*)^x \quad x \in \mathbb{R}\\
    (x^z)^* &= x^{z^*} \quad x \in \mathbb{R}
\end{align*}

Where the last two are not immediately obvious. To prove that $(z^x)^* = (z^*)^x$, it is useful to write out $z$ using the complex notation $r e^{i\theta}$, and the last property is best proven using the identity $a^b = e^{b \ln a}$. As for powers and roots of complex numbers, we have 

\begin{theorem}
    De Moivre's theorem states that

    $$(\cos \theta + i \sin \theta)^s = \cos{s \theta} + i \sin{s \theta}$$

    which is conveniently derived from the fact that $z^s = r^s e^{is\theta}$.
\end{theorem}

In particular, an application of this theorem is where we describe the roots of unity. Any root of unity can be descibed by a power of the first root of unity, 

$$\omega_1 = e^\frac{2 \pi i}{n}$$

the $n$-th roots of unity ($n \in \mathbb{N}$) are given by

$$
\omega_k = e^{\frac{2\pi i}{n} k} = w_1^k \qquad k = 1, 2, \ldots, n-1
$$

which essentially says that there are $n$ roots to the complex polynomial. For $n=5$, we have the 5 roots of unity given by

\includegraphics[width=\textwidth]{3_roots_unity}

In general, we say that there are $n$ values of $k$ that satisfy the equation $\displaystyle \omega_1^n = e^{(\frac{2 \pi i}{n})^n} = 1$. From this, we can generalize what we know into the summations over $\omega_k$, which is any $k$-th root of unity except for $\omega = 1$. 

$$\sum_{k=0}^{n-1} \omega_k = \sum_{k=0}^{n-1} \omega^k_1 = 0$$

This formula can be conviniently proven by applying the formula for summing a geometric sequence to the summation. From this, we can conviniently derive a useful mathmatical condition, being

\begin{example}
    The DFT Orthonormality condition depends on two parameters $k$ and $l$, and is stated as follows

    $$
    \frac{1}{N} \sum_{n=0}^{N-1}e^{- \frac{2 \pi i}{N}kn} e^{\frac{2 \pi i}{N} ln} = \delta_{k-l \bmod N}
    $$
\end{example}

where $\delta_{k-l} \pmod{N} = 1$ if and only if $k \equiv l \pmod{N}$, else $0$. It is saying that when $k$ is congruent to $l$, equivalent to $k - l = mN$, where the difference between $k$ and $l$ is divisible by some integer $m$. The $\delta_{k-l \bmod N}$ term is a Kronecker delta of $k \equiv l \pmod{N}$, where the result is $1$ if $k \equiv l \pmod{N}$ holds and $0$ in the case of $k \ncong l \pmod{N}$. If we define $\omega = \displaystyle  e^{i \frac{2 \pi}{N}}$ as a primitive $N$th root of unity (satisfying $\omega^N = 1$), we have the derivation

$$
\begin{aligned}
\frac{1}{N}\sum_{n=0}^{N-1} e^{-\frac{2\pi i}{N}kn}\, e^{\frac{2\pi i}{N}ln}
&= \frac{1}{N}\sum_{n=0}^{N-1} \omega^{-kn}\omega^{ln} \\
&= \frac{1}{N}\sum_{n=0}^{N-1} \omega^{n(l-k)} \\
&=
\begin{cases}
\displaystyle \frac{1}{N}\sum_{n=0}^{N-1} 1 = 1, & \text{if } l \equiv k \pmod N,\\
\displaystyle \frac{1}{N}\,\frac{1-\omega^{(l-k)N}}{1-\omega^{(l-k)}} = 0, & \text{if } l \not\equiv k \pmod N
\end{cases}
\\
&= \delta_{k-l \bmod N},
\end{aligned}
$$

where we used the fact that $\omega^{n^N} = \left(\omega^N\right)^n = 1^n = 1$ for $n \in \mathbb{N}$.

\subsection{Advanced Operations}

It's probably best to illustrate more advanced operations on complex numbers with the help of some examples

\begin{example}
    Evaluating $\sqrt{i}$ or $\sqrt{\sqrt{1}}$ gives:

    $$\sqrt{i} = \left(e^{\frac{\pi i}{2}}\right)^{\frac{1}{2}} = e^{\frac{\pi i }{4}} = \cos \frac{\pi}{4} + i \sin \frac{\pi}{4} = \frac{1}{\sqrt{2}} (1 + i)$$

    The inverse is given by

    $$\left(\frac{1}{\sqrt{2}} (1 + i)\right)^2 = \frac{1}{2}(1 + 2i + i^2) = \frac{(2i)}{2} = i$$
\end{example}


\begin{example}
    Evaluating $$ \left(\frac{1}{2}+\frac{\sqrt{3}}{2}i\right)^{50} $$ gives

    $$
    \left(\frac{1}{2}+\frac{\sqrt{3}}{2}i\right)^{50}
    =\left(e^{\frac{\pi i}{3}}\right)^{50}
    =e^{\frac{50\pi i}{3}}
    =e^{\left(16+\frac{2}{3}\right)\pi i}
    =e^{\frac{2\pi i}{3}}
    =-\frac{1}{2}+\frac{\sqrt{3}}{2}i.
    $$
\end{example}


\begin{example}
    Evaluating $$2^{3+4i}$$ gives us
    $$
    2^{3+4i}=2^3\cdot 2^{4i}=8\cdot e^{4\ln(2)\,i}
    =8\cos\!\big(4\ln 2\big)+i\,8\sin\!\big(4\ln 2\big).
    $$

\end{example}

\begin{example}
    Evaluating $$\cos(3+4i)$$
    gives us
    \begin{align*}
        \cos(3+4i) &= \frac{1}{2}\!\left(e^{i(3+4i)}+e^{-i(3+4i)}\right) \\
        &= \frac{1}{2}\!\left(e^{-4+3i}+e^{4-3i}\right) \\
        &= \frac{1}{2}e^{-4}(\cos 3+i\sin 3)+\frac{1}{2}e^{4}(\cos 3-i\sin 3) \\
        &= \frac{1}{2}(e^{-4}+e^{4})\cos 3+i\,\frac{1}{2}(e^{-4}-e^{4})\sin 3 \\     
    \end{align*}

\end{example}

\begin{example}
    If we have the equation $z^5=\frac{1}{2}+\frac{\sqrt{3}}{2}i$, solving for $z$ gives

    $$
    z_k = e^{\frac{\pi i}{15}} e^{\frac{2k\pi i}{5}}
    \quad k=0,1,2,3,4.
    $$
\end{example}

It is worth noting that there exists a way to express the trigonometric functions $\sin$ and $\cos$ as a function of euler's number. We know that 

$$
e^{i\theta} = \cos \theta + i \sin \theta \qquad e^{-i\theta} = \cos \theta - i \sin \theta
$$

from this, we can derive that 

\begin{align*}
    e^{i\theta} + e^{-i\theta} &= \cos \theta + i \sin \theta + \cos \theta - i \sin \theta \\
    e^{i\theta} + e^{-i\theta} &= \cos \theta + \cos \theta\\
    \cos \theta &= \frac{1}{2}\left(e^{i\theta} + e^{-i\theta}\right)\\
\end{align*}

and that 

\begin{align*}
    e^{i\theta} + e^{-i\theta} &= \cos \theta + i \sin \theta - (\cos \theta - i \sin \theta) \\
    e^{i\theta} - e^{-i\theta} &= i \sin \theta - \left(i \sin \theta\right)\\
    \sin \theta &= \frac{1}{2i}\left(e^{i\theta} - e^{-i\theta}\right)\\
\end{align*}

Another way of expressing this is by saying

$$
\cos x = \real(e^{i\theta}) = \frac{e^{ix} + e^{-ix}}{2}
$$
$$
\sin x = \imaginary(e^{i\theta}) = \frac{e^{ix} - e^{-ix}}{2i}
$$


One final yet very important item to rememebr throughout the curriculum is that powers for complex numbers are \textbf{rotations}.

\break

\section{Sets, Groups, and Functions}

This chapter is mainly going to fly over the various mathmatical concepts that make up the backbone of many mathmatical fields, including those relevant to quantum computing. 

\subsection{Sets}

The concepts of sets are fundamental to many areas of mathmatics. A set is a well-defined collection of distinct objects, which is also an object in its own right. 

\begin{definition}[Set]
    A set is an (unordered) collection of objects, which are said to be elements or members of the set.
\end{definition}

Let $A = \{2, 4, 6, 8\}$ be the set of even numbers less than 10. The elements of this set are 2, 4, 6, and 8. We can also write sets using set-builder notation, such as $B = \{x \mid x \text{ is a vowel in the English alphabet}\} = \{a, e, i, o, u\}$.

\vspace{12pt}


\begin{definition}[Tuple]
    A tuple (or sequence) is an ordered list of elements. 
\end{definition}

Consider the tuple $T = (3, 1, 4, 1, 5)$. This is an ordered sequence where the first element is 3, the second is 1, the third is 4, the fourth is 1, and the fifth is 5. Note that the order matters and repetition is allowed, so $(3, 1, 4, 1, 5) \neq (1, 1, 3, 4, 5)$.


\vspace{12pt}


\begin{definition}[Cardinality]
    The cardinality of a set $A$, denoted $\vert A \vert$, is the number of elements in $A$.
\end{definition}

For the set $A = \{2, 4, 6, 8\}$, the cardinality is $|A| = 4$ since there are 4 elements. For the set $B = \{x, y, z\}$, we have $|B| = 3$. The empty set has cardinality $|\emptyset| = 0$.


We can categorize sets based on their cardinality into finite, countably infinite, or uncountably infinite. A set is said to be countably infinite if it can be bijectively mapped to the set of natural numbers $\mathbb{N}$ such as the set of integers $\mathbb{Z}$, while the set is said to be uncountably infinite if there is no one-to-one to the set of natural numbers, such as the set of real numbers $\mathbb{R}$.

\begin{example}
    Finite set: $A = \{1, 2, 3, 4, 5\}$ has $|A| = 5$.
    Countably infinite set: The set of integers $\mathbb{Z} = \{\ldots, -2, -1, 0, 1, 2, \ldots\}$ can be put in one-to-one correspondence with the natural numbers $\mathbb{N}$, so $|\mathbb{Z}| = |\mathbb{N}| = \aleph_0$.
    Uncountably infinite set: The set of real numbers $\mathbb{R}$ cannot be put in one-to-one correspondence with $\mathbb{N}$, so $|\mathbb{R}| > \aleph_0$.
\end{example}



\vspace{12pt}


\begin{definition}[Subset and Superset]
    We call $B$ a subset of $A$, denoted $B \subseteq A$ if $\forall b \in B, b \in A$. In this case, $A$ is a superset of $B$, denoted $B \supseteq A$. If $B$ is a subset of $A$ but not equal to $A$, then $B$ is called a proper subset of $A$, denoted $B \subset A$.
\end{definition}


Let $A = \{1, 2, 3, 4, 5\}$ and $B = \{2, 4\}$. Then $B \subseteq A$ since every element of $B$ is also in $A$. We can also say $A \supseteq B$. Since $B \neq A$, we have $B \subset A$ (B is a proper subset of A). Additionally, $A \subseteq A$ since every set is a subset of itself.


\vspace{12pt}


\begin{definition}[Union]
    The union of two sets $A$ and $B$, denoted by $A \cup B$, is the set containing all the elements in $A$, $B$, or both. 
\end{definition}

Let $A = \{1, 3, 5, 7\}$ and $B = \{2, 3, 6, 7, 8\}$. Then $A \cup B = \{1, 2, 3, 5, 6, 7, 8\}$, which contains all elements that appear in either set A or set B (or both).



\vspace{12pt}


\begin{definition}[Intersection]
    The intersection of two sets $A$ and $B$, denoted by $A \cap B$, is the set containing all the elements in both $A$ and $B$.
\end{definition}


Using the same sets $A = \{1, 3, 5, 7\}$ and $B = \{2, 3, 6, 7, 8\}$, we have $A \cap B = \{3, 7\}$, which contains only the elements that appear in both sets.



\vspace{12pt}


\begin{corollary}[Disjoint Sets]
    Two or more sets are said to be disjoint if they have no elements in common, that is, their intersection is the empty set: $A \cap B = \emptyset$
\end{corollary}


Let $A = \{1, 3, 5\}$ and $B = \{2, 4, 6\}$. These sets are disjoint because $A \cap B = \emptyset$ - they share no common elements.



\vspace{12pt}


\begin{definition}[Difference]
    The difference of two sets $A$ and $B$, denoted by $A - B$ or $A \ B$, is the set containing all the elements in $A$ but not in $B$.
\end{definition}


Let $A = \{1, 2, 3, 4, 5\}$ and $B = \{3, 4, 6, 7\}$. Then $A - B = \{1, 2, 5\}$, which contains the elements in A that are not in B. Similarly, $B - A = \{6, 7\}$.


\vspace{12pt}



\begin{definition}[Universal Set]
    The universal set, denoted by $U$, is the set that contains all elements under consideration, usually in relation to a particular problem or discussion. Every other set in that context is a subset of the universal set $U$.
\end{definition}

In a problem about students at a university, the universal set might be $U = $ all students at the university. If we're discussing card games, the universal set could be $U = $ all 52 cards in a standard deck.



\vspace{12pt}



\begin{definition}[Complement]
    The complement of a set $A$, denoted by $\overline{A}$ or $A^{c}$, is the set of all elements in the universal set $U$ that are not in set $A$. 
\end{definition}

Let $U = \{1, 2, 3, 4, 5, 6, 7, 8, 9, 10\}$ be the universal set and $A = \{2, 4, 6, 8, 10\}$. Then the complement of A is $\overline{A} = \{1, 3, 5, 7, 9\}$, containing all elements in U that are not in A.



\vspace{12pt}


\begin{definition}[Cartesian Product]
    The cartesian product of two sets $A$ and $B$, denoted $A \cross B$, is the set of all ordered pairs $(a, b), a \in A, b \in B$. 
\end{definition}


Let $A = \{1, 2\}$ and $B = \{x, y, z\}$. Then $A \times B = \{(1,x), (1,y), (1,z), (2,x), (2,y), (2,z)\}$. The Cartesian product contains all possible ordered pairs where the first element comes from A and the second from B. Note that $|A \times B| = |A| \cdot |B| = 2 \cdot 3 = 6$.



\vspace{12pt}


\begin{definition}[Set Partitions]
    A partition of a set $A$ is a collection of disjoint subsets of $A$ such that every element in $A$ is included in exactly one subset. These subsets are called blocks of the partition. 
\end{definition}

Let $A = \{1, 2, 3, 4, 5, 6\}$. One partition of A could be $\{A_1, A_2, A_3\}$ where $A_1 = \{1, 2\}$, $A_2 = \{3, 5\}$, and $A_3 = \{4, 6\}$. These subsets are disjoint ($A_i \cap A_j = \emptyset$ for $i \neq j$) and their union gives the original set ($A_1 \cup A_2 \cup A_3 = A$).




\vspace{12pt}

\begin{theorem}[Set Partition]
    If $\{A_1, A_2, \ldots, A_n\}$ is a partition of set $A$, and $B \subseteq A$, then  $\{A_1 \cap B, A_2 \cap B, \ldots, A_n \cap B\}$ is a partition of set $B$.
\end{theorem}

Let $A = \{1, 2, 3, 4, 5, 6\}$ with partition $\{A_1, A_2, A_3\}$ where $A_1 = \{1, 2\}$, $A_2 = \{3, 5\}$, and $A_3 = \{4, 6\}$. If $B = \{2, 3, 4, 5\} \subseteq A$, then $\{A_1 \cap B, A_2 \cap B, A_3 \cap B\} = \{\{2\}, \{3, 5\}, \{4\}\}$ forms a partition of B.





\begin{definition}[Power Set]
    The power set of $A$ is the set of all subsets of $A$, denoted by $\mathcal{P}(A)$ is the set of all subsets of $A$.
\end{definition}


Let $A = \{1, 2\}$. The power set $\mathcal{P}(A)$ contains all possible subsets of A:
$\mathcal{P}(A) = \{\emptyset, \{1\}, \{2\}, \{1, 2\}\}$
Note that $|\mathcal{P}(A)| = 2^{|A|} = 2^2 = 4$. For any set with n elements, its power set has $2^n$ elements. For set operations in general, there are a few laws worth noting.

Identity Laws:
$$
A \cup \varnothing = A$$$$ A \cap U = A.
$$

Domination Laws:

$$
A \cup U = U$$$$ A \cap \varnothing = \varnothing.
$$

Idempotent Laws:
$$
A \cup A = A$$$$ A \cap A = A.
$$

Absorption Laws:
$$
A \cup (A \cap B) = A$$$$ A \cap (A \cup B) = A.
$$

Complement Laws:
$$
A \cup \overline{A} = U$$$$ A \cap \overline{A} = \varnothing$$$$ \overline{\overline{A}} = A.
$$

Commutative Laws:
$$
A \cup B = B \cup A$$$$ A \cap B = B \cap A.
$$

Associative Laws:
$$
A \cup (B \cup C) = (A \cup B)\cup C$$$$ A \cap (B \cap C) = (A \cap B)\cap C.
$$

Distributive Laws:
$$
A \cup (B \cap C) = (A \cup B)\cap (A \cup C)$$$$
A \cap (B \cup C) = (A \cap B)\cup (A \cap C).
$$

De Morgan's Laws:
$$
\overline{A \cap B}=\overline{A}\,\cup\,\overline{B}$$$$
\overline{A \cup B}=\overline{A}\,\cap\,\overline{B}.
$$


\begin{definition}[Totally Ordered Set]
A totally ordered set is a set in which every pair of elements is comparable; for any two elements $a$ and $b$, either $a \le b$ or $b \le a$ holds. 
\end{definition}

Examples include the real numbers and the integers with their usual orders.

\vspace{12pt}


\begin{definition}[Partially Ordered Set (Poset)]
A partially ordered set (poset) is a set equipped with a relation $\le$ that is reflexive, antisymmetric, and transitive; in a poset, not every pair of elements must be comparable, so some pairs may be incomparable. For example, the power set $\mathcal{P}(\mathbb{Z})$ ordered by inclusion $(\subseteq)$ contains subsets such as $\{1,2\}$ and $\{2,3\}$ that are not comparable, and the positive integers ordered by divisibility have incomparable primes like $5$ and $7$.
\end{definition}
\vspace{12pt}


\begin{definition}[Supremum]
Let $S$ be a nonempty subset of a partially ordered set $P$. An element $u \in P$ is the supremum of $S$, denoted by $u=\sup S$, if:
\begin{enumerate}
\item \textbf{Upper bound:} every $s\in S$ satisfies $s \le u$.
\item \textbf{Least upper bound:} if $v$ is any upper bound of $S$, then $u \le v$.
\end{enumerate}
For instance, $\sup\{-1,-2,-3,\dots\}=-1$, and $\sup\{\sin x: x\in[0,\pi]\}=1$.
\end{definition}
\vspace{12pt}

\begin{definition}[Infimum]
Let $S$ be a nonempty subset of a partially ordered set $P$. An element $\ell \in P$ is the infimum of $S$, denoted by $\ell=\inf S$, if:
\begin{enumerate}
\item \textbf{Lower bound:} every $s\in S$ satisfies $\ell \le s$.
\item \textbf{Greatest lower bound:} if $v$ is any lower bound of $S$, then $v \le \ell$.
\end{enumerate}
For example, $\inf\{e^{-x}: x>0\}=0$, even though $0$ is not an element of the set.
\end{definition}


\subsection{Groups}

Groups, rings, and fields lay the foundation in mathmatics that build upon sets with additional operations in algebra. These strucutres are ubiquitous in mathmatics and physics and lay the foundation for quantum computing as well. 

A group is a set equipped with a single binary operation that exhibits certain properties, much like addition and multiplication. A ring expands on this by incorporating two operations, typically referred to as multiplication and addition. A field is a more stringent structure where the set is a group under both operations, with multiplication also being commutative, and every non-zero element having a multiplicative inverse. 

\begin{definition}
    A group is a set $G$ which is \textit{closed} under an operation $\cdot$ (that is $\forall x, y \in G, x \cdot y \in G$) and satisfies the following properties:

    \begin{enumerate}
        \item \textbf{Identity:} $\exists e \in G$ where $\forall x \in G, x \cdot e = x = e \cdot x$. We define $e$ to be the identity element.
        \item \textbf{Inverse:} $\forall x \in G, \exists y \in G$ such that $x \cdot y = e = y \cdot x$, where $e$ is the identity element identified above. 
        \item \textbf{Associativity:} The operation $\cdot$ is associative for every $x, y, z \in G$, i.e. $$x\cdot(y\cdot z) = (x \cdot y)\cdot z$$
    \end{enumerate}
\end{definition}

In mathmatical contexts, it is common to omit the symbol $\cdot$ or $*$ for the group operation and simply write $x \cdot y$ as $xy$. Some common exmaples include the gorup of integers $\mathbb{Z}$ under addition, denoted by $(\mathbb{Z}, +)$, with $e = 0$, and the inverse of $x$ being $-x$. Another example would be the group of integers modulo $n$, denoted $\mathbb{Z}/n\mathbb{Z}$, with closure (the sum of any two integers modulo $n$ also forming a group under modulo $n$), identity, inverse, and associativity all holding under the subspace of $\mathbb{Z}/n\mathbb{Z}$.

\begin{definition}[Abelian Group]
   A group is said to be \textit{abelian} if the operation $\cdot$ is commutative $\forall x, y \in G$, that is, $$x \cdot y = y \cdot x$$ 
\end{definition}


While the examples so fare are all abelian groups, there are some of groups that are not abelian. For exmaple, the set of all symmetries of an equilateral triangle, known as the dihedral group $D_3$ is not abelian. As you can see, the table of operations is not symmetrical, rendering the group as a non-abelian group. However, the identity, inverse, and associative elements/properties are all present upon verification. 


\[
\begin{array}{c|cccccc}
   & e & a & b & c & r & s \\
\hline
e & e & a & b & c & r & s \\
a & a & e & r & s & b & c \\
b & b & s & e & r & c & a \\
c & c & r & s & e & a & b \\
r & r & c & a & b & s & e \\
s & s & b & c & a & e & r \\
\end{array}
\]



However, in the context of quantum computing, there are a few symmetry groups worth noting. They are

\begin{enumerate}
    \item $\mathbf{SO}(N):$ The orthogonal group in $N$ dimensions consists of all $N \cross N$ orthogonal matrices with determinant $1$, representing rotations in $\mathbb{R}^N$. These rotations preserver distance sand the orientation of objects in question. SO($2$) corresponds to a circle, and SO($3$) to a sphere. 
    \item $\mathbf{SU}(2):$ The special unitary group of degree 2 comprises of all $2 \cross 2$ unitary matrices with determinant $1$. It is closely related to SO($3$) and is commonly used to describe spins and qubit states. Each rotation in SO($3$) corresponds to two points in SU($2$). 
    \item $\mathbf{SU}(N):$ This represents the special unitary group of degree $N$, extending the concepts of SU($2$). These groups are useful in the study of quantum entanglement in quantum computing concerning $N$-level quantum systems. 
\end{enumerate}

\begin{definition}[Subgroups]
    A subgroup $H$ of group $G$ is a subset of $G$ that is a group in by itself, with the same group operation in $G$. You can think of this as a reduced version of $G$ where still 
    \begin{enumerate}
        \item The identity element of $G$ is in $H$.
        \item $\forall h_1, h_2 \in H, \quad h_1 \cdot h_2 \in H$.
        \item $\forall h \in H, \quad h^{-1} \in H$
    \end{enumerate}
\end{definition}

\begin{theorem}[Lagrange's Theorem]
    For any finite group $G$ and any subgroup $H$ of $G$, the order of $H$ divides the order of $G$, i.e. $$\vert G \vert \equiv 0 \mod \vert H \vert$$
\end{theorem}
\vspace{12pt}


\begin{definition}[Coset]
    Given a group $G$ and a subgroup $H$ of $G$, the \textbf{left coset} of $H$ with representative $g \in G$ is the set $gH = \{gh \vert h \in H\}$. Similarly, the right coset is $Hg = \{hg \vert h \in H\}$.
\end{definition}

\begin{theorem}[Parition Theorem for Cosets]
    The collection of all left cosets of a subgroup $H$ forms a partition of the group $G$. This means 
    \begin{enumerate}
        \item Every element of $G$ belongs to exactly one coset of $H$.
        \item Cosets are disjoint and have no elements in common.
    \end{enumerate}
\end{theorem}

\begin{definition}[Normal Subgroup]
    A subgroup $N$ of a group $G$ is called a normal subgroup if it is invariant under conjugation by elements of $G$. This means that $\forall n \in N, \quad g \in G, \quad gng^{-1} \in N$. In notation, $N \triangleleft G$ if $$gNg^{-1} = \{gng^{-1} \vert n \in N\} \subseteq N \forall g \in G.$$
\end{definition}

Now, note that this is not the same as an abelian group. While the abelian group imples that every subgroup is a normal subgroup, the latter does not imply the former.

\begin{example}[Quaternion Group $Q_8$]
Let $$Q_8=\{\pm 1,\pm i,\pm j,\pm k\}$$ with relations $$i^2=j^2=k^2=ijk=-1.$$ This group is non-abelian since, for instance, $ij=k$ while $ji=-k$, so $ij\neq ji$. Nevertheless, every subgroup of $Q_8$ is normal; the subgroups are $\{1\}$, $\{\pm 1\}$, $\langle i\rangle=\{\pm 1,\pm i\}$, $\langle j\rangle=\{\pm 1,\pm j\}$, $\langle k\rangle=\{\pm 1,\pm k\}$, and $Q_8$ itself, and each is invariant under conjugation by any element of $Q_8$. Thus $Q_8$ is a non-abelian group in which all subgroups are normal (a Hamiltonian group).
\end{example}



Also, it is worth noting that the left and right cosets of a normal subgroup $N$ are the same, allowing the group operations on cosets to be well-defined, which brings us to quotient groups:

\begin{definition}[Quotient Group]
    Let $G$ be a group, and $N \triangleleft G$. The quotient group $G / N$ is the set of cosets of $N \in G$ with the group operations defined by: $$(gN) (hN) (gh)N \quad \forall g, h \in G$$
\end{definition}

\begin{example}
    Consider the group $\mathbb{Z}$ of integers under addition and the subgroup $2\mathbb{Z}$ consisting of all even integers. The quotient group $\mathbb{Z}/2\mathbb{Z}$ is the set of cosets of $2\mathbb{Z}$ in $\mathbb{Z}$:

\begin{itemize}
\item The coset $0+2\mathbb{Z}=\{\ldots,-4,-2,0,2,4,\ldots\}$ represents the even integers.
\item The coset $1+2\mathbb{Z}=\{\ldots,-3,-1,1,3,5,\ldots\}$ represents the odd integers.
\end{itemize}

Thus, $\mathbb{Z}/2\mathbb{Z}$ has two elements: $0+2\mathbb{Z}$ and $1+2\mathbb{Z}$, corresponding to the even and odd integers, respectively. The group operation is addition modulo $2$.
\end{example}

This quotient group is \emph{isomorphic} to $\mathbb{Z}_2=\{0,1\}$ under addition modulo $2$, denoted as $\mathbb{Z}/2\mathbb{Z}\cong \mathbb{Z}_2$. Two groups are isomorphic if they have the same structure, meaning there is a one-to-one correspondence between the elements of the two groups that preserves the group operation.

\begin{definition}[Cyclic Group]
    A group $G$ is cyclic if $\exists g \in G$ such that $\forall x \in G$, $x$ can be expressed as powers (repeated operations) of $g$. 
\end{definition}

A good example of this would be $D_3$, which also has multiple generators. 
\vspace{12pt}

\begin{definition}[Ring]
    A ring is a set $R$ equipped with two operations $+$ and $\times$ satisfying the following properties:
    \begin{enumerate}
        \item $(R, +)$ forms an abelian group. 
        \item $(R, \times)$ is associative, that is $\forall a, b, c \in R$, $$a \times (b \times c) = (a \times b) \times c$$ 
        \item The distributive properties hold, that is $\forall a, b, c \in R$, $$a \times (b + c) = (a \times b) + (a \times c) $$$$ (b + c) \times a = (b \times a) + (c \times a)$$
    \end{enumerate}
\end{definition}

For example, both the set of integers $\mathbb{Z}$ and the group of integers modulo $n$, $\mathbb{Z}/n\mathbb{Z}$, are both rings. However, multiplication notably does not have an inverse under most groups of integers. For this to hold, we need to have fields. 

\begin{definition}[Field]
    A field is a set $F$ with two operations $+$ and $\times$, where
    \begin{enumerate}
        \item $(R, +)$ forms an abelian group. 
        \item $(R - \{0\}, \times)$ forms an abelian group. 
        \item The distributive properties holds as in rings. 
    \end{enumerate}
\end{definition}

Some common examples include the set of rational numbers $\mathbb{Q}$, the set of real numbers $\mathbb{R}$, and the set of complex numbers $\mathbb{C}$. Interestingly, $\mathbb{Z}/p\mathbb{Z}$ where $p$ is a prime is also a field.

\subsection{Functions}

Functions serve as mappings from one set to another. They assign each element in the domain to exactly one element in the codomain, and are essential for describing relationships using math. 

\begin{definition}[Function]
Let $f$ be a function from set $A$ to set $B$. \textit{A function from $A$ to $B$}, denoted $f : A \to B$, is an assignment of exactly one element of $B$ to each element of $A$. 
\end{definition}

We write $f(a) = b$ to denote the assignment of $b$ to an element $a$ of $A$ by the function $f$.

\begin{example}
$f(x) = x^2$ is a function $f : \mathbb{R} \rightarrow \mathbb{R}$. \quad $r = \sqrt{x^2 + y^2}$ is a function $f : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$.
\end{example}

\begin{definition}[Image, Range, and Domain]
Let $f$ be a function from $A$ to $B$.
    \begin{itemize}
        \item We say that $A$ is the \textit{domain} of $f$ and $B$ is the \textit{codomain} of $f$.
        \item If $f(a) = b$, $b$ is the \textit{image} of $a$ and $a$ is a \textit{pre-image} of $b$.
        \item The \textit{range} of $f$ (a subset of $B$) is the set of all images of elements of $A$.
        \item Let $S$ be a subset of $A$. The \textit{image of} $S$ is a subset of $B$ that consists of the images of the elements of $S$. We denote the image of $S$ by $f(S)$, so that $$f(S) = \{ f(s) \mid s \in S \}$$
    \end{itemize}
\end{definition}

\begin{center}
    \includegraphics[width=\textwidth/2]{4_func0}
\end{center}


There are also a few common function types that we will define:

\begin{definition}[Injective]
    A function $f: A \rightarrow B$ is injective if $\forall a_1, a_2 \in A$, we have $f(a_1) = f(a_2) \implies a_1 = a_2$.
\end{definition}


\begin{definition}[Surjective]
    A function $f: A \rightarrow B$ is surjective if the whole codomain is covered, meaning that $\forall b \in B, \quad \exists a \in A$ such that $f(a) = b$.
\end{definition}

Sometimes we call surjective functions \emph{onto} functions.

\begin{definition}[Bijective]
    A function is bijective if it is both \emph{injective} and \emph{surjective}.
\end{definition}


\includegraphics[width=\textwidth]{4_func1}


Only with bijectivity established can we define the inverse of a function.

\begin{definition}[Inverse Function]
    If we take the function $f: A \rightarrow B$, it's domain $A$ and codomain $B$, we can define an inverse function $f^{-1}: B \rightarrow A$ such that $\forall b \in B, \quad f^{-1}(b) = a$ if and only if $f(a) = b$. 
\end{definition}
 
Put plainly, $\forall a \in A, \quad f^{-1}(f(a)) = a$ and $\forall b \in B, \quad f(f^{-1}(b)) = b$.


\subsection{Common Functions and Asymptotic Behavior}

This section will be a brief review of real functions $(f: \mathbb{R} \rightarrow \mathbb{R})$ commonly used in quantum computing. 
\subsubsection*{Power Functions}

Power functions take the form $f(x) = x^p, \quad x \geq 0, p \in \mathbb{R}$. 

The behavior of the function varies significantly with the exponent $p$:

\begin{itemize}
    \item For $p > 0$, $f(x)$ increases as $x$ increases. $f(x)$ exhibits a more rapid growth with a larger $p$.
    
    \item For $p < 0$, $f(x)$ decreases as $x$ increases.
    
    \item When $p = 0$, $f(x) = 1$, regardless of $x$ (excluding $x = 0$), which is a constant function.
    
    \item For $p = 1$, $f(x) = x$, representing a linear relationship.
\end{itemize}

Key properties of power functions include the rules for exponentiation:

\begin{itemize}
    \item Multiplying powers with the same base: $x^a \cdot x^b = x^{a+b}$.
    
    \item Dividing powers with the same base: $x^a / x^b = x^{a-b}$.
\end{itemize}

\subsubsection*{Polynomial Functions}

A polynomial function is a sum of terms $a_i x^i$, where $i$ is a non-negative integer:

\[
f(x) = a_0 + a_1 x + a_2 x^2 + \cdots + a_n x^n.
\]

Its behavior for large $x$ values is predominantly determined by its highest power term, $x^n$, where $n$ is the degree of the polynomial.

An $n$-th degree polynomial has $n$ complex roots (counting multiplicities), and according to Vieta's formulas, the sum of these roots is equal to $-a_{n-1}/a_n$ and their product $(-1)^n a_0/a_n$.

\begin{example}
Consider the polynomial function $f(x) = x^3 - 7x^2 + 14x - 8$. It can be factored as:
\[
f(x) = (x - 1)(x - 2)(x - 4).
\]
\end{example}

The roots of this polynomial are $x = 1, x = 2$, and $x = 4$, which can be found by solving the equations $(x - 1) = 0, (x - 2) = 0$, and $(x - 4) = 0$. According to Vieta's formulas, the sum of the roots is:

\[
1 + 2 + 4 = 7 = -\frac{-7}{1},
\]

and the product is:

\[
1 \cdot 2 \cdot 4 = 8 = (-1)^3 \frac{-8}{1}.
\]

\subsubsection*{Exponential Functions}

Exponential functions are defined as $f(x) = b^x$, where $b$ is a positive constant (called the base) and $b \neq 1$. The variable $x$ is the exponent. The key characteristic is that the variable is in the exponent. Some important notes:

\begin{itemize}
    \item Growth and Decay:
    \begin{itemize}
        \item If $b > 1$, $f(x)$ exhibits exponential growth -- increasing rapidly as $x$ increases. Larger bases lead to faster growth.
        
        \item If $0 < b < 1$, $f(x)$ shows exponential decay -- decreasing towards zero as $x$ increases.
    \end{itemize}
    
    \item Always Positive: Exponential functions are always positive for any real-valued input $x$.
    
    \item Horizontal Asymptote: They approach zero for one direction of $x$ (negative infinity for growth, positive infinity for decay).
    
    \item Base $e$: The natural exponential function with $b = e$ (Euler's number, $\approx 2.718$), i.e, $e^x$, also denoted as $\exp(x)$, has special significance across mathematics.
\end{itemize}

\includegraphics[width=\textwidth]{4_func2}


\subsubsection*{Logarithmic Functions}

Logarithmic functions are the inverses of exponential functions. They are defined as $f(x) = \log_b(x)$, where $b$ is a positive constant $(b \neq 1)$ and $x > 0$. Some key points:

\begin{itemize}
    \item Reversing Exponentiation: If $b^y = x$ then $\log_b(x) = y$.
    
    \item Growth and Behavior
    \begin{itemize}
        \item For $b > 1$, $\log_b(x)$ increases as $x$ increases, but very slowly.
        
        \item For $0 < b < 1$, $\log_b(x)$ decreases as $x$ increases.
    \end{itemize}
    
    \item Vertical Asymptote: Logarithmic functions have a vertical asymptote at $x = 0$.
    
    \item Logarithms of 1 and the Base: $\log_b(1) = 0$ and $\log_b(b) = 1$.
    
    \item The natural logarithm, written as $\ln(x)$ has the base $e$.
\end{itemize}

Key Properties:

\begin{itemize}
    \item The Product Rule: $\log_b(xy) = \log_b(x) + \log_b(y)$
    
    \item Logarithms "Break" Exponents: $\log_b(x^y) = y \cdot \log_b(x)$
    
    \item Changing Bases: $\log_b(x) = \log_a(x) / \log_a(b)$
\end{itemize}

Scaling behavior, especially in the context of data structures and algorithm efficiency, is a significant topic for any computing related fields. It concerns itself with witht he evolution of function curves as the response becomes significantly large. We use the \emph{Big O notation} to describe the behavior of a function $f(x), \quad x \rightarrow \infty$. We say that $f(x)$ is $O(g(x))$ meaning that there is some positive constant $c$ such that the upper bound for complexity growth does not increase faster than $c \cdot g(x)$ for a sufficiently large $x$. 

\begin{example}
    For example, consider $$f(x) = 6x^3 + 2x + 1,$$ we say that $f(x)$ is $O(x^3)$ as $x \rightarrow \infty$, as $X^3$ is the dominant term.  
\end{example}


Most of the common limiting functions are illustrated in the following figure: 

\includegraphics[width=\textwidth]{4_func3}

\begin{enumerate}
    \item Log-log: $g(x) = \log \log(x)$
    \begin{itemize}
        \item Exhibits extremely slow growth. Algorithms within this complexity class increase their running time at a negligible rate with input size escalation.
        
        \item Applications include specialized computational geometry problems.
    \end{itemize}
    
    \item Log: $g(x) = \log(x)$
    \begin{itemize}
        \item Denotes high efficiency. The execution time grows much slower than the input size.
        
        \item Examples include binary search in sorted arrays and operations on certain balanced tree data structures.
    \end{itemize}
    
    \item Sublinear: $g(x) = x^p, 0 < p < 1$
    \begin{itemize}
        \item Exhibits growth slower than linear but faster than logarithmic.
        
        \item Common examples include the Grover's search algorithm in quantum computing, which has a complexity of approximately $O(\sqrt{x})$, and some algorithms that utilize probabilistic methods to achieve faster-than-linear performance on average.
    \end{itemize}
    
    \item Linear: $g(x) = x$
    \begin{itemize}
        \item Indicates direct proportionality. Doubling the input size doubles the running time.
        
        \item Common examples are searching in unsorted lists and identifying max/min elements in a list.
    \end{itemize}
    
    \item Polynomial: $g(x) = x^p, p > 1$
    \begin{itemize}
        \item The growth rate is influenced by the exponent $p$. Higher values lead to rapid increases in running time with input size.
        
        \item Examples: Bubble sort and insertion sort (quadratic complexity), matrix multiplication algorithms (cubic complexity or better).
    \end{itemize}
    
    \item Poly-log: $g(x) = x^p \log(x), p \geq 1$
    \begin{itemize}
        \item Less efficient than the corresponding poly (or linear for $p = 1$) but still considered scalable.
        
        \item Fast Fourier Transform (FFT) algorithms are a prime example of algorithms with linear-log complexity. Some fast sorting algorithms also approach this performance.
    \end{itemize}
    
    \item Exponential: $g(x) = b^x, b > 1$
    \begin{itemize}
        \item Characterized by rapid growth. Algorithms in this class quickly become impractical for moderate input sizes.
        
        \item Examples: Brute-force approaches to the Traveling Salesman Problem. Currently known classical algorithms for integer factorization.
    \end{itemize}
    
    \item Factorial: $g(x) = x!$
    \begin{itemize}
        \item Exhibits extremely rapid growth, surpassing even exponential functions in rate. Practical for only very small input sizes.
        
        \item Example: Generating all permutations of a set.
    \end{itemize}
    
    \item Hyper-exponential: $g(x) = x^x, g(x) = b^{a^x}, g(x) = b^{x!}$, and $g(x) = b^{x^x}$, etc., where $a, b > 1$
    \begin{itemize}
        \item Exhibits growth that is even more rapid than factorial functions.
        
        \item Example: Modeling scenarios with extremely high growth rates, beyond combinatorial complexity.
    \end{itemize}
\end{enumerate}

\break


\section{Vectors and Vector Spaces}

Vectors and the study of vector spaces are fundamental to the study of quantum computing, where they provide the mathmatical framework for representing and manipulating quantum states, commonly used to describe superpositions, entanglement, and other phenomena integral to quantum computing. This section serves as a systematic introduction to the realm of such concepets to lay the foundation for lies ahead. 

\subsection{Real Vectors and Complex Vectors}

A vector is an ordered sequence of numbers. For now, we will focus on vectors with numeric elements. 

\begin{definition}[Ordered $n$-Tuple]
    An ordered sequence of $n$ numbers $(v_1, v_2, \ldots, v_n)$ is called an ordered $n$-tuple. 
\end{definition}

\begin{definition}[Euclidean Space]
    The set comprising all $n$-tuples is called $n$-space or Euclidean space, and denoted as $\mathbb{R}^n$ space. The complex Euclidean space is denoted by $\mathbb{C}^n$ and comprises of all complex $n$-tuples. 
\end{definition}

Now in the context of quantum computing, $n$ is typically finite as we are usually considering finite quantum systems. A real vector is in $\mathbb{R}^n$, while a complex vector is in $\mathbb{C}^n$. Much like $\{\mathbf{0}\}$, we will begin with the defiition of the zero vector.

\begin{definition}[Zero Vector]
    The zero vector, in either $\mathbb{R}^n$ or $\mathbb{C}^n$, denoted by $\{\mathbf{0}\}$, is defined as the vector where all components are zero. $$\{\mathbf{0}\} = (0, 0, \ldots, 0)$$
\end{definition}

While it is difficult to visually represent vectors in higher dimensions, vectors in $\mathbb{R}^2$ and $\mathbb{R}^3$ are easier to represent. Generally, vectors both posess magnitude and direction, conviniently representing force, velocity, and heat flow. They carry weight and direction. 

In linear algebra, vectors with identical length and direction are considered equivalent, unlike vector fields which also consider position, which means that we generally don't draw a distinction between \emph{collinearity} and \emph{parallelism}.

\includegraphics[width=\textwidth]{5_vec0}

While seemingly simple, the intuition from representing vectors in $\mathbb{R}^2$ or $\mathbb{R}^3$ extends to $\mathbb{R}^n, \quad n > 3$ and even $\mathbb{C}^n$, for which geometric applications become exceedingly difficult or near impossible. 



\subsection{Basic Vector Algebra}

A few basic operations on vectors are defined in this subsection in order to establish the foundation of vector manipulation.

\begin{definition}[Vector Equality]
    Given two vectors $\mathbf{u}$ and $\mathbf{v}$ in $\Rn$ or $\Cn$, the two vectors are equal if and only if they are equal element wise. This is also denoted as $\mathbf{u} = \mathbf{v}$.
\end{definition}

For each complex vector, there exists a unique complex conjugate that is defined as follows:

\begin{definition}[Complex Vector Conjugate]
    The complex conjugate of a complex vector $\mathbf{v} = (v_1, v_2, \ldots, v_n)$ is denoted as $\mathbf{v}^*$ and is given by: 
    $$\mathbf{v}^* = (\conj{v_1}, \conj{v_2}, \ldots, \conj{v_n})$$
    Where each element is taken to its complex conjugate.
\end{definition}

Some other common operations also exist for vectors.

\begin{definition}[Vector Sum]
    The sum of two vectors $\mathbf{u}$ and $\mathbf{v}$ in $\Rn$ or $\Cn$ is given by element-wise addition:
    $$\mathbf{u} + \mathbf{v} = (u_1 + v_1, u_2 + v_2, \ldots, u_n + v_n)$$
\end{definition}

While not immediately obvious why this requires a proof, we have 

\begin{theorem}
    Vector addition satisfies the commutative, associative, and identity properties. Mathmatically,
    $$\mathbf{u} + \mathbf{v} = \mathbf{v} + \mathbf{u}$$
    $$\mathbf{u} + (\mathbf{v} + \mathbf{w}) = (\mathbf{u} + \mathbf{v}) + \mathbf{w}$$
    $$\mathbf{v} + \mathbf{0} = \mathbf{v}$$
\end{theorem}


\begin{definition}[Vector Negation]
    The negation of a vector $\mathbf{u}$ in $\Rn$ or $\Cn$ is defined as the element wise negation of $\mathbf{u}$, denoted as $-\mathbf{u}$ and defined as:
    $$-\mathbf{u} = (-v_1, -v_2, \ldots, -v_n)$$
\end{definition}

Which allows us to define vector subtraction. 

\begin{definition}[Vector Subtraction]
    Vector subtraction is defined as adding the negated version of the second vector, also defined as $$\mathbf{u} - \mathbf{v} = \mathbf{u}+ (-\mathbf{v})$$
\end{definition}

Scalar multiplication is also very intuitive, defined as

\begin{definition}
    Given $\mathbf{v}$ in $\Rn$ or $\Cn$ and a scalar $k$, the scalar product is denoted by $k\mathbf{u}$, is defined as:
    $$k\mathbf{v} = (k v_1, k v_2, \ldots, k v_n)$$
\end{definition}

Note that the scalar in this case will usually be from the space in which the vector $\mathbf{v}$ resides. When we combine what we just defined, we have the linear combination. 

\begin{definition}[Linear Combination]
    Consider a vector $\mathbf{w}$ and a set of $r$ vectors $\{\mathbf{v_1}, \mathbf{v_2},$ $ \ldots, \mathbf{v_r}\}$ in $\Rn$ or $\Cn$, we say that $\mathbf{w}$ is a linear combination of $\{\mathbf{v_1}, \mathbf{v_2}, \ldots, \mathbf{v_r}\}$ if $\mathbf{w}$ can be expressed as:
    $$\mathbf{w} = k_1 \mathbf{v_1} + k_2 \mathbf{v_2} + \ldots + k_r \mathbf{v_r} = \sum_{i = 1}^{r} k_i \mathbf{v_i}$$ 
    where each $k_i$ is a scalar, referred to as the \textbf{coefficients} of the linear combination.
\end{definition}

\subsection{Vector Spaces, Subspaces, and Span}

After laying the groundwork for basic vector operations, we're now focusing on vector spaces, which are a collection of vectors satisfying specific axioms. This forms a foundational concept in linear algebra, and provides a structured way to handle and manipulate dobjects in multiple dimensions. 

\begin{definition}[Vector Space]
    A vector space (or linear space) is the combination of a certain set $V$ combined with a scalar field $F$ (either in $\R$ or $\C$) such that the following two operations are defined
    \begin{enumerate}
        \item \emph{Vector Addition:} Vector addition in $V$ remain in $V$.
        \item \emph{Scalar Multiplication:} Scalar multiplications remain in $V$ for scalars from $F$. 
    \end{enumerate}
    These operations must satisfy the following properties:
    \begin{itemize}
        \item Addition is associative and commutative as defined above. Addition also contains an identity element as well as an inverse.
        \item Multiplication is distributive, both for the scalar and the vector. 
        \item $(ab)\mathbf{v} = a(b\mathbf{v})$.
        \item The multiplicative identity is $1 \in F$.    
    \end{itemize}
\end{definition}

We define item 1 and 2 from the definition as \textbf{closure under addition} and \textbf{closure under multiplication}, which are two important properties to satisfy for the whole spiel to work. Unless otherwise specified, discussions over the vector space $\Rn$ assume field $\R$, discussions over vector space $\Cn$ assume field $\C$. It is evident that $\Cn$ is also a vector space over $\R$, depending on the choice of the scalar field. in the context of quantum computing, we consistently assume that $\Cn$ is treated as a complex vector space. 

While most of these definitions may be redundant, they are nevertheless necessary in order for later definitions.

\begin{example}
    Let $V$ be a vector space over the field $F$. Given a vector $v$ in $V$ and a scalar $k \in F$, use the previous axioms to prove that $k\mathbf{0}=\mathbf{0}$.
\end{example}

\begin{proof}
    Using the distributive property for scalar multiplication, we have:
    \[
    k(\mathbf{0}+\mathbf{0})=k\mathbf{0}+k\mathbf{0}.
    \]
    From the existence of the additive identity, we know $\mathbf{0}+\mathbf{0}=\mathbf{0}$, so:
    \[
    k\mathbf{0}=k\mathbf{0}+k\mathbf{0}.
    \]
    By the additive inverse property, there exists an additive inverse $-k\mathbf{0}\in V$. Adding $-k\mathbf{0}$ to both
    sides gives:
    \[
    k\mathbf{0}+(-k\mathbf{0})=(k\mathbf{0}+k\mathbf{0})+(-k\mathbf{0}).
    \]
    The left-hand side simplifies to $\mathbf{0}$, and using the fact that addition is commutative and contains an inverse on the right-hand side gives:
    \[
    \mathbf{0} = k\mathbf{0}+[k\mathbf{0}+(-k\mathbf{0})] = k\mathbf{0}+\mathbf{0} = k\mathbf{0}.
    \]
    This completes the proof. Note that the properties of associativity and identity in addition are also used.
\end{proof}

Next we will move on to subspaces. Subspaces of vector spaces are subsets that form a vector space of its own under the same vector operations. For example, the set of vectors in $\R^2$ with a positive $x$ component violate the properties of a subspace as it is not closed when multiplied with a negative scalar. More rigorously

\begin{definition}[Subspace]
    A non-empty subset $W$ of a vector space $V$ is called a subspace of $V$ if $W$ is a vector space under the \emph{same scalar field} $F$ and the same operations of vector addition and scalar multiplication as in $V$. 
\end{definition}

While the properties listed out in the bullet points are easy to verify as those properties are inherently preserved when defining a subspace. In order to verify that a subset is a subspace, it suffices to verify whether the points 1. and 2. hold.  

\begin{theorem}
    If $W$ is a non-empty subset of a vector space $V$, then $W$ is a subspace of $V$ if and only if $W$ satisfies closure under addition and scalar multiplication.
\end{theorem}

\begin{example}
    Prove that the empty set is a subspace of $\Rn/\Cn$. 
\end{example}

\begin{proof}
    We only need to verify that addition and scalar multiplication holds in the subspace. Since $$\zerovec + \zerovec = \zerovec, \qquad k\zerovec = \zerovec$$
    Therefore, $\{\zerovec\}$ is a subspace. 
\end{proof}

\begin{definition}[Zero Subspace]
    The subset $W = \{\zerovec\}$ is called the zero subspace of a vector space $V$ where $\zerovec$ is the zero vector in $V$.
\end{definition}

An informal corrolary is that any hyperplane spanned by vectors that does not pass through the origin will not form a subspace. 

\begin{example}
    Show that $S = \{\mathbf{x} \vert \mathbf{x} = (z, \conj{z}), z \in \C\}$ is not a subspace of $\C^2$
\end{example}

\begin{proof}
    Since this is the space of the vectors spanned by a complex number $z$ and its inverse $\conj{z}$, we see that it conflicts with a scalar multiplication is defined as $$k \vvec = (kz, k \conj{z}).$$
    The second element in the vector should be the complex conjugate of the first element. However, for every $\vvec \in S$ scaled by $k \in \C$, it becomes evident that the element is no longer in the set. Mathmatically: $$\conj{(kz)} = \conj{k}\conj{z} \neq k \conj{z} \Rightarrow k \vvec \notin S.$$
    Which violates closure under multiplication and proves that $S$ is not a subspace of $\C^2$.
\end{proof}

While the previous definitions can be called foundational or elementary, we are interested to see how subspaces can be constructed using a set of vectors rather than from the top down, i.e., from the reduction of another subspace. 

\begin{theorem}
    If $S = \{\vvec_1, \vvec_2, \ldots, \vvec_r\}$ is a set of vectors from vector space $V$, then all possible linear combinations of the vectors in $S$ form a subspace of $V$. 
\end{theorem}

\begin{proof}
    Let $W$ be the set of all possible linear combinations of the vectors in $S$ and take any two vectors $\mathbf{a}, \mathbf{b} \in W$. Then we can express $\mathbf{a}$ and $\mathbf{b}$ as a linear combination of all vectors in $S$ with coefficients $a_i, b_i \in F$. Then, for any scalar $k \in F$, we see that addition and scalar multiplication hold under $S$, making $W$ a subspace of $V$.
\end{proof}

The formal definition also is as follows:

\begin{definition}[Span]
    Given a set $S = \{\vvec_1, \vvec_2, \ldots, \vvec_r\}$ from the vector space $V$, and let $W$ be the subspoace of $V$ that contains all possible linear combinations of $S$. Here, we define $W$ as the span of $S$, and write $$W = \text{span}(S) = \text{span}\{\vvec_1, \vvec_2, \ldots, \vvec_r\}.$$ In other words, S \textbf{spans} W. 
\end{definition}



\subsection{Linear Independence, Basis, and Dimension}

We commonly use basis to describe the fundamental units on the cartesian coordiante and commonly use $\mathbf{\hat{i}} = (1, 0)$ and $\mathbf{\hat{j}} = (0, 1)$ in order to form a basis for $\R^2$. It is also easy to see that any vector in $\R^2$ can be expressed as a linear combination of these two vectors. However, it is not immediately mathmatically obvious why we must chose these to vectors.  to form the basis of these vectors. 

The first prerequisite for some combination of vectors to span a space, is for them to be linearly independent. This ensures that no vectors are redundant, which brings us to an associated property of subspaces

\begin{definition}[Linear Dependence]
    Let $S = \{\vvec_1, \vvec_2, \ldots, \vvec_r\}$ be a set of vectors from a vector space $V$. The set $S$ is said to be linearly dependent if there exists scalars $c_1, c_2, \ldots, c_n$ not all zero such that $$c_1 \vvec_1 + c_2 \vvec_2 + \ldots + c_n \vvec_n = \mathbf{0}.$$ If not such scalars exists, such that $\forall c_i = 0, i \in [1, n]$, then the set is linearly independent. 
\end{definition}

A minimal set of vectors that spans a vector space $V$ is a basis. Of course, there is also a rigorous definition.

\begin{definition}[Basis]
    A set of vectors $S = \{\vvec_1, \vvec_2, \ldots, \vvec_r\}$ from a vector space $V$ forms the basis for $V$ if and only if 
    \begin{itemize}
        \item $\text{span}(S) = V$
        \item $S$ is linearly independent
    \end{itemize}
\end{definition}

These two properties together construct the following theorem:

\begin{theorem}[Uniqueness of Basis Representation]
    If $S$ is a basis of a vector space $V$, then every vector $\mathbf{w} \in V$ has a unique representation as a linear combination of $S$. 
\end{theorem}

\begin{proof}
    Assuming that $\text{span}(S) = V$ and take 
    $$\mathbf{w} = \sum_{i=1}^{n}k_i \vvec_i$$
    and another representation of the same vector, 
    $$\mathbf{w} = \sum_{i=1}^{n}m_i \vvec_i$$
    then we have 
    $$\zerovec = \sum_{i=1}^{n} (k_i - m_i) \vvec_i$$
    since $S$ is linearly independent, we must have $$k_i - m_i = 0, \quad \forall k_i, m_i \Rightarrow k_i = m_i$$
    which renders the representation unique.
\end{proof}

From this, we can see that there is a well-defined and unique set of coefficients for every vector $\mathbf{w} \in V$ expressed as a linear combination of vector set $S$, where $\text{span}(S) = V$. There is also a special term that we assign to the coefficients:

\begin{definition}[Coordinate]
    Take the vector $\wvec$ expressed in the terms of a basis $S$ for from a vector space $V$ over a field $\R/\C$:
    $$\wvec = \sum_{i=1}^{n}k_i \vvec_i$$
    The unique vector $(k_1, k_2, \ldots, k_n)$ formed from the scalar coefficients in $\Rn/\Cn$ is said to be the coordinate vector, or the coordinate of $\wvec$ relative to $S$, denoted as $$(\wvec)_S = (k_1, k_2, \ldots, k_n)$$
\end{definition}

\begin{example}
    Any vector $(a, b) \in \R^2/\C^2$ in the standard basis $\mathbf{\hat{i}} = (1, 0)$ and $\mathbf{\hat{j}} = (0, 1)$ gives 
    $$(a, b) = a(1, 0) + b(0, 1)$$ which in turn gives us the coordinates for $(a, b)$, that is 
    $$(a, b)_{\{\ibase,\ \jbase\}} = (a, b)$$
\end{example}

This example goes to show that the coordinates in a Cartesian coordinate system uses the unit vectors along the $x$ and $y$ axis as its coordinate system. While this sentence sounds like a broken record, we can see that, while we prefer to use the unit vectors as the basis vectors to represent vectors in lower dimensional space, we can use alternative axes for representing the same points in $n$-dimensional space, and we'll introduce dimensions right off the bat with a theorem:

\begin{theorem}
    For a finite-dimensional vector space, all bases posess the same number of vectors. 
\end{theorem}

While the formal proof of this theorem requires extensive background in linear systems and matrix algebra, we nevertheless wish to develop an intuitive understanding. Consider a basis $S$ for a finite-dimensional vector space $V$. If we add an extra vector to this, the set would be linearly dependent. Conversely, removing any vector would reduce the span and fail to cover all of $V$. However, this theorem confirms that the dimension of a vector space is well defined.

\begin{definition}[Dimension]
    The dimensions of a finite-dimensional vector space $V$ is the number of vectors in its basis, denoted as $\dim(V)$. 
\end{definition}

\break

\section{Inner Product Spaces}

What lays the foundation for representing $n$-qubit systems is being able to mathmatically represent them as state vectors residing in $\C^{2^n}$, which must be orthonormal basis â€“ vectors mutually orthogonal of length 1. In order to rigorously define inner product spaces, we require a well-defined inner product â€“ a mathmatical structure that generalizes length and orthogonality to higher dimensions. This, in turn, serves as the foundation for Hilbert spaces. We will also introduce dirac (or bra-ket) notation in this chapter which presents itself as an elegant method to represent quantum states and operations. 


\subsection{Dirac Notation Basics}

\begin{definition}[Matrix]
    A matrix is a rectangular array of numbers, which can be either real or complex. A matrix with $m$ rows and $n$ columns is represented as:
    \[
    A = \begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n} \\
    a_{21} & a_{22} & \cdots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{m2} & \cdots & a_{mn}
    \end{bmatrix}_{m \times n}
    \]
    where the entries $a_{11}, a_{12}, \ldots, a_{mn}$ are called \textbf{elements}. The matrix is referred to as an $m \times n$ matrix, indicating its size.
\end{definition}

When a matrix contains only one column (or one row), it is called a column vector (or a row vector):

\begin{definition}[Column and Row Vectors]
    A matrix with only one column ($n \times 1$ in size) is called a column vector, denoted as:
    \[
    \begin{bmatrix}
    v_1 \\
    v_2 \\
    \vdots \\
    v_n
    \end{bmatrix} .
    \]

    Similarly, a matrix with only one row ($1 \times m$ in size) is called a row vector, denoted as:
    \[
    \begin{bmatrix}
    v_1 & v_2 & \cdots & v_m
    \end{bmatrix} .
    \]
\end{definition}

The process of matrix transposition, which interchanges the rows and columns of a matrix, is a fundamental operation in matrix algebra, defined as follows.

\begin{definition}[Transpose]
    Given an $m \times n$ matrix $A$ as given in Eq.~6.1, the transpose of $A$, denoted as $A^T$, is an $n \times m$ matrix defined as:
    \[
    A^T = \begin{bmatrix}
    a_{11} & a_{21} & \cdots & a_{m1} \\
    a_{12} & a_{22} & \cdots & a_{m2} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{1n} & a_{2n} & \cdots & a_{mn}
    \end{bmatrix}_{n \times m}
    \]
    resulting from interchanging the rows and columns of $A$.
\end{definition}

From here on out, vectors such as $\vvec$ will represent column vectors and the transpose thereof will represent row vectors. In dirac notation, column vectors in $\Cn$ are usually represented by a symbol called ket. 

\begin{definition}[Ket]
    Given a general vector $v = (v_1, v_2, \ldots, v_n)$ in $\mathbb{C}^n$, we use $\ket{v}$ to denote its column vector form:
    \[
    \ket{v} \equiv \begin{bmatrix}
    v_1 \\
    v_2 \\
    \vdots \\
    v_n
    \end{bmatrix}
    \]
    where $\ket{v}$ is referred to as a ket.
\end{definition}

There are a few general and commmon methods of representing a state vector ket, which include:

\begin{itemize}
    \item $\ket{\psi}$ for a general state vector;
    \item $\ket{\lambda_i}$ for eigenvectors corresponding to the eigenvalue $\lambda_i$;
    \item $\ket{j}, \ket{k}$ with $j, k \in \{0, 1, 2, \ldots, n - 1 \}$ to denote the computational basis vectors in $\Cn$.
\end{itemize}

Therea realso certain symbols reserved for specific vectors of significance, much like how we reserve $\pi$ and $e$ in common arithmatic. These include

\begin{itemize}
    \item $\ketzero, \ketone$ for the computational basis vectors in single qubit systems;
    \item $\ket{V}, \ket{H}$ for rectilinear polarization states of a photon;
    \item $\bellpsi, \bellpsim, \bellphi, \bellphim$ for Bell states of two qubits. 
\end{itemize}

the standard basis vectors of $\Cn$ are indispensable in quantum computing, so it is necessary to define the computational basis for it

\begin{definition}[Computational Basis]
For a single-qubit system in $\C^2$, the standard basis vectors, commonly referred to as the \textbf{computational basis}, are defined as the following ket vectors:
\[
\ketzero \equiv \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \quad \ketone \equiv \begin{bmatrix} 0 \\ 1 \end{bmatrix}
\]

For a one-qudit system (with $d$ distinct levels) in $\C^d$, the computational basis includes vectors:
\[
\ketzero \equiv \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix}, \quad \ketone \equiv \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}, \quad \cdots, \quad \ket{d-1} \equiv \begin{bmatrix} 0 \\ 0 \\ \vdots \\ 1 \end{bmatrix}
\]
\end{definition}

Therefore, a general vector can be expressed as 

$$
\begin{bmatrix}
    1 \\ 0
\end{bmatrix}
= 
\alpha \ketzero
+
 \beta \ketone, \quad \forall \alpha, \beta \in \C
$$

We conversely also have the hermitian adjoint of any vector $\ketpsi$, which is defined as 

\begin{definition}[Hermitian Adjoint]
    Consider a column vector in $\mathbb{C}^n$:
    \[
    v = \begin{bmatrix}
    v_1 \\
    v_2 \\
    \vdots \\
    v_n
    \end{bmatrix},
    \]

    its Hermitian adjoint, or simply \textit{adjoint}, is defined as its conjugate transpose, denoted as:
    \[
    v^\dagger \equiv (v^*)^T = (v^T)^* = \begin{bmatrix} v_1^* & v_2^* & \cdots & v_n^* \end{bmatrix}.
    \]
\end{definition}

We have a convinient method of encapsulating the concept of adjoint vectors using a bra in dirac notation, defined as:

\begin{definition}[Bra]
The bra, denoted as $\bra{v}$, represents the Hermitian adjoint of $\ket{v}$, formally defined as:
\[
\bra{v} \equiv \ket{v}^\dagger.
\]
\end{definition}

\begin{theorem}
    Given vectors $\bra{u}$, $\bra{v}$ in $\Cn$ and scalars $\alpha, \beta \in \mathbb{C}$, bras exhibit the following conjugate-linear properties:
        \begin{align}
        \bra{u + v} &= \bra{u} + \bra{v}\\
        \bra{\alpha v} &= \alpha^* \bra{v}\\
        \bra{\alpha u + \beta v} &= \alpha^* \bra{u} + \beta^* \bra{v}.
        \end{align}
\end{theorem}

Note that $\brapsi$ and $\ketpsi$ are fundamentally not in the same dimension, so that operations on them will not be very straightforward. 

\subsection{Norm and Unit Vectors}

We know that the squared distance of a line can be calculated by adding the squared values of their $x$ and $y$ componenets. In three dimensional space, it means adding the squared value of the $z$ component as well. This allows us to generalize to the $n$th dimension, where we have the following, rather intuitive, definition for norm. 

\begin{definition}[Norm of a Real Vector]
    For any real vector $\vvec \in \Rn$, its length is denoted $\left\lVert \vvec \right\rVert$ and defined by: $$\left\lVert \vvec \right\rVert = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2}$$
\end{definition}


\begin{theorem}
    The norm of a vector in $\Rn/\Cn$ exhibits the following properties:
    \begin{enumerate}
        \item $\norm{\vvec} \geq 0$
        \item $\vnorm = 0$ if and only if $\vvec = 0$
        \item $\norm{k \vvec} = \abs{k} \norm{\vvec}$
    \end{enumerate} 
\end{theorem}

For complex numbers, we just need to use the complex conjugate into the equation. 

\begin{definition}[Norm of a Complex Number]
    For any complex vector $\vvec \in \Cn$, the norm is defined by the formula: 
    $$\vnorm = \sqrt{v_1 \conj{v_1} + v_2 \conj{v_2} + \cdots + v_n \conj{v_n} }$$
\end{definition}

We can also derive the following theorem:

\begin{theorem}
    For a complex number $\vvec \in \C$, the following identity holds:
    $$\vnorm^2 = \abs{v_1}^2 + \abs{v_2}^2 + \cdots + \abs{v_n}^2$$
\end{theorem}

\begin{proof}
    Since we know that $\abs{z} = \sqrt{z^* z}$, we can square both sides of the equation to give us:
    $$\vnorm^2 = z_1^* z_1 + z_2^* z_2 + \cdots + z_n^* z_n = \abs{v_1}^2 + \abs{v_2}^2 + \cdots + \abs{v_n}^2$$
\end{proof}

On a side note, we have the following equivalencies for a vector in dirac notation:

$$\norm{v} \equiv \norm{\ket{v}} = \norm{\bra{v}}$$

In quantum mechanics, most of the vectors must be represented as complex vectors of norm 1. 

\begin{theorem}
    For any non-zero vector $\vvec \in \R$, the following formula will give the normalized vector in the direction of $\vvec$, denoted by $\hat{\vvec}$: $$\unitv = \frac{1}{\vnorm}\vvec$$ 
\end{theorem}

\begin{proof}
    $$
    \norm{\unitv} = \norm{\frac{1}{\vnorm}\vvec} = \abs{\frac{1}{\vnorm}}\vnorm = \frac{1}{\vnorm}\vnorm = 1
    $$
\end{proof}

You might realize by this point, that the unit vectors reside on some version of a unit hypersphere. In 2 dimensions, this would be the unit circle. Consequently, rotations in hyperspace keep the unit vector as a unit vector. Distance is defined as:

\begin{definition}[Distance]
    Given two vectors $\vvec$ and $\uvec$ in $\Rn$ or $\Cn$, the distance between $\vvec$ and $\uvec$ is defined as:
    $$d(\uvec, \vvec) = \norm{\uvec - \vvec}$$
\end{definition}

\subsection{Complex Inner Product Spaces}

A vector space will only become an inner product space when it contains the inner product as a valid operation. In $\Rn$, this operation is simply the dot product. Generally speaking, the inner product is the operation defined for a vector space $V$, scalar field $F$, as $$\left\langle \cdot, \cdot \right\rangle: V \cross V \rightarrow F $$

The more familiar form of the inner product in $\Rn$, defined as:

\begin{definition}[Real Dot Product]
    For vectors $\vvec$ and $\uvec$ in $\Rn$, their dot product is deinfed as:
    $$\uvec \cdot \vvec = u_1 v_1 + u_2 v_2 + \ldots + u_n v_n$$
    Which is also known as the Euclidian inner product.
\end{definition}


The norm of a real vector and the distance between two vectors can also be expressed as a dot product as:

\begin{theorem}
    For $\uvec, \vvec \in \Rn$, we have
    \begin{align*}
        \norm{\vvec}^2 &= \vvec \cdot \vvec \\
        \norm{\uvec - \vvec}^2 &= (\uvec - \vvec) \cdot (\uvec - \vvec)
    \end{align*}
\end{theorem}

\begin{proof}
    Write the equation out and group it together using the definition of the real dot product. 
\end{proof}

At the same time, we have a few more intuitive and convinient algebraic properties of the dot product. 

\begin{theorem}
    For $\uvec, \vvec, \wvec \in \Rn$ and $k \in \R$, we have the following algebraic identities:
    \begin{align*}
        \uvec \cdot \vvec &= \vvec \cdot \uvec &\text{(Commutative Law)} \\
        \uvec \cdot (\vvec + \wvec) &= \uvec \cdot \vvec + \uvec \cdot \wvec &\text{(Left Distributive Law)} \\
        (\uvec + \vvec) \cdot \wvec &= \uvec \cdot \wvec + \vvec \cdot \wvec &\text{(Right Distributive Law)} \\
        (k \uvec) \cdot \vvec &= k(\uvec \cdot \vvec) &\text{(Left Homogeneity Law)} \\
        \uvec \cdot (k \vvec) &= k(\uvec \cdot \vvec) &\text{(Right Homogeneity Law)}
    \end{align*}
\end{theorem}

In the complex plane, things get more interesting. 

\begin{definition}[Complex Dot Product]
    Given $\vvec, \uvec \in \Cn$, we have the complex dot product:
    $$\uvec \cdot \vvec = u_1^* v_1 + u_2^* v_2 + \cdots + u_n^* v_n = \sum_{n}^{i=1}u_i^* v_i$$
\end{definition}

Generally speaking, the complex dot product will produce a complex scalar. In some other contexts, one might find the definition of the complex dot product to have the second element be the conjugate element. However, the previous definition is in line with the conventions of quantum mechanics and computing. The use of the complex conjugate is also designed to ensure that the norm defined previously is the dot product of a complex number with itself. 

\begin{theorem}
    Given a complex number $\vvec$, it's norm can be expressed as:
    $$\vnorm = \sqrt{\vvec \cdot \vvec} \quad \Longleftrightarrow \quad \vnorm^2 = \vvec \cdot \vvec$$
\end{theorem}

Complex inner product spaces are complex vector spaces "equipped" with a complex inner prodcut. You can think of them as spaces where the dot product (a resulting scalar from two vector multiplications) is a valid operation. We like using $\Rn/\Cn$ as examples of common inner product spaces, and in quantum computing we really like $\Cn$ as it is the norm for representing quantum systems. 

\begin{definition}[Complex Inner Product Space]
    A complex inner product space is a complex vector space $V$ equipped with an inner product, defined as: $\left\langle \cdot, \cdot \right\rangle: V \cross V \rightarrow \C. $ The inner product, for $\uvec, \vvec, \wvec \in V, \quad k \in \C$, must satisfy:
    \begin{enumerate}
        \item $\innerprod{\uvec}{\vvec} = \conj{\innerprod{\vvec}{\uvec}}$
        \item $\innerprod{\uvec}{\vvec + \wvec} = \innerprod{\uvec}{\vvec} + \innerprod{\uvec}{\wvec}$
        \item $\innerprod{\uvec}{k \vvec} = k \innerprod{\uvec}{\vvec}$
        \item $\innerprod{\vvec}{\vvec} \in \R$
        \item $\innerprod{\vvec}{\vvec} = 0 \quad \Longleftrightarrow \quad \vvec = 0$
    \end{enumerate}
\end{definition}

If we put the dot product and the complex vector space together, we have:

\begin{theorem}
    $\Cn$ is a complex inner product space when the complex dot product is used as the inner product.
\end{theorem}

\begin{proof}
    Take $V \equiv \Cn$. In order to prove that the dot product qualifies as an inner product space, we need to make sure that it satisfies the properties laid out in the previous definition. 
    \begin{enumerate}
        \item $$\uvec \cdot \vvec = \sumonen \conj{u_i} v_i = \sumonen \conj{(u_i \conj{v_i})} = \conj{\left(\sumonen u_i \conj{v_i}\right)} = \conj{(\vvec \cdot \uvec)}$$
        \item $$\uvec \cdot (\vvec + \wvec) = \sumonen \conj{u_i}(v_i + w_i) = \sumonen \conj{u_i} v_i + \sumonen \conj{u_i} w_i = \uvec \cdot \vvec + \uvec \cdot \wvec$$
        \item $$\uvec \cdot (k \vvec) = \sumonen \conj{u_i} (k v_i) = k \sumonen \conj{u_i} v_i = k (\uvec \cdot \vvec)$$
        \item $$\vvec \cdot \vvec = \sumonen \conj{v_i} v_i = \sumonen \abs{v_i}^2 \geq 0, \qquad \forall \vvec$$
        \item $$\forall i \in [1, n], \qquad \abs{v_i} = 0 \quad \Rightarrow \quad v_i = 0 \quad \Rightarrow \quad \vvec = \zerovec$$
    \end{enumerate}
\end{proof}

In addition to the previously defined identities, we also have:

\begin{theorem}
    Given a complex inner product space $V$ with an inner product $\innerprod{\cdot}{\cdot}$, with $\uvec, \vvec, \wvec \in V, \quad k \in \C$, the following identities hold:
    \begin{align*}
        \innerprod{\zerovec}{\vvec} &= \innerprod{\vvec}{\zerovec} = 0 \\
        \innerprod{\uvec + \vvec}{\wvec} &= \innerprod{\uvec}{\wvec} + \innerprod{\vvec}{\wvec} \\
        \innerprod{k \uvec}{\vvec} &= \conj{k} \innerprod{\uvec}{\vvec}
    \end{align*}
\end{theorem}

\begin{proof}
    Considering that the conjugate is taken for the left-handed element in the inner product:
    \begin{align*}
        \innerprod{k \uvec}{\vvec} &= \conj{\innerprod{\vvec}{k \uvec}} \\
        &= \conj{(k \innerprod{\vvec}{\uvec})} \\ 
        &= \conj{k} \conj{\innerprod{\vvec}{\uvec}}\\
        &= \conj{k} \innerprod{\uvec}{\vvec}
    \end{align*}
\end{proof}

Now as a subset of complex inner product spaces, we have the real inner product space. 

\begin{definition}[Real Inner Product Space]
    A real inner product space is a vector space $V$ equipped with the binary function (inner product) $\left\langle \cdot, \cdot \right\rangle: V \cross V \rightarrow \R.$  It must, for all $\uvec, \vvec, \wvec \in V, \quad k \in \R$, satisfy:
    \begin{enumerate}
        \item $\innerprod{\uvec}{\vvec} = \innerprod{\vvec}{\uvec}$
        \item $\innerprod{\uvec}{\vvec + \wvec} = \innerprod{\uvec}{\vvec} + \innerprod{\uvec}{\wvec}$
        \item $\innerprod{\uvec}{k \vvec} = k \innerprod{\uvec}{\vvec}$
        \item $\innerprod{\vvec}{\vvec} \in \R$
        \item $\innerprod{\vvec}{\vvec} = 0 \quad \Longleftrightarrow \quad \vvec = 0$
    \end{enumerate}
\end{definition}

The only key difference lies in the first property. We also have the following theorem for the real inner product:

\begin{theorem}
    The following identities hold for the real inner product space:
    \begin{align*}
        \innerprod{\uvec}{k\vvec + m\wvec} &= k\innerprod{\uvec}{\vvec} + m\innerprod{\uvec}{\wvec} \\
        \innerprod{k\uvec + m\vvec}{\wvec} &= k\innerprod{\uvec}{\wvec} + m\innerprod{\vvec}{\wvec} \\
    \end{align*}
\end{theorem}

Which, conviniently, leads us to the definition for norm and distance using the inner product.

\begin{definition}[Norm and Distance]
    For any $\vvec, \uvec \in V$, the length (norm) is defined by 
    $$\vnorm = \sqrt{\innerprod{\vvec}{\vvec}}$$
    The distance between two vectors is defined by:
    $$d(\vvec, \uvec) = \norm{\vvec - \uvec} = \sqrt{\innerprod{\vvec - \uvec}{\vvec - \uvec}}$$
\end{definition}

Using dirac notation, representing inner products become much simpler and straightforward.

\begin{definition}[Product of Row and Column Vectors]
    Given a row vector $\mathbf{r}$ and a column vector $\mathbf{c}$ in $\Cn$:
    $$\mathbf{r} = 
    \begin{bmatrix}
    r_1 &
    r_2 &
    \cdots &
    r_n
    \end{bmatrix}, \quad \mathbf{c} = 
    \begin{bmatrix}
    c_1 \\ c_2 \\ \vdots \\ c_m
    \end{bmatrix}
    $$
    their matrix product is defined as:
    $$\mathbf{rc} = \sumonen r_i c_i$$
\end{definition}

Now, noting that we can simplify the product betweenn a vector $\ketphi$ and a hermitian adjoint ($\psi^\dagger = \brapsi$) as $\brapsi \ketphi \equiv \braket{\psi}{\phi}$, we have the following definition:

\begin{definition}[Dirac Notation of Inner Product]
    Given two vectors $\ket{u} = \uvec, \ket{v} = \vvec$, we have the inner product defined as:
    $$\braket{u}{v} \equiv \innerprod{\uvec}{\vvec} = \uvec \cdot \vvec.$$
\end{definition}

For convinience, we have the following theorem:

\begin{theorem}
    For vectors $\ket{u}, \ket{v}, \ket{w} \in \Cn, \alpha \in \C$, the following identities hold:
    \begin{align*}
        \braket{u}{v} &= \conj{\braket{v}{u}} \\
        \braket{u + v}{w} &= \braket{u}{w} + \braket{v}{w} \\
        \braket{u}{v + w} &= \braket{u}{v} + \braket{u}{w} \\ 
        \braket{u}{\alpha v} &= \alpha \braket{u}{v} \\
        \braket{\alpha u}{v} &= \conj{\alpha} \braket{u}{v} \\
        \vnorm^2 &= \braket{v}{v} \\
    \end{align*}
\end{theorem}

We also have the following theorem for any unit vector:

\begin{theorem}
    Any unit vector $\ketphi$ satisfies:
    $$\braket{\phi}{\phi} = 1$$
\end{theorem}

\begin{proof}
    Since $\ketphi$ is a unit vector, $\norm{\phi} = 1$, which means that $\braket{\phi}{\phi} = \norm{\phi}^2 = 1$
\end{proof}

There are a host of computational examples that can be found in the textbook and it is highly recommended to look over them and compute for yourself. Now it is worth mentioning a quick note on \textbf{Hilbert Spaces}, which is the mathmatical framework under which quantum states reside. While such spaces are often infinite-dimensional, the complex inner product spaces $\Cn$ are often special cases of finite-dimensional Hilbert spaces. 

\subsubsection*{Infinite-Dimensional Complex Vector Spaces $\C^\infty$}

We first start with $\C^\infty$ where columns consist of infinite sequences of complex numbers. This forms the foundation for infinite dimensional Hilbert spaces. An inner product here is defined as:

$$\innerprod{\uvec}{\vvec} = \sumoneinf \conj{u_i} v_i$$

Notably, for the inner product to be well defined, the series must converge, which leads to the concept of square-summable sequences ($l^2$).

\subsubsection*{Functional Inner Spaces}

Functional Spaces, which are defined as the spaces of functions, consist of functions with specific properties. If we equip them with an inner product, these functional spaces become (you guessed it) functional inner spaces. For the inner product of functions $f(x), g(x)$ over the interval $[a, b]$, we have:

$$\innerprod{f}{g} = \int_{a}^{b} \conj{f}(x) g(x)\,dx. $$

One of the most important examples of functional inner spaces is $L^2([a, b])$, is the space of all square-intergrable functions on $[a, b]$, defined as:

$$L^2([a, b]) = \left\{f: [a, b] \longrightarrow \C \vert \int_{a}^{b} \abs{f(x)}^2 \, dx < \infty \right\}$$

The corresponding norm, (unironically) called the $L^2$ norm, is defined as:

$$\norm{f} = \sqrt{\int_{a}^{b} \abs{f(x)}^2 \,dx }$$

One will see later how concepts like orthogonality and projection naturally extends to functions. In the textbook, there is a brief tangent that connects this to the fourier series $\phi_k(x)$, which will not be covered at this time. 

\subsubsection*{Hilbert Space}

Building on the previous concepts of $\Cn$ and functional spaces, we are ready to define Hilber spaces. These are generalization of inner product spaces that is \textit{complete}. 

\begin{definition}[Hilbert Space]
    A Hilber space is an \textbf{inner product space} that is \textit{complete} with respect to the norm induced by its inner product. Completeness means that every Cauchy sequence in the space converges to a point within the space. 
\end{definition}

So what is a cauchy sequence? It is what Augustin Cauchy proposed as the solution to the question: how do you decide when to consider a sequence as convergent?

\begin{definition}[Cauchy Sequence]
    A $\left\{s_n\right\} = (s_1, s_2, \ldots)$ is called a \textbf{Cauchy Sequence} if:
    $$\forall N \in \N, \quad \epsilon > 0, \quad n > N \qquad \Longrightarrow \qquad \abs{s_n - s_{n+1}} < \epsilon$$
    \begin{center}
        \includegraphics[width=200px]{6_cauchy}
    \end{center}
\end{definition}

In relation to the Hilbert space, it means that every sequence in the Hilbert space will converge to a point in the Hilbert space, that it is complete. In the context of quantum computing and representing information, the $n$-qubit state space is a finite system constructed from an inner product space with dimension $2^n$. This is formalized by the following theorem:

\begin{theorem}
    Every finite-dimensional real or complex inner product space is a Hilbert space.
\end{theorem}


In quantum mechanics, Hilbert spaces will often extend to infinite dimensions. It ensures that any linear combination or superposition remains a valid quantum state, providing consistency to the space. In other words, this space is \textbf{complete}.

\subsection{Orthogonality and Projection}

We define orthogonality using the inner product, where, in close conjunction with the norm and establishing orthonormal bases, is a key concept. It entertains a close relationship with the projection problem, playing a crucial role in determining the measurement probabilities for different outcome states. From a geometric standpoint, the angle between two non-zero vectors in $\R^2/\R^3$ is defined as the intuitive angle between the two vectors. 

When we talk about the dot product, it is closely related to the two. Applying the law of cosines:

$$
    \norm{\uvec - \vvec}^2 = \unorm^2 + \vnorm^2 - 2 \unorm \vnorm \cos \theta
$$

and rearranging this equation gives us:

$$
    \unorm \vnorm \cos \theta = \frac{1}{2} (\unorm^2 + \vnorm^2 - \norm{\uvec - \vvec}^2 )
$$

We can see that the right hand side is equal to the dot product of $\uvec$ and $\vvec$, bringing us to the following theorem and definition.

\begin{center}
    \includegraphics[width=200px]{6_dot_prod_angle}
\end{center}

\begin{theorem}
    For vectors $\uvec, \vvec \in \R^2/\R^3$, we have the following identity:
    
    $$\uvec \cdot \vvec = \unorm \vnorm \cos \theta$$

    where $\theta$ is the geometric angle between $\uvec$ and $\vvec$.
\end{theorem}

In fact, while we say that this is true for $\uvec, \vvec \in \R^2/\R^3$, it is only because we mention that this is for the geometric angle $\theta$. It is very difficult to visualize $\theta$ in hyperspace, yet the following general definition holds for non-zero vectors in $\Rn$. 

\begin{definition}[Angle Between Two Real Vectors]
    For any $\uvec, \vvec \in \R^n$, the angle is said to lie within the interval [$0, \pi$] and is given by:
    $$\cos \theta = \frac{\uvec \cdot \vvec}{\unorm \vnorm}$$
\end{definition}

Note that for the previous definition to hold, the computed cosine value must lie within the interval of $[-1, 1]$. This condition is guaranteed to be satisfied given the \textbf{Cauchy-Schwarz Inequality}, stating that:

$$ - \unorm \vnorm \leq \vvec \cdot \uvec \leq \unorm \vnorm $$


For complex numbers, we say that $\uvec \cdot \vvec$ is generally a complex number. As a result, the previous definition for $\cos \theta$ no longer holds since the right side is generally complex. However, the condition $\vvec \cdot \uvec = 0$ still holds regardless and remains of great use even for complex vectors. 

\begin{definition}[Orthogonality in $\Cn$]
    Given two vectors $\uvec, \vvec \in \Cn$, we say that $\uvec$ and $\vvec$ are orthogonal if $\vvec \cdot \uvec = \uvec \cdot \vvec = 0$.
\end{definition}

\begin{definition}[Orthogonality in Inner Product Spaces]
    Generally, for $\uvec, \vvec \in V$, where $V$ is an inner product space, we say that $\uvec$ and $\vvec$ are orthogonal when $\innerprod{\uvec}{\vvec} = \innerprod{\vvec}{\uvec} = 0$
\end{definition}

\begin{definition}[Generalized Pythagorean Theorem]
    For $\uvec, \vvec \in \Cn$, if $\uvec$ and $\vvec$ are orthognal, then 
    $$\vnorm^2 + \unorm^2 = \norm{\uvec + \vvec}^2$$
\end{definition}

\begin{proof}
    Since we know that orthogonal vectors $\uvec, \vvec$ have the property $\innerprod{\uvec}{\vvec} = 0$, then:
    $$\innerprod{\uvec}{\vvec} = (\uvec + \vvec) \cdot (\uvec + \vvec) = \unorm^2 + \uvec \cdot \vvec + \vvec \cdot \uvec + \vnorm^2 =  \unorm^2 + \vnorm^2$$
\end{proof}

There is an intriguing property of orthogonal vectors that can be illustrated with the following example:

\begin{example}
    Given $\uvec = (1, 0)$, find a unit vector $\vvec \in \C^2$ that is orthogonal to $\uvec$. If we take $\vvec = (v_1, v_2)$, we know that $\innerprod{\uvec}{\vvec} = 0$, which means that $v_1 = 0$, and it will not matter what $v_2$ is equal to, as long as it is of length one. Therfore, we have
    $$\vvec = (0, e^{i \phi}), \quad \phi \in \R$$
\end{example}

This example illustrates a simple concept about complex vectors, whcih is that there exists an infinite number of unit vectors that are orthognoal to $(1, 0)$ in $\C^2$. This comes from the fact that multiplying, or scaling, by a factor $i \phi$ only maintains the magnitude of a complex number $z$. Therefore, we have the the following theorem:

\begin{theorem}
    Given two unit vectors $\mathbf{z}, \wvec \in \Cn$ that are orthogonal, there are an infinitely many unit vecros in the span of $\wvec$ which are ortogonal to $\mathbf{z}$ in the form of $e^{i \phi}\wvec, \phi \in \R$. 
\end{theorem}

\begin{proof}
    We rewrite $\wvec^\prime = e^{i \phi}\wvec, \phi \in \R$ as $\wvec^\prime = \lambda \wvec, \lambda \in \C$. The orthogonality operation then becomes:
    $$\mathbf{z} \cdot \wvec^\prime = \mathbf{z} \cdot (\lambda \wvec) = \lambda (\mathbf{z} \cdot \wvec) = \lambda \zerovec = \zerovec$$
    Furthermore, since $\norm{\wvec^\prime} = 1$, we see that:
    $$\norm{\wvec^\prime} = \norm{\lambda \wvec} = \abs{\lambda} \cdot \norm{\wvec} = \abs{\lambda}$$
    Implying that $\lambda$ is of the form $e^{i \phi}$.
\end{proof}

Since quantum states are represented by unit vectors in $\Cn$, we can see that multiplying any unit vector by a factor of $e^{i \phi}$ preserves its modulus, hence preserving it as a unit vector. We therefore call these vectors \textbf{global phase factors}, which highlights its effect across the quantum state. 

\begin{theorem}
    Given two orthogonal vectors $\ketphi, \ketpsi \in \Cn$, they satisfy:
    $$\braket{\phi}{\psi} = \braket{\psi}{\phi} = 0$$
\end{theorem}

Orthogonaliy is closely related to the concept of projections in the sense that it is commonly known as an orthogonal projection. An example of this in $\R^2$ is illustrated by the dashed line in the figure below. 

\begin{center}
    \includegraphics[width=\textwidth]{6_2d_proj}
\end{center}

Once you understand this conceptually in two dimensions, it becomes easier to see how $\mathbf{a}$ can be extended to a plane in three dimensions, and so on, bringing us to the general projection theorem in $\Rn/\Cn$. 

\begin{theorem}[Projection Theorem]
    Given $\vvec, \mathbf{a} \in \Rn/\Cn$, there is an \textit{unique} way to decompose $\vvec$ as $\vvec = \wvec + \wvec^\prime$ where $\wvec$ is in the span of $\mathbf{a}$ and $\wvec^\prime$ is orthogonal to $\mathbf{a}$. 

    Here, we denote $\wvec$ as the \textbf{orthogonal projection of $\vvec$ onto $\mathbf{a}$} and denote it as $\text{proj}_{\mathbf{a}}{\vvec}$ while $\wvec^\prime$ as the \textbf{complement of $\vvec$ orthogonal to $\mathbf{a}$}, denoted as $\vvec_{\perp \mathbf{a}}$. They are given by the following formulas:
    \begin{align*}
        \text{proj}_{\mathbf{a}}{\vvec} = \vvec_{\Vert \mathbf{a}} &= \frac{\mathbf{a} \cdot \vvec}{\norm{\mathbf{a}}^2}\mathbf{a} \\
        \vvec_{\perp \mathbf{a}} &= \vvec - \text{proj}_{\mathbf{a}}{\vvec} \\
    \end{align*}
\end{theorem}

\begin{proof}
    Since $\wvec$ is in the span of the vector $\mathbf{a}$, it can be writen as a scaled version of $\mathbf{a}$, i.e. $\wvec = \lambda \mathbf{a}, \lambda \in \C$. It is thus possible to expand the equation into:
    $$\mathbf{a} \cdot \vvec = \mathbf{a} \cdot (\wvec + \wvec^\prime) = \mathbf{a} \cdot \wvec + \mathbf{a} \cdot \wvec^\prime = \mathbf{a} \cdot \wvec = \mathbf{a} \cdot (\lambda \mathbf{a}) = \lambda (\mathbf{a} \cdot \mathbf{a})$$
    Solving for $\lambda$ gives us:
    $$\lambda = \frac{\mathbf{a} \cdot \vvec}{\mathbf{a} \cdot \mathbf{a}} = \frac{\mathbf{a} \cdot \vvec}{\norm{\mathbf{a}}^2}$$
    We therefore have:
    $$\wvec = \lambda \mathbf{a} = \frac{\mathbf{a} \cdot \vvec}{\norm{\mathbf{a}}^2} \mathbf{a}, \qquad \wvec^\prime = \vvec - \wvec = \vvec - \frac{\mathbf{a} \cdot \vvec}{\norm{\mathbf{a}}^2} \mathbf{a}$$
\end{proof}

Since in quantum computing the state vectors are unit vectors, the projection between unit complex vectors are of vital importance. We thus have:

\begin{theorem}[Projection Theorem Between Complex Unit Vectors]
    Consider $\ketphi, \ketpsi \in \Cn$. The projection of $\ketphi$ onto $\ketpsi$ is given by the formula:
    $$\text{proj}_{\ketpsi} \ketphi = \braket{\psi}{\phi} \ketpsi$$
    where $\abs{\braket{\psi}{\phi}} \leq 1$.
\end{theorem}

Now in order to prove this, we need an adaptation of the Cauchy-Schwarz Inequality for Dirac notation$$\abs{\braket{\phi}{\psi}}^2 \leq \braket{\phi}{\psi} \cdot \braket{\psi}{\psi} \quad \Longleftrightarrow \quad \abs{\innerprod{\phi}{\psi}} \leq \norm{\psi} \norm{\phi}$$.


\begin{proof}
    If we rewrite this equation into Dirac notation, it gives us 
    $$\text{proj}_{\ketphi} \ketpsi = \frac{\braket{\psi}{\phi}}{\braket{\phi}{\phi}} \ketpsi$$
    Since $\ketphi$ is a unit vector, $\braket{\phi}{\phi} = \norm{\phi}^2 = 1$, simplifying to the equation in the theorem above. 
    In order to prove that $\braket{\phi}{\psi} \leq 1$, we apply the Cauchy-Schwarz inequality and get:
    $$\abs{\braket{\phi}{\psi}}^2 \leq \braket{\phi}{\phi} \braket{\psi}{\psi} = 1$$
    since both $\braket{\phi}{\phi} = \braket{\psi}{\psi} = 1$.
\end{proof}

This is important for properties such as the Born Rule, which states that upon measuring an obvservable $M$ on a quantum state $\ketpsi$, the probability of obtaining each eigenvalue $\lambda_i$ is given by $\abs{\braket{\lambda_i}{\psi}}^2$, where $\ket{\lambda_i}$ is the corresponding eigenvector. Since the aforementioned theorem ensures that $\abs{\braket{\lambda_i}{\psi}}^2 \leq 1$, we can ensure that it is a valid probability. It is now necessary to reintroduce the general form of the following theorem:

\begin{theorem}[Cauchy-Schwarz Inequality]
    Consider any two vectors $\uvec, \vvec \in V$, where $V$ is an inner product space.Then we have the following inequality:
    $$\abs{\innerprod{\uvec}{\vvec}}^2 = \leq \innerprod{\uvec}{\uvec} \innerprod{\vvec}{\vvec}.$$ 
\end{theorem}

This theorem can be adapted into several other forms for various inner product spaces, such as $\Rn$, $\Cn$, etc. 

\subsection{Orthonormal Bases}

We know that bases of vector spaces are a set of linearly independent vectors that span a given space. In this subsection, we will extend this generalization to inner product spaces (which have additional properties such as norm and orthogonality). In these spaces, orthonormal bases are formed by sets of vectors that are both orthognoal and normalized, are an intuitive and funcamental concept for both quantum mechanics and quantum computing, simplifying the representation of quantum states and illustrating key concepts such as superposition, entanglement, state evoluytion, and measurement. Here, Dirac notation is the norm. 

\begin{definition}[Orthonormal Basis]
    A basis $S = \left\{\ket{b_1}, \ket{b_2}, \ldots, \ket{b_n}\right\}$ for an $n$-dimensional complex inner product space $V$ is said to be orthonormal when they have norm $1$ and are all orthogonal to each other. That is:
    $$\norm{b_1} = \norm{b_2} = \ldots = \norm{b_n} = 1$$
    $$\braket{b_i}{b_j} = 0 \quad \forall i \neq j$$
\end{definition}

This is very much intuitive and analogous to the definition of any orthonormal basis. However, extending this to complex inner product spaces, we have the following theorem:

\begin{theorem}[Fundamental Property of Orthonormal Bases]
    Given a basis $S$of a complex inner product space, $S$ is orthonormal if and only if all inner products between pairs of basis vectors satisfy:
    $$
    \braket{b_i}{b_j} = \delta_{ij}, \qquad \delta_{ij} = \begin{cases}
        0 \quad i \neq j \\
        1 \quad i = j
    \end{cases}
    $$
\end{theorem}

When expressing any unit vector in terms of orthonormal basses, the following key property holds true.

\begin{theorem}
    Let $\ketpsi \in \Cn$ be a \textit{unit vector}, and take a set of orthonormal basis $\left\{\ket{b_i}\right\}$. Suppose we can express $\ketpsi$ as a linear combination of the basis vectors as:
    $$\ketpsi = c_1 \ket{b_1} + c_2 \ket{b_2} + \cdots + c_n \ket{b_n}$$
    with $\left\{c_i\right\} \in \C$, then we have the squared magnitudes of the coefficients satisfying the following constraint:
    $$\sum_{i=1}^{n} \abs{c_i}^2 = 1$$ 
\end{theorem}

\begin{proof}
    Using rules of the complex dot product and the properties of an orthonormal basis, we can see that this holds:
    \begin{align}
        \braket{\psi}{\psi} &= \left(c_1^* \bra{b_1} + c_2^* \bra{b_2} + \cdots c_n^* \bra{b_n}\right) \left(c_1 \ket{b_1} + c_2 \ket{b_2} + \ldots c_n \ket{b_n}\right) \\
        &= \sum_{i=1}^{n} c_i^* c_i \braket{b_i}{b_i} + \sum_{i \neq j} c_i^* c_j \braket{b_i}{b_j} \\
        &= \sum_{i=1}^{n} c_i^* c_i 1 + \sum_{i \neq j} c_i^* c_j \mathbf{0} \\
        &= \sum_{i=1}^{n} \abs{c_i}^2
    \end{align}
    Since $\ketpsi$ is a unit vector, $\braket{\psi}{\psi} = \norm{\psi}^2 = 1$, so $\braket{\psi}{\psi} = \sum_{i=1}^{n} \abs{c_i}^2 = 1$.
\end{proof}

In quantum computing, we would take $\ketphi$ to be a \textbf{superposition state} within the basis $\left\{\ket{b_i}\right\}$, and when measured, the probability that this quantum state would reduce itself to $\left\{\ket{b_i}\right\}$ is $\abs{c_i}^2$. The previous theorem simply proves that the summation of these states is equal to one, consistent with the definition of probability. Now note that, starting at this point in the notes, that \textbf{all complex vectors expressed in Dirac notation are unit vectors}, unless stated otherwise. This is standard practice for quantum computing literature. 

In quantum computing, state vectors $\ketphi$ are typically expressed as superpositions within an orthonormal basis, rather than the standard basis themselves. For example, take the state $\ketplus = \frac{1}{\sqrt{2}}(\ketzero + \ketone)$, rather than taking the matrix form $\ketplus = \left[\frac{1}{\sqrt{2}} \quad \frac{1}{\sqrt{2}}\right]^T$. This greatly simplifies operations such as outer and tensor products, introduced in the later chapters. 

\begin{theorem}
    Given $\ketpsi$ and an orthonormal basis $S$ in an $n$-dimensional complex inner product space, $\ketpsi$ can be decomposed as a superposition of basis vectors:
    $$\ketpsi = c_1 \ket{b_1} + c_2 \ket{b_2} + \cdots + c_n \ket{b_n}$$
    where each coefficient $c_i$ is given by:
    $$c_i = \braket{b_i}{\psi}$$
\end{theorem}

\begin{proof}
    Since we are expressing $\ketpsi$ as a linear combination of the basis vectors $\left\{\ket{b_i}\right\}$, 
    $$\ketpsi = c_1 \ket{b_1} + c_2 \ket{b_2} + \cdots + c_n \ket{b_n}$$
    then in order to find $c_i$, we only need to take 
    \begin{align*}
        \braket{b_i}{\psi} &= \bra{b_i}(c_1 \ket{b_1} + c_2 \ket{b_2} + \cdots + c_n \ket{b_n}) \\
        &= c_1 \braket{b_i}{b_1} + c_2 + \cdots + c_n \braket{b_i}{b_n} \\
        &= c_1 \cdot 1 + \sum_{i \neq j} c_j \cdot 0 \\
        &= c_i \\
    \end{align*}
\end{proof}

This can also be rewritten as 

$$\ketpsi = \braket{b_1}{\psi} \ket{b_1} + \braket{b_2}{\psi} \ket{b_2} + \cdots + \braket{b_n}{\psi} \ket{b_n} = \sum_{i=1}^{n} \braket{b_i}{\psi} \ket{b_i}$$

where each component $\braket{b_i}{\phi} \ket{b_i}$ is just the projection of $\ketphi$ onto $\ket{b_i}$ or $\projection{\ket{b_i}}{\ketpsi}$.

\begin{center}
    \includegraphics[width=\textwidth]{6_orthonorm_decomp}
\end{center}

Now we know that generally vectors can be decomposed into some naturally existing subspaces and vectors defined in real linear algebra. This translates well to complex inner product spaces, where $c_i = \braket{b_i}{\psi}$ represents the $i$-th coordinate of $\ketpsi$ relative to the orthonormal basis $S = \left\{\ket{b_i}\right\}$, and obtained by projecting $\ketpsi$ onto the corresponding basis vector $\ket{b_i}$. 

We know that all finite-dimensional inner product spaces have an orthonormal basis. Actually, they have infinite orthonormal bases, and in some situations it is critical to create an orthonormal basis from a non-orthonormal one. The gram-schmidt process defines a methodical algorithm for htis task, which is especially beneficial when working with degenerate eigenvalues.

In order to fully understand this processs, we must first introduce a few theorems that lay the foundation for this. 

\begin{theorem}[Basis from Linearly Independent Vectors]
    Given a set $S$ with exactly $n$ vectors from a vector space $V$ with $\dim(V) = n$, $S$ forms a basis for $V$ if and only if $S$ is linearly independent. 
\end{theorem}

\begin{proof}
    Assume that $S$ forms a basis yet is linearly independent. Then it must be possible to remove at least one vector from $S$ without changing it's span. However, this means that the number of vectors will now be strictly less than $n$, which is in direct contradiction to the previous theorem that states the minimum number of vectors required to span a space is equal to its dimension. Therefore, $S$ must be linearly independent. 
\end{proof}

\begin{theorem}[Orthogonal Basis]
    Take $S = \left\{\vvec_1, \vvec_2, \ldots, \vvec_n\right\}$ is a set of nonzero vectors in an inner product space $V, \dim(V) = n$, and that these vectors are orthogonal to each other s.t. $\braket{\vvec_i}{\vvec_j} = 0, \forall i \neq j$. Thus, $S$ constitues an orthogonal basis of $V$. 
\end{theorem}

\begin{proof}
    Since $S$ is already defined as an orthogonal basis, we only need to demonstrate that $S$ is linearly independent. Consider the following equation:
    $$k_1 \vvec_1 + k_2 \vvec_2 + \cdots + k_n \vvec_n = \zerovec$$
    If we take the left dot product for each $\vvec_i$ with this equation, then we have 
    \begin{align*}
        \braket{\vvec_i}{k_1 \vvec_1 + k_2 \vvec_2 + \cdots + k_n \vvec_n} &= k_1 \braket{\vvec_i}{\vvec_1} + k_2 \braket{\vvec_i}{\vvec_2} + \cdots + k_n \braket{\vvec_i}{\vvec_n} \\
        &= k_i \braket{\vvec_i}{\vvec_i} \\
    \end{align*}
    We are left with:
    $$k_i \braket{\vvec_i}{\vvec_i} = 0$$
    Since $\braket{\vvec_i}{\vvec_i} = \norm{\vvec_i}^2 \neq 0$ for all non-zero vectors, it follows that $k_i = 0$. Hence, $S$ is linearly independent.
\end{proof}

Normalizing each basis vector in an orthogonal basis naturally leads to an orthonormal basis. This transformation is formalized in:

\begin{theorem}
    Given an orthogonal basis $S$ for an inner product space $V$, an orthonormal basis can be obtained by normalizing each basis vector in $S$:
    $$\hat{\vvec_1} = \frac{\vvec_1}{\norm{\vvec_1}}, \hat{\vvec_2} = \frac{\vvec_2}{\norm{\vvec_2}}, \ldots, \hat{\vvec_n} = \frac{\vvec_n}{\norm{\vvec_n}}$$
\end{theorem}

The Gram Schmidt process, 

\begin{theorem}[The Gram-Schmidt Process]
    The following computational steps, known as the Gram-Schmidt process, transform any basis \(\{\uvec_1, \uvec_2, \ldots, \uvec_n\}\) of an inner product space \(V\) into an orthogonal basis \(\{\vvec_1, \vvec_2, \ldots, \vvec_n\}\):

        \begin{enumerate}
            \item \(\vvec_1 = \uvec_1\)
            
            \item \(\vvec_2 = \uvec_2 - \projection{\vvec_1}{\uvec_2}\)
            
            \item \(\vvec_3 = \uvec_3 - \projection{\vvec_1}{\uvec_3} - \projection{\vvec_2}{\uvec_3}\)
            
            \item \(\vvec_4 = \uvec_4 - \projection{\vvec_1}{\uvec_4} - \projection{\vvec_2}{\uvec_4} - \projection{\vvec_3}{\uvec_4}\)
             
            \item[n.] \(\displaystyle \vvec_n = \uvec_n - \sum_{i=1}^{n-1} \projection{\vvec_i}{\uvec_n}\)
        \end{enumerate}
\end{theorem}

\begin{proof}
    In this proof, we utilize the complex dot product as the inner product. Given that \(\{\uvec_1, \uvec_2, \ldots, \uvec_n\}\) is a basis for \(V\), and hence \(\dim(V) = n\), we aim to demonstrate that \(\{\vvec_1, \vvec_2, \ldots, \vvec_n\}\) forms an orthogonal basis. This follows from the previous theorem where we showed that \(\vvec_i \cdot \vvec_j = 0, \forall i \neq j\).

    We establish the following relationships: that the inner product of a vector with another one onto itself is just the dot product with that vector, and the dot product of two vectors that are already orthogonal, then the projection of another vector onto the orthogonal vector is simply zero. Mathmatically:

    \begin{align}
        \vvec_i \cdot \projection{\vvec_i}{\uvec_j} &= \vvec_i \cdot \left(\frac{\vvec_i \cdot \uvec_j}{\vvec_i \cdot \vvec_i}\vvec_i\right) = \frac{\vvec_i \cdot \uvec_j}{\vvec_i \cdot \vvec_i}\vvec_i \cdot \vvec_i = \vvec_i \cdot \uvec_j,  \\
        \vvec_i \cdot \projection{\vvec_k}{\uvec_j} &= \vvec_i \cdot \left(\frac{\vvec_k \cdot \uvec_j}{\vvec_k \cdot \vvec_k}\vvec_k\right) = \frac{\vvec_k \cdot \uvec_j}{\vvec_k \cdot \vvec_k}\vvec_i \cdot \vvec_k = 0 \quad \text{if } \vvec_i \cdot \vvec_k = 0.
    \end{align}

    With these formulas, we proceed with the proof using mathematical induction.

    \textbf{Base Case:}

    For \(n = 2\), the vectors \(\vvec_1\) and \(\vvec_2\) are given by:
    \[
    \vvec_1 = \uvec_1, \quad \vvec_2 = \uvec_2 - \projection{\vvec_1}{\uvec_2}.
    \]

    Compute the inner product \(\vvec_1 \cdot \vvec_2\):
    \begin{align*}
        \vvec_1 \cdot \vvec_2 &= \vvec_1 \cdot (\uvec_2 - \projection{\vvec_1}{\uvec_2}) \\
        &= \vvec_1 \cdot \uvec_2 - \vvec_1 \cdot \projection{\vvec_1}{\uvec_2} \\
        &= \vvec_1 \cdot \uvec_2 - \vvec_1 \cdot \uvec_2 \\
        &= 0.
    \end{align*}

    Thus, \(\vvec_1\) and \(\vvec_2\) are orthogonal.

    \textbf{Inductive Step:}

    Assume that \(\vvec_1, \vvec_2, \ldots, \vvec_k\) are mutually orthogonal. We prove that \(\vvec_{k+1}\) is orthogonal to all previous \(\vvec_i\) for \(i = 1, 2, \ldots, k\). By definition:
    \[
        \vvec_{k+1} = \uvec_{k+1} - \sum_{i=1}^{k} \projection{\vvec_i}{\uvec_{k+1}}.
    \]

    Compute the inner product \(\vvec_j \cdot \vvec_{k+1}\) for \(j = 1, 2, \ldots, k\):
    \begin{align*}
        \vvec_j \cdot \vvec_{k+1} &= \vvec_j \cdot \left(\uvec_{k+1} - \sum_{i=1}^{k} \projection{\vvec_i}{\uvec_{k+1}}\right) \\
        &= \vvec_j \cdot \uvec_{k+1} - \sum_{i=1}^{k} \vvec_j \cdot \projection{\vvec_i}{\uvec_{k+1}}.
    \end{align*}

    For \(i \neq j\), \(\vvec_j \cdot \projection{\vvec_i}{\uvec_{k+1}} = 0\) because \(\vvec_j \cdot \vvec_i = 0\) by the induction hypothesis. For \(i = j\):
    \[
    \vvec_j \cdot \projection{\vvec_j}{\uvec_{k+1}} = \vvec_j \cdot \uvec_{k+1}.
    \]

    Thus:
    \[
    \vvec_j \cdot \vvec_{k+1} = \vvec_j \cdot \uvec_{k+1} - \vvec_j \cdot \uvec_{k+1} = 0.
    \]

    By induction, \(\vvec_1, \vvec_2, \ldots, \vvec_{k+1}\) are mutually orthogonal.
\end{proof}


An illustration of the Gram-Schmidt Process in $\R^2$ and $\R^3$ is depicted in the above figure a few pages back. We can generally define the projection of a vector onto a subspace as:

\begin{definition}[Projection]
    Let $V$ be an inner product space and $\uvec \in V$, and take $W \subseteq V$. If $\left\{\vvec_1, \vvec_2, \ldots, \vvec_n\right\}$ is an orthogonal basis for $W$, then we define $\projection{W}{\uvec}$ as:
    $$\projection{W}{\uvec} = \sum_{i=1}^{r}\projection{\vvec_i}{\uvec} = \sum_{i=1}^{r} \frac{\innerprod{\vvec_i}{\uvec}}{\innerprod{\vvec_i}{\vvec_i}} \vvec_i$$
    The component of $\uvec$ orthogonal to $W$, denoted by $\projection{W^\bot}{\uvec}$, is:
    $$\projection{W^\bot}{\uvec} = \uvec = \projection{W}{\uvec}$$
\end{definition}

Here, we take $W^\bot$ to be the \textbf{orthogonal complement} of $W$, where the vectors in $W^\bot$ are orthogonal to every vector in $W$. 

\begin{center}
    \includegraphics[width=\textwidth]{6_gram_schmidt}
\end{center}

Following the previous definition, the Gram-Schmidt Process can be succinctly summarized:

\begin{enumerate}
    \item \(\vvec_1 = \uvec_1\),
    
    \item \(\vvec_2 = \text{proj}_{W_1^{\perp}} \uvec_2\), where \(W_1 = \text{span}\{\vvec_1\}\),
    
    \item \(\vvec_3 = \text{proj}_{W_2^{\perp}} \uvec_3\), where \(W_2 = \text{span}\{\vvec_1, \vvec_2\}\),
    
    \item \(\vvec_4 = \text{proj}_{W_3^{\perp}} \uvec_4\), where \(W_3 = \text{span}\{\vvec_1, \vvec_2, \vvec_3\}\),
    
    \vdots
    
    \item[n.] \(\vvec_n = \text{proj}_{W_{n-1}^{\perp}} \uvec_n\), where \(W_{n-1} = \text{span}\{\vvec_1, \vvec_2, \ldots, \vvec_{n-1}\}\),
\end{enumerate}

where \(W_i\) represents the subspace spanned by \(\vvec_1, \vvec_2, \ldots, \vvec_i\).

Interestingly, there have been several proposed forms for the Gram-Schmidt process in Python, a few examples we will include below:

\textbf{Classical Implementation}

The original implementation by \textbf{iizukak} provides a straightforward translation of the Gram-Schmidt algorithm, directly computing projections and subtracting them iteratively. However, this approach uses Python's \texttt{map} function and doesn't leverage NumPy's vectorization capabilities, making it slower for larger matrices:

\begin{lstlisting}[language=Python]
import numpy

def gs_coefficient(v1, v2):
    return numpy.dot(v2, v1) / numpy.dot(v1, v1)

def multiply(coefficient, v):
    return map((lambda x : x * coefficient), v)

def proj(v1, v2):
    return multiply(gs_coefficient(v1, v2), v1)

def gs(X):
    Y = []
    for i in range(len(X)):
        temp_vec = X[i]
        for inY in Y:
            proj_vec = proj(inY, X[i])
            temp_vec = map(lambda x, y : x - y, temp_vec, proj_vec)
        Y.append(temp_vec)
    return Y
\end{lstlisting}

\textbf{Modified Gram-Schmidt (Clear Implementation)}

\textbf{mretegan's} implementation is notable for its clarity and adherence to the textbook algorithm. It processes each column sequentially, subtracting projections onto previously computed orthogonal vectors. This is the \textbf{modified Gram-Schmidt} algorithm, which offers better numerical stability than the classical version by reducing floating-point error accumulation:

\begin{lstlisting}[language=Python]
import numpy as np

def gram_schmidt(A):
    # Get the number of vectors.
    n = A.shape[1]
    for j in range(n):
        # To orthogonalize the vector in column j with respect to the
        # previous vectors, subtract from it its projection onto
        # each of the previous vectors.
        for k in range(j):
            A[:, j] -= np.dot(A[:, k], A[:, j]) * A[:, k]
        A[:, j] = A[:, j] / np.linalg.norm(A[:, j])
    return A
\end{lstlisting}

\textbf{Vectorized Implementation}

\textbf{aditya0by0} improved upon the classical approach with a vectorized version that computes projections using matrix operations rather than loops. The key advantage is computing all projection coefficients simultaneously, which significantly improves performance for large matrices:

\begin{lstlisting}[language=Python]
def gram_schmidt(A, norm=True, row_vect=False):

    if row_vect:
        A = A.T
    
    no_of_vectors = A.shape[1]
    G = A[:, 0:1].copy()
    
    for i in range(1, no_of_vectors):
        numerator = A[:, i].dot(G)
        denominator = np.diag(np.dot(G.T, G))
        weights = np.squeeze(numerator / denominator)
        
        projected_vector = np.sum(weights * G, axis=1, keepdims=True)
        orthogonalized_vector = A[:, i:i+1] - projected_vector
        G = np.hstack((G, orthogonalized_vector))
        
    if norm:
        G = G / np.linalg.norm(G, axis=0)
    
    if row_vect:
        return G.T
    
    return G
\end{lstlisting}

\textbf{Handling Singular Matrices}

\textbf{JJGO} addressed an important practical issue: handling linearly dependent vectors. This implementation checks if a vector is already in the span of previous vectors (using a small tolerance like \texttt{1e-10}) and skips it if so:

\begin{lstlisting}[language=Python]
def gram_schmidt(vectors):
    basis = []
    for v in vectors:
        w = v - np.sum(np.dot(v, b) * b for b in basis)
        if (w > 1e-10).any():  
            basis.append(w / np.linalg.norm(w))
    return np.array(basis)
\end{lstlisting}

\textbf{NumPy QR Factorization}

\textbf{jakevdp} pointed out that NumPy's built-in \texttt{qr()} function provides the fastest and most numerically stable approach. The QR decomposition inherently performs Gram-Schmidt orthogonalization (using a modified version), but with optimized C implementations:

\begin{lstlisting}[language=Python]
import numpy as np

def gram_schmidt_columns(X):
    Q, R = np.linalg.qr(X)
    return Q
\end{lstlisting}

\textbf{Modified Gram-Schmidt for Complex Vectors}

For complex inner product spaces (relevant to quantum computing), \textbf{scrpy} provided a modified implementation using \texttt{np.vdot()} for proper complex conjugation:

\begin{lstlisting}[language=Python]
import numpy as np

def modifiedGramSchmidt(A):
    dim = A.shape[0]
    Q = np.zeros(A.shape, dtype=A.dtype)
    for j in range(0, dim):
        q = A[:, j]
        for i in range(0, j):
            rij = np.vdot(Q[:, i], q)
            q = q - rij * Q[:, i]
        rjj = np.linalg.norm(q, ord=2)
        if np.isclose(rjj, 0.0):
            raise ValueError
        else:
            Q[:, j] = q / rjj
    return Q
\end{lstlisting}

\textbf{Custom Inner Product Implementation}

For non-Euclidean inner product spaces, the algorithm can be adapted by replacing the dot product with a custom inner product function:

\begin{lstlisting}[language=Python]
import numpy as np

def inner_product(x, y):
    # Example: weighted inner product
    return x[0]*y[0] + 2*x[1]*y[1] + x[2]*y[2]

def gram_schmidt(V):
    U = []
    for i in range(len(V)):
        u = V[i]
        for j in range(i):
            proj = (inner_product(V[i], U[j]) / 
                    inner_product(U[j], U[j])) * U[j]
            u = u - proj
        U.append(u / ((inner_product(u, u))**0.5))
    return np.array(U)
\end{lstlisting}

\textbf{Performance Comparison}

According to \textbf{victorhcm's} testing, the NumPy QR approach and manual Gram-Schmidt implementations produce identical results (up to sign differences in the basis vectors). However, the QR factorization is typically 2-3 times faster and more numerically stable, especially for ill-conditioned matrices.

\break

\section{Fundamentals of Matrix Algebra}

\end{document}