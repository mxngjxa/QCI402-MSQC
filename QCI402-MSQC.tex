\documentclass[12pt]{article}

% Geometry
\usepackage[legalpaper, portrait, margin=1in]{geometry}

% Math packages
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{physics}
% \usepackage{braket}

% Graphics and drawing
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{quantikz} % For quantum circuit drawing
\usepackage{pgfplots}

% Code formatting
\usepackage{minted}

% Layout and formatting
\usepackage{titling}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{multicol}
\usepackage{xcolor}

% Tables
\usepackage{multirow}
\usepackage{array}
\usepackage{booktabs}
\usepackage{tabularx}


\usemintedstyle{manni}
\graphicspath{ {./figures/} }
\newcommand{\Rlogo}{\protect\includegraphics[height=1.8ex,keepaspectratio]{Rlogo.png}}

\input{config.tex}
% \input{quantum-commands.tex}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{postulate}[theorem]{Postulate}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}

\newtheorem*{example}{Example}

\theoremstyle{definition}
\newtheorem*{solution}{Solution}

\title{\COURSECODE\ - \FULLCOURSENAME}
\author{\PROFESSORNAME\ - \STUDENTNAME}
\date{\SEMESTER\ Semester \YEAR}

\begin{document}

\maketitle

\hfill

The underpinnings of all scientific advancements is the ability to express natural phenomena with the art of Mathmatics; this is no different for the subject of Quantum Computing. While the boundaries of quantum computing have been pushed beyond limits in theoretical terms on university blackboards, it has become of great interest to realize the theoretical computational power with the advances of hardware and technology. 

However, these notes mainly concerns itself with the mathmatical underpinnings of quantum computing that the course surrounds itself with. \FULLCOURSENAME takes a scaffolding approach designed to efficiently convey the required theoretical understanding of mathmatics in order to able to learn quantum computing. As of writing, we are basing the notes on verison one of the textbook published in March 2025. In this text, we will primarily be using dirac notation for the expression of vectors, operators, and their interactions. 

\tableofcontents

\break


\section{Summation and Product Notations}

This section primarily focuses on the common notations applied across mathmatics to denote and shorten addition and product notation. 

\subsection{Summation over a single Variable}

The sigma notation is defined as follows

$$\sum_{i=1}^{n}f(i)$$

where we use sigma $\sum$ to represent the sum of a series. For example, the sum of all numbers in a series beginning with $m$ and ending at index $n$ is written as:

$$\sum_{i=m}^{n} a_i = a_m + a_{m+1} + a_{m+2} + \cdots + a_{n-1} + a_n$$

Sums can also be infinite, commonly seen when Sigma looks as follows: $\sum^{\infty}_{i=m}$. Infinite sums are either convergent or divergent. A few of the most common converging infinite sums are as follows:

$$\sum_{i=0}^{\infty} \frac{1}{2^i} = 1 + \frac{1}{2} + \frac{1}{4} + \cdots = 2$$
$$\sum_{i=0}^{\infty}  \frac{1}{i^2} = \frac{1}{1^2} + \frac{1}{2^2} + \frac{1}{3^2} + \cdots = \frac{\pi^2}{6}$$

The first example is an infinite geometric series, and the sum of the first $n$ terms is given by:

$$S_n = \sum_{i=0}^{n} \frac{1}{2^i} = \frac{1 - \frac{1}{2^n}}{1 - \frac{1}{2}}$$

As $n \rightarrow \infty, \frac{1}{2^n} \rightarrow 0$. Consequently, $S_n \rightarrow \frac{1}{1 - \frac{1}{2}} = 2$. A rigorous proof of the second example requires extensive calculus and is not immediately obvious. While any mathmatical symbol can be used for the index of a summation, it is more practical to use something other than $i$ as in the context of complex numbers, $i$ commonly denotes the complex number$\sqrt{-1}$. moreover, sume can also be specified using descriptions. For example, 

$$\sum_{p \in P} f(p) \qquad P \in \mathbb{N^\prime}$$

where $\mathbb{N^\prime}$ is the set of all prime numbers. Summations can also contain parameters other than the index, which results in functions of those parameters. For example the discrete Fourier transform (DFT) is given by

$$\tilde{x}_k = \frac{1}{\sqrt{N}} \sum_{n=0}^{N - 1} x_n e^{- \frac{2 \pi i}{N} kn}, \quad k = 0, 1, \cdots N-1$$

where $x_n$ represents the $N$ values index by $n$ and $\tilde{x}_k$ are the Fourier coefficients. Here, $i$ is the imaginary numebr and $N$ is a positive integer representing the dimension fo the DFT, of which we will cover in greater depth in Chapter 3. The following are some useful summation forumae commonly encountered in quantum computing:

\[
\sum_{i=1}^{n} i = \frac{n(n+1)}{2}
\]

\[
\sum_{i=1}^{n} i^2 = \frac{n(n+1)(2n+1)}{6}
\]

\[
\sum_{i=1}^{n} i^3 = \left(\frac{n(n+1)}{2}\right)^2
\]

\[
\sum_{i=0}^{n} \left(a_0 + id\right) = (n+1)\left(a_0 + \frac{nd}{2}\right) \quad \text{(arithmetic series)}
\]

\[
\sum_{i=0}^{n} a^i = \frac{1 - a^{n+1}}{1 - a} \quad \text{(geometric series)}
\]

\[
(a + b)^n = \sum_{i=0}^{n} \binom{n}{i} a^{n-i} b^i \quad \text{(binomial theorem)}
\]

\[
\frac{1}{1 - x} = \sum_{n=0}^{\infty} x^n = 1 + x + x^2 + x^3 + \cdots \quad (\lvert x \rvert < 1)
\]

\[
\frac{1}{(1-x)^2} = \sum_{n=1}^{\infty} n x^{n-1} = 1 + 2x + 3x^2 + 4x^3 + \cdots \quad (\lvert x \rvert < 1)
\]

\[
\ln(1 + x) = \sum_{n=1}^{\infty} \frac{(-1)^{n+1}}{n} x^n = x - \frac{x^2}{2} + \frac{x^3}{3} - \cdots \quad (\lvert x \rvert < 1)
\]

\[
e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots
\]

\[
\sin x = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n+1)!} x^{2n+1} = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots
\]

\[
\cos x = \sum_{n=0}^{\infty} \frac{(-1)^n}{(2n)!} x^{2n} = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \cdots
\]

Below are also a list of the common summations rules and manupulations:

\[
\sum_{i=m}^{n} a_i = \sum_{j=m}^{n} a_j \quad \text{(change of index variable)}
\]

\[
\sum_{i=s}^{t} f(i) = \sum_{n=s}^{t} f(n) \quad \text{(change of index variable)}
\]

\[
\sum_{n=s}^{t} f(n) = \sum_{n=s}^{j} f(n) + \sum_{n=j+1}^{t} f(n) \quad \text{(splitting a sum)}
\]

\[
\sum_{n=s}^{t} f(n) = \sum_{n=0}^{t-s} f(t-n) \quad \text{(reverse order)}
\]

\[
\sum_{n=s}^{t} f(n) = \sum_{n=s+p}^{t+p} f(n-p) \quad \text{(index shift)}
\]

\[
\sum_{n=s}^{t} a \cdot f(n) = a \cdot \sum_{n=s}^{t} f(n) \quad \text{(distributivity)}
\]

\[
\sum_{n=s}^{t} f(n) \pm \sum_{n=s}^{t} g(n) = \sum_{n=s}^{t} \left(f(n) \pm g(n)\right) \quad \text{(commutativity)}
\]


\subsection{Products and other Notations}

Similar to the $\sum$ notation for addition, the $\prod$ (Pi) symbol is also more commonly used to dentoe the product of a series of terms. In this 

$$\prod_{i=m}^{n} a_i = a_m \cdot a_{m+1} \cdot a_{m+2} \cdot \cdots \cdot a_{n-1} \cdot a_n$$ 

for example, the factorial of $n$ is expressed as 

$$\prod_{i=0}^{n} i = n!$$

and the relationship between $\sum$ and $\prod$, which are 

$$b^{\sum_{n=s}^{t} f(n)} = \prod_{n=s}^{t} b^{f(n)}$$

$$\sum_{n=s}^{t} \log_b f(n) = log_b \prod_{n=s}^{t} f(n)$$

It is worth noting that in quantum computing and linear algebra, there are a few special notations such as the modulo-2 sum (bitwise XOR), or in other contexts the direct sum of linear spaces, represented by $\oplus$, and the tensor product represented by $\otimes$.

\subsection{Summation over Multiple Variables}

The double summation over a rectangular array is given by 

\begin{align*}
    \sum_{i=1,j=1}^{n_1, n_2} a_{i,j} &= \sum_{i=1}^{n_1} \sum_{j=1}^{n_2} a_{i, j} = \sum_{j=1}^{n_2} \sum_{i=1}^{n_1} a_{i, j} \\
    &= a_{1, 1} + a_{1, 2} + a_{1, 3} + a_{1, 4} + \cdots + a_{1, n_2} \\
    &+ a_{2, 1} + a_{2, 2} + a_{2, 3} + a_{2, 4} + \cdots + a_{2, n_2} \\
    & + a_{3, 1} + a_{3, 2} + a_{3, 3} + a_{3, 4} + \cdots + a_{3, n_2} \\
    & + a_{4, 1} + a_{4, 2} + a_{4, 3} + a_{4, 4} + \cdots + a_{4, n_2} \\
    & + \cdots \\
    & + a_{n_1, 1} + a_{n_1, 2} + a_{n_1, 3} + a_{n_1, 4} + \cdots + a_{n_1, n_2}
\end{align*}

Here, $\sum_{i=1}^{n_1} \sum_{j=1}^{n_2}$ represents summing over each row first and then summing the results, while $\sum_{j=1}^{n_2} \sum_{i=1}^{n_1}$ will represent summing over the columns and then summing those results. The term $\sum_{i=1,j=1}^{n_1, n_2} a_{i,j}$ represents the summation over the rectangular array, irrespecive of the order. The product of two sums can be expanded into a double sum as follows:

\begin{align*}
\left( \sum_{i=1}^{m} a_i \right) \left( \sum_{j=1}^{n} b_j \right)
    &= (a_1 + a_2 + \cdots + a_m)(b_1 + b_2 + \cdots + b_n) \\
    &=\; a_1b_1 + a_1b_2 + a_1b_3 + a_1b_4 + \cdots + a_1b_n \\
    &+ a_2b_1 + a_2b_2 + a_2b_3 + a_2b_4 + \cdots + a_2b_n \\
    &+ a_3b_1 + a_3b_2 + a_3b_3 + a_3b_4 + \cdots + a_3b_n \\
    &+ \cdots \\
    &+ a_mb_1 + a_mb_2 + a_mb_3 + a_mb_4 + \cdots + a_mb_n \\
    &= \sum_{i=1}^{m} \sum_{j=1}^{n} a_i b_j = \sum_{i=1}^{m} a_i \sum_{j=1}^{n} b_j
\end{align*}

which is actually rather intuitive given how the expansion of the standard expansion of the term $(a+b)^2$ plays out, a more elementary application of the distributive property which the above equation generalizes over. For a triangular matrix, in this case the lower triangular matrix, the sum is given by

\begin{align*}
    \sum_{1 \leq j \leq n}^{} a_{i,j} &= \sum_{i=1}^{n} \sum_{j=1}^{i} a_{i, j} = 
    \sum_{j=1}^{n} \sum_{i=j}^{n} a_{i, j} = 
    \sum_{j=0}^{n-1} \sum_{j=1}^{n-j} a_{i+j, i} \\
    &= a_{1, 1} \\
    & + a_{2, 1} + a_{2, 2}  \\
    & + a_{3, 1} + a_{3, 2} + a_{3, 3} \\
    & + a_{4, 1} + a_{4, 2} + a_{4, 3} + a_{4, 4} \\
    & + \cdots \\
    & + a_{n, 1} + a_{n, 2} + a_{n, 3} + a_{n, 4} + \cdots + a_{n, n}
\end{align*}

where the term $\sum_{1 \leq j \leq n}^{} a_{i,j}$ denotes the summation over all elements in a lower triangular array including the diagonal. The first notation variation will sum up each row to the $i$th element then aggregate while the second notation sums each column starting from the $j$th element downwards then aggregate the sums. The final expression will sum along the diagonal where $j=0$ represents the main diagonal and $j=n-1$ is the first off-diagonal, which is a single term. 

\begin{example}
    Say we would like to expand the product of $\left(1 + x_i\right)$ from $1$ to $n$. We have 

    $$\prod_{i=1}^{n} \left(1 + x_i\right) = 1 + \sum_{k=1}^{n} \left(\sum_{1 \leq i_1 < \cdots < i_k \leq n} \prod_{j=1}^{k} x_{i_j}\right)$$
\end{example}

This formula represents the \textit{multinomial expansion} of a product. When you expand the equation by hand, you get the product

$$\prod_{i=1}^{n} \left(1 + x_i\right) = (1 + x_1)(1 + x_2)\cdots(1 + x_n)$$

If we break this down, we see that the outer summation $\sum_{k=1}^{n}$ will go through each possible summation size in terms of the variables in question, and that the inner summation $\sum_{1 \leq i_1 < \cdots < i_k \leq n}$ will iterate through each possible unique product of the variables. while ensuring that they are unique. Not sure how this works, but if all $x_i$ are the same, then wesee that the equation actually simplifies to a subset of the binomial theorem 

$$(1 + x)^n = \sum_{k=0}^{n} \binom{n}{k} x^k$$

where $\binom{n}{k}$ is the binomial coefficient representing the numebr of ways to choose $k$ elements from a set of $n$ distinct elements.

\break

\section{Trigonometry}

You can't escape this.

\subsection{Definitions}

I mean, where do I start? The basic trigonometric functions are defined as the ratios between the angles of a right triangle. I will not show how these ratios remain the same given the same angle, nor will I go into great mathmatical detail of how to prove these items. However, we still have to go over this. Don't ask me why. 

\includegraphics[width=\textwidth]{2_trig0}

The functions of trigonometric functions can also be plotted out as follows:

\includegraphics[width=\textwidth]{2_trig1}

\subsection{Basic Properties and Inverse Functions}

\includegraphics[width=\textwidth]{2_trig2}

% --- 1. Trig Functions ---
\begin{center}
\begin{tikzpicture}[scale=1.5]
\begin{axis}[
    axis lines=middle,
    xlabel={}, ylabel={},
    xmin=-pi, xmax=pi,
    ymin=-5, ymax=5,
    restrict y to domain=-5:5,
    samples=300,
    legend style={at={(1.05,1)}, anchor=north west},
    domain=-2*pi:2*pi
]
\addplot[blue, thick] {sin(deg(x))};
\addlegendentry{$\sin(x)$}

\addplot[red, thick] {cos(deg(x))};
\addlegendentry{$\cos(x)$}

\addplot[green!70!black, thick] {tan(deg(x))};
\addlegendentry{$\tan(x)$}

\addplot[orange, thick, domain=-2*pi:-0.1] {1/sin(deg(x))};
\addplot[orange, thick, domain=0.1:2*pi] {1/sin(deg(x))};
\addlegendentry{$\csc(x)$}

\addplot[yellow, thick, domain=-2*pi:-pi/2-0.1] {1/cos(deg(x))};
\addplot[yellow, thick, domain=-pi/2+0.1:pi/2-0.1] {1/cos(deg(x))};
\addplot[yellow, thick, domain=pi/2+0.1:2*pi] {1/cos(deg(x))};
\addlegendentry{$\sec(x)$}

\addplot[brown, thick, domain=-2*pi:-0.1] {cos(deg(x))/sin(deg(x))};
\addplot[brown, thick, domain=0.1:2*pi] {cos(deg(x))/sin(deg(x))};
\addlegendentry{$\cot(x)$}

\end{axis}
\end{tikzpicture}
\end{center}

We can also see that there are certain useful symmetric properties of the trigonometric functions

\begin{align*}
\sin(-\theta) &= - \sin(\theta)           &  \sin(\pi-\theta) &= \sin(\theta)              &  \sin(\pi+\theta) &= - \sin(\theta)\\
\cos(-\theta) &= \cos(\theta)         &  \cos(\pi-\theta) &= - \cos(\theta)   &  \cos(\pi+\theta) &= - \cos(\theta)\\
\tan(-\theta) &= - \tan(\theta)   &  \tan(\pi-\theta) &= - \tan(\theta)         &  \tan(\pi+\theta) &= \tan(\theta)
\end{align*}

There are also some common inverse functions associated with the functions.

\includegraphics[width=\textwidth]{2_trig4}

But also some interesting extensions of the commonly known inverse functions, with the example being $\mathrm{arctan2}$, a function that effectively doubles the domain of the function while preserving its properties for the purpose of, say, converting values from cartesian to spherical coordinates for the azimuthal angle $\phi$.

\[
\mathrm{arctan2}(y, x) = 
\begin{cases}
  \arctan\left(\dfrac{y}{x}\right) & \text{if } x > 0, \\[2ex]
  \arctan\left(\dfrac{y}{x}\right) + \pi & \text{if } x < 0 \text{ and } y \geq 0, \\[2ex]
  \arctan\left(\dfrac{y}{x}\right) - \pi & \text{if } x < 0 \text{ and } y < 0, \\[2ex]
  +\dfrac{\pi}{2} & \text{if } x = 0 \text{ and } y > 0, \\[2ex]
  -\dfrac{\pi}{2} & \text{if } x = 0 \text{ and } y < 0, \\[2ex]
  0 & \text{if } x = 0 \text{ and } y = 0.
\end{cases}
\]

\includegraphics[width=\textwidth]{Arctangent2}

\subsection{Special Angles and Function Values}

There are a few special angles that are worth remembering for the trigonometric functions mentioned above, given by the wheel below.

\includegraphics[width=\textwidth]{2_trig6}

\subsection{Trigonometric Identities}

\subsubsection{Reciprocal and Quotient Identities}

\begin{align*}
\csc \theta &= \frac{1}{\sin \theta} &
\sec \theta &= \frac{1}{\cos \theta} &
\cot \theta &= \frac{1}{\tan \theta} \\
\end{align*}

\begin{align*}
    \tan \theta &= \frac{\sin \theta}{\cos \theta}, &
\cot \theta &= \frac{\cos \theta}{\sin \theta}
\end{align*}

\subsubsection{Cofunction Identities}
\begin{align*}
\sin\!\left(\frac{\pi}{2} - \theta \right) &= \cos \theta, &
\cos\!\left(\frac{\pi}{2} - \theta \right) &= \sin \theta \\
\tan\!\left(\frac{\pi}{2} - \theta \right) &= \cot \theta, &
\cot\!\left(\frac{\pi}{2} - \theta \right) &= \tan \theta \\
\sec\!\left(\frac{\pi}{2} - \theta \right) &= \csc \theta, &
\csc\!\left(\frac{\pi}{2} - \theta \right) &= \sec \theta
\end{align*}

\subsubsection{Pythagorean Identities}
\begin{align*}
\sin^2 \theta + \cos^2 \theta &= 1 \\
1 + \tan^2 \theta &= \sec^2 \theta \\
1 + \cot^2 \theta &= \csc^2 \theta
\end{align*}

\subsubsection{Even–Odd Symmetry}
\begin{align*}
\sin(-\theta) &= -\sin \theta, &
\cos(-\theta) &= \cos \theta, &
\tan(-\theta) &= -\tan \theta \\
\csc(-\theta) &= -\csc \theta, &
\sec(-\theta) &= \sec \theta, &
\cot(-\theta) &= -\cot \theta
\end{align*}

\subsubsection{Sum and Difference Formulas}
\begin{align*}
\sin(\alpha \pm \beta) &= \sin \alpha \cos \beta \pm \cos \alpha \sin \beta \\
\cos(\alpha \pm \beta) &= \cos \alpha \cos \beta \mp \sin \alpha \sin \beta \\
\tan(\alpha \pm \beta) &= \frac{\tan \alpha \pm \tan \beta}{1 \mp \tan \alpha \tan \beta}
\end{align*}

\subsubsection{Double Angle Formulas}
\begin{align*}
\sin(2\theta) &= 2\sin \theta \cos \theta \\
\cos(2\theta) &= \cos^2 \theta - \sin^2 \theta \\
&= 2\cos^2 \theta - 1 \\
&= 1 - 2\sin^2 \theta \\
\tan(2\theta) &= \frac{2\tan \theta}{1 - \tan^2 \theta}
\end{align*}

\subsubsection{Half Angle Formulas}
\begin{align*}
\sin^2\!\left(\frac{\theta}{2}\right) &= \frac{1 - \cos \theta}{2} \\
\cos^2\!\left(\frac{\theta}{2}\right) &= \frac{1 + \cos \theta}{2} \\
\tan\!\left(\frac{\theta}{2}\right) &= \frac{\sin \theta}{1 + \cos \theta} 
= \frac{1 - \cos \theta}{\sin \theta}
\end{align*}

\subsubsection{Product-to-Sum Identities}
\begin{align*}
\sin \alpha \sin \beta &= \frac{1}{2}\big[\cos(\alpha - \beta) - \cos(\alpha + \beta)\big] \\
\cos \alpha \cos \beta &= \frac{1}{2}\big[\cos(\alpha - \beta) + \cos(\alpha + \beta)\big] \\
\sin \alpha \cos \beta &= \frac{1}{2}\big[\sin(\alpha + \beta) + \sin(\alpha - \beta)\big]
\end{align*}

\subsubsection{Sum-to-Product Identities}
\begin{align*}
\sin \alpha + \sin \beta &= 2 \sin\!\left(\frac{\alpha + \beta}{2}\right) 
                           \cos\!\left(\frac{\alpha - \beta}{2}\right) \\
\sin \alpha - \sin \beta &= 2 \cos\!\left(\frac{\alpha + \beta}{2}\right) 
                           \sin\!\left(\frac{\alpha - \beta}{2}\right) \\
\cos \alpha + \cos \beta &= 2 \cos\!\left(\frac{\alpha + \beta}{2}\right) 
                           \cos\!\left(\frac{\alpha - \beta}{2}\right) \\
\cos \alpha - \cos \beta &= -2 \sin\!\left(\frac{\alpha + \beta}{2}\right) 
                           \sin\!\left(\frac{\alpha - \beta}{2}\right)
\end{align*}

A rather nice photo to sum the secion up is by relating the angles to each other using the following image

\includegraphics[width=\textwidth]{2_trig7.png}

\subsection{The Spherical Coordinate System}

While the expansion of the cartesian coordinate system into three dimensions is the logical linear expansion to take, some interesting basis begin to form if we consider the angles as a unit of measurement in three dimensions

\includegraphics[width=\textwidth]{2_trig8.png}

The cylindrical coordinate system elevates the polar coordinate system into three dimensions with the addition of the $z$ axis, while the spherical system is a more organic translation of the polar coordinate system into three dimensions. While the former is best suited for describing not only cylinder-like structures yet also helices, while the latter is best suited for rotations in three-dimensional space, which is particularly potent in the field of quantum computing. 

For example, the spherical coordinate system is commonly used to represent qubit states on the Bloch sphere, employing a radius $(r)$, a polar angle $(\theta)$, and an azimuthal angle $(\phi)$ to represent a point in three-dimensional space. The azimuthal angle $\phi$ is measured in the $xy$-plane from the positive $x$-axis with common values ranging from $(-\pi, \pi]$ or $(0, 2\pi]$. The polar angle is commonly measured from the positive $z$ axis towards the $xy$-plane, with values ranging from $[0, \pi]$. Note that $r \in \mathbb{R}$, meaning that we can cover the other half of the range simply by flipping the sign around. 

Conversion is relatively simple, with conversion to and from spherical to cartesian being as follows

\begin{align*}
    x &= r \sin \theta \cos \phi &     r &= \sqrt{x^2 + y^2 + z^2} \\
    y &= r \sin \theta \sin \phi &     \phi &= \mathrm{arctan2} (y, x) \\
    z &= r \cos \theta &         \theta &= \arccos \frac{z}{r}
\end{align*}

where $\mathrm{arctan2}$ was previously defined as a optimal inverse mapping onto the range of $[- \pi, \pi]$. We also have a few definitions

\begin{definition}
    The Law of Sines is defined as 

    $$\frac{\sin A}{a} = \frac{\sin B}{b} = \frac{\sin C}{c}$$
\end{definition}

\begin{definition}
    The Law of Cosines is defined as 

    $$a^2 = b^2 + c^2 - 2bc \cos A$$

    which can be rewritten as 

    $$\cos A = \frac{b^2 + c^2 - 2bc}{a^2}$$
\end{definition}

\begin{definition}
    The Law of Tangents is defined as 

    $$\frac{a - b}{a + b} = \frac{\tan \tfrac{1}{2}(A - B)}{\tan \tfrac{1}{2}(A + B)}$$
\end{definition}

\section{Complex Numbers}

Very important and fundamental to the math and quantum computing. 

\begin{definition}
    A complex number $z$ is defined as 

    $$
        z = x + iy, \qquad x, y \in \mathbb{R}, \quad i^2 = -1
    $$
\end{definition}

this is called the \textbf{Cartesian form} of the complex number $z$ and corresponds to a point in the two-dimensional complex plane. We commonly refer to $i$ as the imaginary unit. It may seem ironic that we need the imaginary number in quantum computing, or that we need the imaginary number to be real. Take it as you may. 

this can be easily converted into the conjugate by inverting the sign of the second unit.

Complex numbers not motivated by quantum computing. In the numbers system, we have the real numbers $\mathbb{N}$, the integer numbers $\mathbb{Z}$, the real numbers $\mathbb{R}$, and the complex numbers $\mathbb{C}$. the set incursions go this way $\mathbb{N} \subset \mathbb{Z} \subset \mathbb{R} \subset \mathbb{C}$. The set of all complex numbers $\mathbb{C}$ is closed over all algabraic operations, which include addition, substraction, multiplication, division, power, and root. 

Later it was discovered that we need this. 

\begin{definition}
    The basic components of a complex number are defined as follows. 

    $$Re(z) = x, Im(z) = y$$

    which are the real and imaginary components of a complex number $z$. The complex conjugate is defined as 

    $$  z^* = x - iy$$

    defined as inverting the sign of the imaginary component. We can express the modulus and argument (which are $r$ and $\theta$ in polar coordinates), as follows:

    $$r = \vert z \vert = \sqrt{zz^*} = \sqrt{x^2 + y^2}$$

    A very convenient property derived from algebra is that $zz^* = x^2 + y^2$.

    $$\theta = arg(z)$$

    For the angle, we note that

    $$\tan \frac{y}{x} \Longrightarrow \arctan2(x, y)$$

    where $\arctan2$ has been defined in the previous section. 
\end{definition}


\begin{definition}
    The exponential/euler forms of the complex numbers can be thought of as a circular form of the function $z = x + iy$. In polar coordinates, we can rewrite this number as 

    $$
        z = r \cos \theta + i \sin \theta, \qquad r \in \mathbb{R}
    $$
    Conversely, the conversion between cartesian and polar are 
    
    $$
    x = r \cos \theta \qquad y = r \sin \theta
    $$
    
    The formula for $z$ above can be rewritten as 

    $$z = r e^{i\theta}$$
\end{definition}


\begin{theorem}
    Euler's formula states that for any complex number $z = r \cos \theta + i \sin \theta$, we have:

    $$
    e^{i \theta} = \cos \theta + i \sin \theta
    $$
\end{theorem}

\begin{proof}
    Euler's formula can be proven using the Taylor series expansion for the functions:

    \[
    e^x = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \cdots
    \]

    \[
    \sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots
    \]

    \[
    \cos x = 1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \cdots
    \]

    if we replace $x$ in the formula for $e^x$ with $e^{ix}$, we then have 

    \begin{align*}
    e^{ix} &= 1 + (ix) + \frac{(ix)^2}{2!} + \frac{(ix)^3}{3!} + \frac{(ix)^4}{4!} + \frac{(ix)^5}{5!} + \frac{(ix)^6}{6!} + \frac{(ix)^7}{7!} + \cdots \\
    &= 1 + i x - \frac{x^2}{2!} - i \frac{x^3}{3!} + \frac{x^4}{4!} + i \frac{x^5}{5!} - \frac{x^6}{6!} - i \frac{x^7}{7!} + \cdots \\
    &= \left(1 - \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + \cdots \right) + \left(i x - i \frac{x^3}{3!} + i \frac{x^5}{5!}- i \frac{x^7}{7!} + \cdots\right) \\
    &= \cos \theta + i \sin \theta
    \end{align*}

    which sums up essentially what mathmaticians call the most beautiful proof man has known. 
\end{proof}

As noted before, we know that the set of all algebraic operations is well defined and closed on the set of all complex numbers $\mathbb{C}$. Addition will be easier in cartesian form, while multiplication will be considerably simpler in exponential form.

\end{document}